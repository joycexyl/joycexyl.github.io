{
  "papers": [
    {
      "id": "2602.17640",
      "title": "huff: A Python package for Market Area Analysis",
      "authors": [
        "Thomas Wieland"
      ],
      "abstract": "Market area models, such as the Huff model and its extensions, are widely used to estimate regional market shares and customer flows of retail and service locations. Another, now very common, area of application is the analysis of catchment areas, supply structures and the accessibility of healthcare locations. The huff Python package provides a complete workflow for market area analysis, including data import, construction of origin-destination interaction matrices, basic model analysis, parameter estimation from empirical data, calculation of distance or travel time indicators, and map visualization. Additionally, the package provides several methods of spatial accessibility analysis. The package is modular and object-oriented. It is intended for researchers in economic geography, regional economics, spatial planning, marketing, geoinformation science, and health geography. The software is openly available via the [Python Package Index (PyPI)](https://pypi.org/project/huff/); its development and version history are managed in a public [GitHub Repository](https://github.com/geowieland/huff_official) and archived at [Zenodo](https://doi.org/10.5281/zenodo.18639559).",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.AP",
        "cs.SE"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.17640v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17640v1",
      "doi": null
    },
    {
      "id": "2602.17603",
      "title": "SOLVAR: Fast covariance-based heterogeneity analysis with pose refinement for cryo-EM",
      "authors": [
        "Roey Yadgar",
        "Roy R. Lederman",
        "Yoel Shkolnisky"
      ],
      "abstract": "Cryo-electron microscopy (cryo-EM) has emerged as a powerful technique for resolving the three-dimensional structures of macromolecules. A key challenge in cryo-EM is characterizing continuous heterogeneity, where molecules adopt a continuum of conformational states. Covariance-based methods offer a principled approach to modeling structural variability. However, estimating the covariance matrix efficiently remains a challenging computational task. In this paper, we present SOLVAR (Stochastic Optimization for Low-rank Variability Analysis), which leverages a low-rank assumption on the covariance matrix to provide a tractable estimator for its principal components, despite the apparently prohibitive large size of the covariance matrix. Under this low-rank assumption, our estimator can be formulated as an optimization problem that can be solved quickly and accurately. Moreover, our framework enables refinement of the poses of the input particle images, a capability absent from most heterogeneity-analysis methods, and all covariance-based methods. Numerical experiments on both synthetic and experimental datasets demonstrate that the algorithm accurately captures dominant components of variability while maintaining computational efficiency. SOLVAR achieves state-of-the-art performance across multiple datasets in a recent heterogeneity benchmark. The code of the algorithm is freely available at https://github.com/RoeyYadgar/SOLVAR.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.ML",
        "stat.AP"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2602.17603v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17603v1",
      "doi": null
    },
    {
      "id": "2602.17592",
      "title": "BMW: Bayesian Model-Assisted Adaptive Phase II Clinical Trial Design for Win Ratio Statistic",
      "authors": [
        "Di Zhu",
        "Yong Zang"
      ],
      "abstract": "The win ratio (WR) statistic is increasingly used to evaluate treatment effects based on prioritized composite endpoints, yet existing Bayesian adaptive designs are not directly applicable because the WR is a summary statistic derived from pairwise comparisons and does not correspond to a unique data-generating mechanism. We propose a Bayesian model-assisted adaptive design for randomized phase II clinical trials based on the WR statistic, referred to as the BMW design. The proposed design uses the joint asymptotic distribution of WR test statistics across interim and final analyses to compute posterior probabilities without specifying the underlying outcome distribution. The BMW design allows flexible interim monitoring with early stopping for futility or superiority and is extended to jointly evaluate efficacy and toxicity using a graphical testing procedure that controls the family-wise error rate (FWER). Simulation studies demonstrate that the BMW design maintains valid type I error and FWER control, achieves power comparable to conventional methods, and substantially reduces expected sample size. An R Shiny application is provided to facilitate practical implementation.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.ME",
        "stat.AP"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.17592v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17592v1",
      "doi": null
    },
    {
      "id": "2602.17543",
      "title": "genriesz: A Python Package for Automatic Debiased Machine Learning with Generalized Riesz Regression",
      "authors": [
        "Masahiro Kato"
      ],
      "abstract": "Efficient estimation of causal and structural parameters can be automated using the Riesz representation theorem and debiased machine learning (DML). We present genriesz, an open-source Python package that implements automatic DML and generalized Riesz regression, a unified framework for estimating Riesz representers by minimizing empirical Bregman divergences. This framework includes covariate balancing, nearest-neighbor matching, calibrated estimation, and density ratio estimation as special cases. A key design principle of the package is automatic regressor balancing (ARB): given a Bregman generator $g$ and a representer model class, genriesz} automatically constructs a compatible link function so that the generalized Riesz regression estimator satisfies balancing (moment-matching) optimality conditions in a user-chosen basis. The package provides a modulr interface for specifying (i) the target linear functional via a black-box evaluation oracle, (ii) the representer model via basis functions (polynomial, RKHS approximations, random forest leaf encodings, neural embeddings, and a nearest-neighbor catchment basis), and (iii) the Bregman generator, with optional user-supplied derivatives. It returns regression adjustment (RA), Riesz weighting (RW), augmented Riesz weighting (ARW), and TMLE-style estimators with cross-fitting, confidence intervals, and $p$-values. We highlight representative workflows for estimation problems such as the average treatment effect (ATE), ATE on treated (ATT), and average marginal effect estimation. The Python package is available at https://github.com/MasaKat0/genriesz and on PyPI.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.ML",
        "cs.LG",
        "econ.EM",
        "math.ST",
        "stat.ME"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2602.17543v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17543v1",
      "doi": null
    },
    {
      "id": "2602.17503",
      "title": "An extension to reversible jump Markov chain Monte Carlo for change point problems with heterogeneous temporal dynamics",
      "authors": [
        "Emily Gribbin",
        "Benjamin Davis",
        "Daniel Rolfe",
        "Hannah Mitchell"
      ],
      "abstract": "Detecting brief changes in time-series data remains a major challenge in fields where short-lived states carry meaning. In single-molecule localisation microscopy, this problem is particularly acute as fluorescent molecules used to tag protein oligomers display heterogenous photophysical behaviour that can complicate photobleach step analysis; a key step in resolving nanoscale protein organisation. Existing methods often require extensive filtering or prior calibration, and can fail to accurately account for blinking or reversible dark states that may contaminate downstream analysis. In this paper, an extension to RJMCMC is proposed for change point detection with heterogeneous temporal dynamics. This approach is applied to the problem of estimating per-frame active fluorophore counts from one-dimensional integrated intensity traces derived from Fluorescence Localisation Imaging with Photobleaching (FLImP), where compound change point pair moves are introduced to better account for short-lived events known as blinking and dark states. The approach is validated using simulated and experimental data, demonstrating improved accuracy and robustness when compared with current photobleach step analysis methods and with the existing analysis approach for FLImP data. This Compound RJMCMC (CRJMCMC) algorithm performs reliably across a wide range of fluorophore counts and signal-to-noise conditions, with signal-to-noise ratio (SNR) down to 0.001 and counts as high as seventeen fluorophores, while also effectively estimating low counts observed when studying EGFR oligomerisation. Beyond single molecule imaging, this work has applications for a variety of time series change point detection problems with heterogeneous state persistence. For example, electrocorticography brain-state segmentation, fault detection in industrial process monitoring and realised volatility in financial time series.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.17503v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17503v1",
      "doi": null
    },
    {
      "id": "2602.17414",
      "title": "Nested Sampling with Slice-within-Gibbs: Efficient Evidence Calculation for Hierarchical Bayesian Models",
      "authors": [
        "David Yallup"
      ],
      "abstract": "We present Nested Sampling with Slice-within-Gibbs (NS-SwiG), an algorithm for Bayesian inference and evidence estimation in high-dimensional models whose likelihood admits a factorization, such as hierarchical Bayesian models. We construct a procedure to sample from the likelihood-constrained prior using a Slice-within-Gibbs kernel: an outer update of hyperparameters followed by inner block updates over local parameters. A likelihood-budget decomposition caches per-block contributions so that each local update checks feasibility in constant time rather than recomputing the global constraint at linearly growing cost. This reduces the per-replacement cost from quadratic to linear in the number of groups, and the overall algorithmic complexity from cubic to quadratic under standard assumptions. The decomposition extends naturally beyond independent observations, and we demonstrate this on Markov-structured latent variables. We evaluate NS-SwiG on challenging benchmarks, demonstrating scalability to thousands of dimensions and accurate evidence estimates even on posterior geometries where state-of-the-art gradient-based samplers can struggle.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.CO",
        "astro-ph.IM",
        "stat.ME"
      ],
      "primaryCategory": "stat.CO",
      "pdfUrl": "https://arxiv.org/pdf/2602.17414v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17414v1",
      "doi": null
    },
    {
      "id": "2602.17272",
      "title": "Estimating Zero-inflated Negative Binomial GAMLSS via a Balanced Gradient Boosting Approach with an Application to Antenatal Care Data from Nigeria",
      "authors": [
        "Alexandra Daub",
        "Elisabeth Bergherr"
      ],
      "abstract": "Statistical boosting algorithms are renowned for their intrinsic variable selection and enhanced predictive performance compared to classical statistical methods, making them especially useful for complex models such as generalized additive models for location scale and shape (GAMLSS). Boosting this model class can suffer from imbalanced updates across the distribution parameters as well as long computation times. Shrunk optimal step lengths have been shown to address these issues. To examine the influence of socio-economic factors on the distribution of the number of antenatal care visits in Nigeria, we generalize boosting of GAMLSS with shrunk optimal step lengths to base-learners beyond simple linear models and to a more complex response variable distribution. In an extensive simulation study and in the application we demonstrate that shrunk optimal step lengths yield a more balanced regularization of the overall model and enhance computational efficiency across diverse settings, in particular in the presence of base-learners penalizing the size of the fit.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.17272v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17272v1",
      "doi": null
    },
    {
      "id": "2602.17262",
      "title": "Quantifying and Mitigating Socially Desirable Responding in LLMs: A Desirability-Matched Graded Forced-Choice Psychometric Study",
      "authors": [
        "Kensuke Okada",
        "Yui Furukawa",
        "Kyosuke Bunji"
      ],
      "abstract": "Human self-report questionnaires are increasingly used in NLP to benchmark and audit large language models (LLMs), from persona consistency to safety and bias assessments. Yet these instruments presume honest responding; in evaluative contexts, LLMs can instead gravitate toward socially preferred answers-a form of socially desirable responding (SDR)-biasing questionnaire-derived scores and downstream conclusions. We propose a psychometric framework to quantify and mitigate SDR in questionnaire-based evaluation of LLMs. To quantify SDR, the same inventory is administered under HONEST versus FAKE-GOOD instructions, and SDR is computed as a direction-corrected standardized effect size from item response theory (IRT)-estimated latent scores. This enables comparisons across constructs and response formats, as well as against human instructed-faking benchmarks. For mitigation, we construct a graded forced-choice (GFC) Big Five inventory by selecting 30 cross-domain pairs from an item pool via constrained optimization to match desirability. Across nine instruction-tuned LLMs evaluated on synthetic personas with known target profiles, Likert-style questionnaires show consistently large SDR, whereas desirability-matched GFC substantially attenuates SDR while largely preserving the recovery of the intended persona profiles. These results highlight a model-dependent SDR-recovery trade-off and motivate SDR-aware reporting practices for questionnaire-based benchmarking and auditing of LLMs.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "cs.CL",
        "stat.ME"
      ],
      "primaryCategory": "cs.CL",
      "pdfUrl": "https://arxiv.org/pdf/2602.17262v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17262v1",
      "doi": null
    },
    {
      "id": "2602.17261",
      "title": "Parametric or nonparametric: the FIC approach for stationary time series",
      "authors": [
        "Gudmund Hermansen",
        "Nils Lid Hjort",
        "Martin Jullum"
      ],
      "abstract": "We seek to narrow the gap between parametric and nonparametric modelling of stationary time series processes. The approach is inspired by recent advances in focused inference and model selection techniques. The paper generalises and extends recent work by developing a new version of the focused information criterion (FIC), directly comparing the performance of parametric time series models with a nonparametric alternative. For a pre-specified focused parameter, for which scrutiny is considered valuable, this is achieved by comparing the mean squared error of the model-based estimators of this quantity. In particular, this yields FIC formulae for covariances or correlations at specified lags, for the probability of reaching a threshold, etc. Suitable weighted average versions, the AFIC, also lead to model selection strategies for finding the best model for the purpose of estimating e.g.~a sequence of correlations.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.17261v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17261v1",
      "doi": null
    },
    {
      "id": "2602.17255",
      "title": "Selection and Collider Restriction Bias Due to Predictor Availability in Prognostic Models",
      "authors": [
        "Marc Delord"
      ],
      "abstract": "This methodological note investigates and discuss possible selection and collider restriction bias due to predictor availability in prognostic models.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.ME",
        "stat.AP"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.17255v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17255v1",
      "doi": null
    },
    {
      "id": "2602.17225",
      "title": "Wide-Surface Furnace for In Situ X-Ray Diffraction of Combinatorial Samples using a High-Throughput Approach",
      "authors": [
        "Giulio Cordaro",
        "Juande Sirvent",
        "Cristian Mocuta",
        "Fjorelo Buzi",
        "Thierry Martin",
        "Federico Baiutti",
        "Alex Morata",
        "Albert Tarancòn",
        "Dominique Thiaudière",
        "Guilhem Dezanneau"
      ],
      "abstract": "The combinatorial approach applied to functional oxides has enabled the production of material libraries that formally contain infinite compositions. A complete ternary diagram can be obtained by pulsed laser deposition (PLD) on 100 mm silicon wafers. However, interest in such materials libraries is only meaningful if high-throughput characterization enables the information extraction from the as-deposited library in a reasonable time. While much commercial equipment allows for XY-resolved characterization at room temperature, very few sample holders have been made available to investigate structural, chemical, and functional properties at high temperatures in controlled atmospheres. In the present work, we present a furnace that enables the study of 100 mm wafers as a function of temperature. This furnace has a dome to control the atmosphere, typically varying from nitrogen gas to pure oxygen atmosphere with external control. We present the design of such a furnace and an example of X-ray diffraction (XRD) and fluorescence (XRF) measurements performed at the DiffAbs beamline of the SOLEIL synchrotron. We apply this high-throughput approach to a combinatorial library up to 735 {\\textdegree}C in nitrogen and calculate the thermal expansion coefficients (TEC) of the ternary system using custom-made MATLAB codes. The TEC analysis revealed the potential limitations of Vegard's law in predicting lattice variations for high-entropy materials.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "cond-mat.mtrl-sci",
        "physics.data-an",
        "stat.ME"
      ],
      "primaryCategory": "cond-mat.mtrl-sci",
      "pdfUrl": "https://arxiv.org/pdf/2602.17225v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17225v1",
      "doi": null
    },
    {
      "id": "2602.17161",
      "title": "Dynamic likelihood hazard rate estimation",
      "authors": [
        "Nils Lid Hjort"
      ],
      "abstract": "The best known methods for estimating hazard rate functions in survival analysis models are either purely parametric or purely nonparametric. The parametric ones are sometimes too biased while the nonparametric ones are sometimes too variable. In the present paper a certain semiparametric approach to hazard rate estimation, proposed in Hjort (1991), is developed further, aiming to combine parametric and nonparametric features. It uses a dynamic local likelihood approach to fit the locally most suitable member in a given parametric class of hazard rates, and amounts to a version of nonparametric parameter smoothing within the parametric class. Thus the parametric hazard rate estimate at time $s$ inserts a parameter estimate that also depends on $s$. We study bias and variance properties of the resulting estimator and methods for choosing the local smoothing parameter. It is shown that dynamic likelihood estimation often leads to better performance than the purely nonparametric methods, while also having capacity for not losing much to the parametric methods in cases where the model being smoothed is adequate.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.17161v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17161v1",
      "doi": null
    },
    {
      "id": "2602.17079",
      "title": "Environmental policy in the context of complex systems: Statistical optimization and sensitivity analysis for ABMs",
      "authors": [
        "Dylan Munson",
        "Arijit Dey",
        "Simon Mak"
      ],
      "abstract": "Coupled human-environment systems are increasingly being understood as complex adaptive systems (CAS), in which micro-level interactions between components lead to emergent behavior. Agent-based models (ABMs) hold great promise for environmental policy design by capturing such complex behavior, enabling a sophisticated understanding of potential interventions. One limitation, however, is that ABMs can be computationally costly to simulate, which hinders their use for policy optimization. To address this, we propose a new statistical framework that exploits machine learning techniques to accelerate policy optimization with costly ABMs. We first develop a statistical approach for sensitivity testing of the optimal policy, then leverage a reinforcement learning method for efficient policy optimization. We test this framework on the classic ``Sugarscape'' model, an ABM for resource harvesting. We show that our approach can quickly identify optimal and interpretable policies that improve upon baseline techniques, with insightful sensitivity and dynamic analyses that connect back to economic theory.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.AP"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.17079v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17079v1",
      "doi": null
    },
    {
      "id": "2602.17070",
      "title": "General sample size analysis for probabilities of causation: a delta method approach",
      "authors": [
        "Tianyuan Cheng",
        "Ruirui Mao",
        "Judea Pearl",
        "Ang Li"
      ],
      "abstract": "Probabilities of causation (PoCs), such as the probability of necessity and sufficiency (PNS), are important tools for decision making but are generally not point identifiable. Existing work has derived bounds for these quantities using combinations of experimental and observational data. However, there is very limited research on sample size analysis, namely, how many experimental and observational samples are required to achieve a desired margin of error. In this paper, we propose a general sample size framework based on the delta method. Our approach applies to settings in which the target bounds of PoCs can be expressed as finite minima or maxima of linear combinations of experimental and observational probabilities. Through simulation studies, we demonstrate that the proposed sample size calculations lead to stable estimation of these bounds.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.ME",
        "cs.AI"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.17070v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17070v1",
      "doi": null
    },
    {
      "id": "2602.17052",
      "title": "Generative modeling for the bootstrap",
      "authors": [
        "Leon Tran",
        "Ting Ye",
        "Peng Ding",
        "Fang Han"
      ],
      "abstract": "Generative modeling builds on and substantially advances the classical idea of simulating synthetic data from observed samples. This paper shows that this principle is not only natural but also theoretically well-founded for bootstrap inference: it yields statistically valid confidence intervals that apply simultaneously to both regular and irregular estimators, including settings in which Efron's bootstrap fails. In this sense, the generative modeling-based bootstrap can be viewed as a modern version of the smoothed bootstrap: it could mitigate the curse of dimensionality and remain effective in challenging regimes where estimators may lack root-$n$ consistency or a Gaussian limit.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.ME",
        "econ.EM"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.17052v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17052v1",
      "doi": null
    },
    {
      "id": "2602.17043",
      "title": "Quantifying the limits of human athletic performance: A Bayesian analysis of elite decathletes",
      "authors": [
        "Paul-Hieu V. Nguyen",
        "James M. Smoliga",
        "Benton Lindaman",
        "Sameer K. Deshpande"
      ],
      "abstract": "Because the decathlon tests many facets of athleticism, including sprinting, throwing, jumping, and endurance, many consider it to be the ultimate test of athletic ability. On this view, estimating the maximal decathlon score and understanding what it would take to achieve that score provides insight into the upper limits of human athletic potential. To this end, we develop a Bayesian composition model for forecasting how individual athletes perform in each of the 10 decathlon events of time. Besides capturing potential non-linear temporal trends in performance, our model carefully captures the dependence between performance in an event and all preceding events. Using our model, we can simulate and evaluate the distribution of the maximal possible scores and identify profiles of athletes who could realistically attain scores approaching this limit.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.AP"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.17043v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17043v1",
      "doi": null
    },
    {
      "id": "2602.17041",
      "title": "Reframing Population-Adjusted Indirect Comparisons as a Transportability Problem: An Estimand-Based Perspective and Implications for Health Technology Assessment",
      "authors": [
        "Conor Chandler",
        "Jack Ishak"
      ],
      "abstract": "Population-adjusted indirect comparisons (PAICs) are widely used to synthesize evidence when randomized controlled trials enroll different patient populations and head-to-head comparisons are unavailable. Although PAICs adjust for observed population differences across trials, adjustment alone does not ensure transportability of estimated effects to decision-relevant populations for health technology assessment (HTA). We examine and formalize transportability in PAICs from an estimand-based perspective. We distinguish conditional and marginal treatment effect estimands and show how transportability depends on effect modification, collapsibility, and alignment between the scale of effect modification and the effect measure. Using illustrative examples, we demonstrate that even when effect modifiers are shared across treatments, marginal effects are generally population-dependent for commonly used non-collapsible measures, including hazard ratios and odds ratios. Conversely, collapsible and conditional effects defined on the linear predictor scale exhibit more favorable transportability properties. We further show that pairwise PAIC approaches typically identify effects defined in the comparator population and that applying these estimates to other populations entails an additional, often implicit, transport step requiring further assumptions. This has direct implications for HTA, where PAIC-derived effects are routinely applied within cost-effectiveness and decision models defined for different target populations. Our results clarify when applying PAIC-derived treatment effects to desired target populations is justified, when doing so requires additional assumptions, and when results should instead be interpreted as population-specific rather than decision-relevant, supporting more transparent and principled use of indirect evidence in HTA and related decision-making contexts.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.17041v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17041v1",
      "doi": null
    },
    {
      "id": "2602.17034",
      "title": "Using Time Series Measures to Explore Family Planning Survey Data and Model-based Estimates",
      "authors": [
        "Oluwayomi Akinfenwa",
        "Niamh Cahill",
        "Catherine Hurley"
      ],
      "abstract": "Family planning is a global development priority and a key indicator of reproductive health. Monitoring progress is challenged by gaps in survey data across countries. The United Nations Population Division addresses this with the Family Planning Estimation Model (FPEM), a Bayesian hierarchical time series model producing annual estimates of modern contraceptive use while sharing information across countries and regions. This paper evaluates how well FPEM estimates align with survey data using time series diagnostic indices from the wdiexplorer R package, which account for countries nested within sub-regions. Visualisation of survey data, modelled trajectories, and diagnostics enables assessment of model performance, highlighting where trends align and where discrepancies occur.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.AP"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.17034v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17034v1",
      "doi": null
    },
    {
      "id": "2602.16992",
      "title": "Modeling Multivariate Missingness with Tree Graphs and Conjugate Odds",
      "authors": [
        "Daniel Suen",
        "Yen-Chi Chen"
      ],
      "abstract": "In this paper, we analyze a specific class of missing not at random (MNAR) assumptions called tree graphs, extending upon the work of pattern graphs. We build off previous work by introducing the idea of a conjugate odds family in which certain parametric models on the selection odds can preserve the data distribution family across all missing data patterns. Under a conjugate odds family and a tree graph assumption, we are able to model the full data distribution elegantly in the sense that for the observed data, we obtain a model that is conjugate from the complete-data, and for the missing entries, we create a simple imputation model. In addition, we investigate the problem of graph selection, sensitivity analysis, and statistical inference. Using both simulations and real data, we illustrate the applicability of our method.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.16992v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16992v1",
      "doi": null
    },
    {
      "id": "2602.16970",
      "title": "Temperature and Respiratory Emergency Department Visits: A Mediation Analysis with Ambient Ozone Exposure",
      "authors": [
        "Chen Li",
        "Thomas W. Hsiao",
        "Stefanie Ebelt",
        "Rebecca H. Zhang",
        "Howard H. Chang"
      ],
      "abstract": "High temperatures are associated with adverse respiratory health outcomes and increases in ambient air pollution. Limited research has quantified air pollution's mediating role in the relationship between temperature and respiratory morbidity, such as emergency department (ED) visits. In this study, we conducted a causal mediation analysis to decompose the total effect of daily temperature on respiratory ED visits in Los Angeles from 2005 to 2016. We focused on ambient ozone as a mediator because its precursors and formation are directly driven by sunlight and temperature. We estimated natural direct, indirect, and total effects on the relative risk scale across deciles of temperature exposure compared to the median. We utilized Bayesian additive regression trees (BART) to flexibly characterize the nonlinear relationship between temperature and ozone and quantified uncertainty via posterior prediction and the Bayesian bootstrap. Our results showed that ozone partially mediated the association between high temperatures and respiratory ED visits, particularly at moderately high temperatures. We also validated our modeling approach through simulation studies. This study extends the existing literature by considering acute respiratory morbidity and employing a flexible modeling approach, offering new insights into the mechanisms underlying temperature-related health risks.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.AP"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.16970v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16970v1",
      "doi": null
    },
    {
      "id": "2602.16933",
      "title": "M-estimation under Two-Phase Multiwave Sampling with Applications to Prediction-Powered Inference",
      "authors": [
        "Dan M. Kluger",
        "Stephen Bates"
      ],
      "abstract": "In two-phase multiwave sampling, inexpensive measurements are collected on a large sample and expensive, more informative measurements are adaptively obtained on subsets of units across multiple waves. Adaptively collecting the expensive measurements can increase efficiency but complicates statistical inference. We give valid estimators and confidence intervals for M-estimation under adaptive two-phase multiwave sampling. We focus on the case where proxies for the expensive variables -- such as predictions from pretrained machine learning models -- are available for all units and propose a Multiwave Predict-Then-Debias estimator that combines proxy information with the expensive, higher-quality measurements to improve efficiency while removing bias. We establish asymptotic linearity and normality and propose asymptotically valid confidence intervals. We also develop an approximately greedy sampling strategy that improves efficiency relative to uniform sampling. Data-based simulation studies support the theoretical results and demonstrate efficiency gains.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.ML"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.16933v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16933v1",
      "doi": null
    },
    {
      "id": "2602.16914",
      "title": "A statistical perspective on transformers for small longitudinal cohort data",
      "authors": [
        "Kiana Farhadyar",
        "Maren Hackenberg",
        "Kira Ahrens",
        "Charlotte Schenk",
        "Bianca Kollmann",
        "Oliver Tüscher",
        "Klaus Lieb",
        "Michael M. Plichta",
        "Andreas Reif",
        "Raffael Kalisch",
        "Martin Wolkewitz",
        "Moritz Hess",
        "Harald Binder"
      ],
      "abstract": "Modeling of longitudinal cohort data typically involves complex temporal dependencies between multiple variables. There, the transformer architecture, which has been highly successful in language and vision applications, allows us to account for the fact that the most recently observed time points in an individual's history may not always be the most important for the immediate future. This is achieved by assigning attention weights to observations of an individual based on a transformation of their values. One reason why these ideas have not yet been fully leveraged for longitudinal cohort data is that typically, large datasets are required. Therefore, we present a simplified transformer architecture that retains the core attention mechanism while reducing the number of parameters to be estimated, to be more suitable for small datasets with few time points. Guided by a statistical perspective on transformers, we use an autoregressive model as a starting point and incorporate attention as a kernel-based operation with temporal decay, where aggregation of multiple transformer heads, i.e. different candidate weighting schemes, is expressed as accumulating evidence on different types of underlying characteristics of individuals. This also enables a permutation-based statistical testing procedure for identifying contextual patterns. In a simulation study, the approach is shown to recover contextual dependencies even with a small number of individuals and time points. In an application to data from a resilience study, we identify temporal patterns in the dynamics of stress and mental health. This indicates that properly adapted transformers can not only achieve competitive predictive performance, but also uncover complex context dependencies in small data settings.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "stat.ME",
        "cs.LG",
        "stat.ML"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.16914v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16914v1",
      "doi": null
    },
    {
      "id": "2602.16830",
      "title": "The Impact of Formations on Football Matches Using Double Machine Learning. Is it worth parking the bus?",
      "authors": [
        "Genís Ruiz-Menárguez",
        "Llorenç Badiella"
      ],
      "abstract": "This study addresses a central tactical dilemma for football coaches: whether to employ a defensive strategy, colloquially known as \"parking the bus\", or a more offensive one. Using an advanced Double Machine Learning (DML) framework, this project provides a robust and interpretable tool to estimate the causal impact of different formations on key match outcomes such as goal difference, possession, corners, and disciplinary actions. Leveraging a dataset of over 22,000 matches from top European leagues, formations were categorized into six representative types based on tactical structure and expert consultation. A major methodological contribution lies in the adaptation of DML to handle categorical treatments, specifically formation combinations, through a novel matrix-based residualization process, allowing for a detailed estimation of formation-versus-formation effects that can inform a coach's tactical decision-making. Results show that while offensive formations like 4-3-3 and 4-2-3-1 offer modest statistical advantages in possession and corners, their impact on goals is limited. Furthermore, no evidence supports the idea that defensive formations, commonly associated with parking the bus, increase a team's winning potential. Additionally, red cards appear unaffected by formation choice, suggesting other behavioral factors dominate. Although this approach does not fully capture all aspects of playing style or team strength, it provides a valuable framework for coaches to analyze tactical efficiency and sets a precedent for future research in sports analytics.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "stat.AP",
        "cs.LG"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.16830v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16830v1",
      "doi": null
    },
    {
      "id": "2602.16789",
      "title": "First versus full or first versus last: U-statistic change-point tests under fixed and local alternatives",
      "authors": [
        "Herold Dehling",
        "Daniel Vogel",
        "Martin Wendler"
      ],
      "abstract": "The use of U-statistics in the change-point context has received considerable attention in the literature. We compare two approaches of constructing CUSUM-type change-point tests, which we call the first-vs-full and first-vs-last approach. Both have been pursued by different authors. The question naturally arises if the two tests substantially differ and, if so, which of them is better in which data situation. In large samples, both tests are similar: they are asymptotically equivalent under the null hypothesis and under sequences of local alternatives. In small samples, there may be quite noticeable differences, which is in line with a different asymptotic behavior under fixed alternatives. We derive a simple criterion for deciding which test is more powerful. We examine the examples Gini's mean difference, the sample variance, and Kendall's tau in detail. Particularly, when testing for changes in scale by Gini's mean difference, we show that the first-vs-full approach has a higher power if and only if the scale changes from a smaller to a larger value -- regardless of the population distribution or the location of the change. The asymptotic derivations are under weak dependence. The results are illustrated by numerical simulations and data examples.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.16789v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16789v1",
      "doi": null
    },
    {
      "id": "2602.16784",
      "title": "Omitted Variable Bias in Language Models Under Distribution Shift",
      "authors": [
        "Victoria Lin",
        "Louis-Philippe Morency",
        "Eli Ben-Michael"
      ],
      "abstract": "Despite their impressive performance on a wide variety of tasks, modern language models remain susceptible to distribution shifts, exhibiting brittle behavior when evaluated on data that differs in distribution from their training data. In this paper, we describe how distribution shifts in language models can be separated into observable and unobservable components, and we discuss how established approaches for dealing with distribution shift address only the former. Importantly, we identify that the resulting omitted variable bias from unobserved variables can compromise both evaluation and optimization in language models. To address this challenge, we introduce a framework that maps the strength of the omitted variables to bounds on the worst-case generalization performance of language models under distribution shift. In empirical experiments, we show that using these bounds directly in language model evaluation and optimization provides more principled measures of out-of-distribution performance, improves true out-of-distribution performance relative to standard distribution shift adjustment methods, and further enables inference about the strength of the omitted variables when target distribution labels are available.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "cs.LG",
        "cs.CL",
        "stat.ME"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2602.16784v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16784v1",
      "doi": null
    },
    {
      "id": "2602.16709",
      "title": "Knowledge-Embedded Latent Projection for Robust Representation Learning",
      "authors": [
        "Weijing Tang",
        "Ming Yuan",
        "Zongqi Xia",
        "Tianxi Cai"
      ],
      "abstract": "Latent space models are widely used for analyzing high-dimensional discrete data matrices, such as patient-feature matrices in electronic health records (EHRs), by capturing complex dependence structures through low-dimensional embeddings. However, estimation becomes challenging in the imbalanced regime, where one matrix dimension is much larger than the other. In EHR applications, cohort sizes are often limited by disease prevalence or data availability, whereas the feature space remains extremely large due to the breadth of medical coding system. Motivated by the increasing availability of external semantic embeddings, such as pre-trained embeddings of clinical concepts in EHRs, we propose a knowledge-embedded latent projection model that leverages semantic side information to regularize representation learning. Specifically, we model column embeddings as smooth functions of semantic embeddings via a mapping in a reproducing kernel Hilbert space. We develop a computationally efficient two-step estimation procedure that combines semantically guided subspace construction via kernel principal component analysis with scalable projected gradient descent. We establish estimation error bounds that characterize the trade-off between statistical error and approximation error induced by the kernel projection. Furthermore, we provide local convergence guarantees for our non-convex optimization procedure. Extensive simulation studies and a real-world EHR application demonstrate the effectiveness of the proposed method.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "cs.LG",
        "math.ST",
        "stat.ME"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2602.16709v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16709v1",
      "doi": null
    },
    {
      "id": "2602.16696",
      "title": "Parameter-free representations outperform single-cell foundation models on downstream benchmarks",
      "authors": [
        "Huan Souza",
        "Pankaj Mehta"
      ],
      "abstract": "Single-cell RNA sequencing (scRNA-seq) data exhibit strong and reproducible statistical structure. This has motivated the development of large-scale foundation models, such as TranscriptFormer, that use transformer-based architectures to learn a generative model for gene expression by embedding genes into a latent vector space. These embeddings have been used to obtain state-of-the-art (SOTA) performance on downstream tasks such as cell-type classification, disease-state prediction, and cross-species learning. Here, we ask whether similar performance can be achieved without utilizing computationally intensive deep learning-based representations. Using simple, interpretable pipelines that rely on careful normalization and linear methods, we obtain SOTA or near SOTA performance across multiple benchmarks commonly used to evaluate single-cell foundation models, including outperforming foundation models on out-of-distribution tasks involving novel cell types and organisms absent from the training data. Our findings highlight the need for rigorous benchmarking and suggest that the biology of cell identity can be captured by simple linear representations of single cell gene expression data.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "q-bio.GN",
        "cs.LG",
        "q-bio.QM"
      ],
      "primaryCategory": "q-bio.GN",
      "pdfUrl": "https://arxiv.org/pdf/2602.16696v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16696v1",
      "doi": null
    },
    {
      "id": "2602.16690",
      "title": "Synthetic-Powered Multiple Testing with FDR Control",
      "authors": [
        "Yonghoon Lee",
        "Meshi Bashari",
        "Edgar Dobriban",
        "Yaniv Romano"
      ],
      "abstract": "Multiple hypothesis testing with false discovery rate (FDR) control is a fundamental problem in statistical inference, with broad applications in genomics, drug screening, and outlier detection. In many such settings, researchers may have access not only to real experimental observations but also to auxiliary or synthetic data -- from past, related experiments or generated by generative models -- that can provide additional evidence about the hypotheses of interest. We introduce SynthBH, a synthetic-powered multiple testing procedure that safely leverages such synthetic data. We prove that SynthBH guarantees finite-sample, distribution-free FDR control under a mild PRDS-type positive dependence condition, without requiring the pooled-data p-values to be valid under the null. The proposed method adapts to the (unknown) quality of the synthetic data: it enhances the sample efficiency and may boost the power when synthetic data are of high quality, while controlling the FDR at a user-specified level regardless of their quality. We demonstrate the empirical performance of SynthBH on tabular outlier detection benchmarks and on genomic analyses of drug-cancer sensitivity associations, and further study its properties through controlled experiments on simulated data.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "stat.ME",
        "cs.LG",
        "stat.ML"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.16690v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16690v1",
      "doi": null
    },
    {
      "id": "2602.16616",
      "title": "Design and Analysis Strategies for Pooling in High Throughput Screening: Application to the Search for a New Anti-Microbial",
      "authors": [
        "Byran Smucker",
        "Benjamin Brennan",
        "Emily Rego",
        "Meng Wu",
        "Zhihong Lin",
        "Brian Ahmer",
        "Blake Peterson"
      ],
      "abstract": "A major public health issue is the growing resistance of bacteria to antibiotics. An important part of the needed response is the discovery and development of new antimicrobial strategies. These require the screening of potential new drugs, typically accomplished using high-throughput screening (HTS). Traditionally, HTS is performed by examining one compound per well, but a more efficient strategy pools multiple compounds per well. In this work, we study several recently proposed pooling construction methods, as well as a variety of pooled high-throughput screening analysis methods, in order to provide guidance to practitioners on which methods to use. This is done in the context of an application of the methods to the search for new drugs to combat bacterial infection. We discuss both an extensive pilot study as well as a small screening campaign, and highlight both the successes and challenges of the pooling approach.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "stat.AP"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.16616v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16616v1",
      "doi": null
    },
    {
      "id": "2602.16606",
      "title": "On Sharpened Convergence Rate of Generalized Sliced Inverse Regression for Nonlinear Sufficient Dimension Reduction",
      "authors": [
        "Chak Fung Choi",
        "Yin Tang",
        "Bing Li"
      ],
      "abstract": "Generalized Sliced Inverse Regression (GSIR) is one of the most important methods for nonlinear sufficient dimension reduction. As shown in Li and Song (2017), it enjoys a convergence rate that is independent of the dimension of the predictor, thus avoiding the curse of dimensionality. In this paper we establish an improved convergence rate of GSIR under additional mild eigenvalue decay rate and smoothness conditions. Our convergence rate can be made arbitrarily close to $n^{-1/3}$ under appropriate decay rate and smoothness parameters. As a comparison, the rate of Li and Song (2017) is $n^{-1/4}$ under the best conditions. This improvement is significant because, for example, in a semiparametric estimation problem involving an infinite-dimensional nuisance parameter, the convergence rate of the estimator of the nuisance parameter is often required to be faster than $n^{-1/4}$ to guarantee desired semiparametric properties such as asymptotic efficiency. This can be achieved by the improved convergence rate, but not by the original rate. The sharpened convergence rate can also be established for GSIR in more general settings, such as functional sufficient dimension reduction.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "math.ST",
        "stat.ME"
      ],
      "primaryCategory": "math.ST",
      "pdfUrl": "https://arxiv.org/pdf/2602.16606v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16606v1",
      "doi": null
    },
    {
      "id": "2602.16583",
      "title": "Physical Activity Trajectories Preceding Incident Major Depressive Disorder Diagnosis Using Consumer Wearable Devices in the All of Us Research Program: Case-Control Study",
      "authors": [
        "Yuezhou Zhang",
        "Amos Folarin",
        "Hugh Logan Ellis",
        "Rongrong Zhong",
        "Callum Stewart",
        "Heet Sankesara",
        "Hyunju Kim",
        "Shaoxiong Sun",
        "Abhishek Pratap",
        "Richard JB Dobson"
      ],
      "abstract": "Low physical activity is a known risk factor for major depressive disorder (MDD), but changes in activity before a first clinical diagnosis remain unclear, especially using long-term objective measurements. This study characterized trajectories of wearable-measured physical activity during the year preceding incident MDD diagnosis. We conducted a retrospective nested case-control study using linked electronic health record and Fitbit data from the All of Us Research Program. Adults with at least 6 months of valid wearable data in the year before diagnosis were eligible. Incident MDD cases were matched to controls on age, sex, body mass index, and index time (up to four controls per case). Daily step counts and moderate-to-vigorous physical activity (MVPA) were aggregated into monthly averages. Linear mixed-effects models compared trajectories from 12 months before diagnosis to diagnosis. Within cases, contrasts identified when activity first significantly deviated from levels 12 months prior. The cohort included 4,104 participants (829 cases and 3,275 controls; 81.7% women; median age 48.4 years). Compared with controls, cases showed consistently lower activity and significant downward trajectories in both step counts and MVPA during the year before diagnosis (P < 0.001). Significant declines appeared about 4 months before diagnosis for step counts and 5 months for MVPA. Exploratory analyses suggested subgroup differences, including steeper declines in men, greater intensity reductions at older ages, and persistently low activity among individuals with obesity. Sustained within-person declines in physical activity emerged months before incident MDD diagnosis. Longitudinal wearable monitoring may provide early signals to support risk stratification and earlier intervention.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "stat.AP"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.16583v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16583v1",
      "doi": null
    },
    {
      "id": "2602.16567",
      "title": "Scattering and sputtering on the lunar surface; Insights from negative ions observed at the surface",
      "authors": [
        "Romain Canu-Blot",
        "Martin Wieser",
        "Umberto Rollero",
        "Thomas Maynadié",
        "Stas Barabash",
        "Gabriella Stenberg Wieser",
        "Aibing Zhang",
        "Wenjing Wang",
        "Chi Wang"
      ],
      "abstract": "Context. Airless planetary bodies are directly exposed to solar wind ions, which can scatter or become implanted upon impact with the regolith-covered surface, while also sputtering surface atoms. Aims. We construct a semi-analytical model for the scattering of ions of hundreds of eV and the sputtering of surface atoms, both resulting in the emission of negative ions from the lunar surface. Our model contains a novel description of the scattering process that is physics-based and constrained by observations. Methods. We use data from the Negative Ions at the Lunar Surface (NILS) instrument on the Chang'e-6 lander to update prior knowledge of ion scattering and sputtering from lunar regolith through Bayesian inference. Results. Our model shows good agreement with the NILS data. A precipitating solar wind proton has roughly a 22% chance of scattering from the lunar surface in any charge state, and about an 8% chance of sputtering a surface hydrogen atom. The resulting ratio of scattered to sputtered hydrogen flux is eta_sc / eta_sp = 1.5 for a proton speed of 300 km/s. We find a high probability (7-20%) that a hydrogen atom leaves the surface negatively charged. The angular emission distributions at near-grazing angles for both scattered and sputtered fluxes are controlled by surface roughness. Our model also indicates significant inelastic energy losses for hydrogen interacting with the regolith, suggesting a longer effective path length than previously assumed. Finally, we estimate a surface binding energy of 5.5 eV, consistent with the observations. Conclusions. Our model describes the scattering and sputtering of particles of any charge state from any homogeneous, multi-species surface. Using NILS data, we successfully applied the model to update our understanding of solar wind interacting with lunar regolith, and the emission of negative hydrogen ions.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "physics.space-ph",
        "physics.atom-ph",
        "physics.ins-det",
        "stat.AP"
      ],
      "primaryCategory": "physics.space-ph",
      "pdfUrl": "https://arxiv.org/pdf/2602.16567v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16567v1",
      "doi": null
    },
    {
      "id": "2602.16540",
      "title": "Generalised Linear Models Driven by Latent Processes: Asymptotic Theory and Applications",
      "authors": [
        "Wagner Barreto-Souza",
        "Ngai Hang Chan"
      ],
      "abstract": "This paper introduces a class of generalised linear models (GLMs) driven by latent processes for modelling count, real-valued, binary, and positive continuous time series. Extending earlier latent-process regression frameworks based on Poisson or one-parameter exponential family assumptions, we allow the conditional distribution of the response to belong to a bi-parameter exponential family, with the latent process entering the conditional mean multiplicatively. This formulation substantially broadens the scope of latent-process GLMs, for instance, it naturally accommodates gamma responses for positive continuous data, enables estimation of an unknown dispersion parameter via method of moments, and avoids restrictive conditions on link functions that arise under existing formulations. We establish the asymptotic normality of the GLM estimators obtained from the GLM likelihood that ignores the latent process, and we derive the correct information matrix for valid inference. In addition, we provide a principled approach to prediction and forecasting in GLMs driven by latent processes, a topic not previously addressed in the literature. We present two real data applications on measles infections in North Rhine-Westphalia (Germany) and paleoclimatic glacial varves, which highlight the practical advantages and enhanced flexibility of the proposed modelling framework.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.16540v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16540v1",
      "doi": null
    },
    {
      "id": "2602.16527",
      "title": "Model selection confidence sets for time series models with applications to electricity load data",
      "authors": [
        "Piersilvio De Bortoli",
        "Davide Ferrari",
        "Francesco Ravazzolo",
        "Luca Rossini"
      ],
      "abstract": "This paper studies the Model Selection Confidence Set (MSCS) methodology for univariate time series models involving autoregressive and moving average components, and applies it to study model selection uncertainty in the Italian electricity load data. Rather than relying on a single model selected by an arbitrary criterion, the MSCS identifies a set of models that are statistically indistinguishable from the true data-generating process at a given confidence level. The size and composition of this set reveal crucial information about model selection uncertainty: noisy data scenarios produce larger sets with many candidate models, while more informative cases narrow the set considerably. To study the importance of each model term, we consider numerical statistics measuring the frequency with which each term is included in both the entire MSCS and in Lower Boundary Models (LBM), its most parsimonious specifications. Applied to Italian hourly electricity load data, the MSCS methodology reveals marked intraday variation in model selection uncertainty and isolates a collection of model specifications that deliver competitive short-term forecasts while highlighting key drivers of electricity load like intraday hourly lags, temperature, calendar effects and solar energy generation.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "econ.EM"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.16527v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16527v1",
      "doi": null
    },
    {
      "id": "2602.16504",
      "title": "GRIMM: Genetic stRatification for Inference in Molecular Modeling",
      "authors": [
        "Ashley Babjac",
        "Adrienne Hoarfrost"
      ],
      "abstract": "The vast majority of biological sequences encode unknown functions and bear little resemblance to experimentally characterized proteins, limiting both our understanding of biology and our ability to harness functional potential for the bioeconomy. Predicting enzyme function from sequence remains a central challenge in computational biology, complicated by low sequence diversity and imbalanced label support in publicly available datasets. Models trained on these data can overestimate performance and fail to generalize. To address this, we introduce GRIMM (Genetic stRatification for Inference in Molecular Modeling), a benchmark for enzyme function prediction that employs genetic stratification: sequences are clustered by similarity and clusters are assigned exclusively to training, validation, or test sets. This ensures that sequences from the same cluster do not appear in multiple partitions. GRIMM produces multiple test sets: a closed-set test with the same label distribution as training (Test-1) and an open-set test containing novel labels (Test-2), serving as a realistic out-of-distribution proxy for discovering novel enzyme functions. While demonstrated on enzymes, this approach is generalizable to any sequence-based classification task where inputs can be clustered by similarity. By formalizing a splitting strategy often used implicitly, GRIMM provides a unified and reproducible framework for closed- and open-set evaluation. The method is lightweight, requiring only sequence clustering and label annotations, and can be adapted to different similarity thresholds, data scales, and biological tasks. GRIMM enables more realistic evaluation of functional prediction models on both familiar and unseen classes and establishes a benchmark that more faithfully assesses model performance and generalizability.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "q-bio.QM"
      ],
      "primaryCategory": "q-bio.QM",
      "pdfUrl": "https://arxiv.org/pdf/2602.16504v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16504v1",
      "doi": null
    },
    {
      "id": "2602.16497",
      "title": "Factor-Adjusted Multiple Testing for High-Dimensional Individual Mediation Effects",
      "authors": [
        "Chen Shi",
        "Zhao Chen",
        "Christina Dan Wang"
      ],
      "abstract": "Identifying individual mediators is a central goal of high-dimensional mediation analysis, yet pervasive dependence among mediators can invalidate standard debiased inference and lead to substantial false discovery rate (FDR) inflation. We propose a Factor-Adjusted Debiased Mediation Testing (FADMT) framework that enables large-scale inference for individual mediation effects with FDR control under complex dependence structures. Our approach posits an approximate factor structure on the unobserved errors of the mediator model, extracts common latent factors, and constructs decorrelated pseudo-mediators for the subsequent inferential procedure. We establish the asymptotic normality of the debiased estimator and develop a multiple testing procedure with theoretical FDR control under mild high-dimensional conditions. By adjusting for latent factor induced dependence, FADMT also improves robustness to spurious associations driven by shared latent variation in observational studies. Extensive simulations demonstrate the superior finite-sample performance across a wide range of correlation structures. Applications to TCGA-BRCA multi-omics data and to China's stock connect study further illustrate the practical utility of the proposed method.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.16497v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16497v1",
      "doi": null
    },
    {
      "id": "2602.16463",
      "title": "Focused Relative Risk Information Criterion for Variable Selection in Linear Regression",
      "authors": [
        "Nils Lid Hjort"
      ],
      "abstract": "This paper motivates and develops a novel and focused approach to variable selection in linear regression models. For estimating the regression mean $μ=\\E\\,(Y\\midd x_0)$, for the covariate vector of a given individual, there is a list of competing estimators, say $\\hattμ_S$ for each submodel $S$. Exact expressions are found for the relative mean squared error risks, when compared to the widest model available, say $\\mse_S/\\mse_\\wide$. The theory of confidence distributions is used for accurate assessments of these relative risks. This leads to certain Focused Relative Risk Information Criterion scores, and associated FRIC plots and FRIC tables, as well as to Confidence plots to exhibit the confidence the data give in the submodels. The machinery is extended to handle many focus parameters at the same time, with appropriate averaged FRIC scores. The particular case where all available covariate vectors have equal importance yields a new overall criterion for variable selection, balancing complexity and fit in a natural fashion. A connection to the Mallows criterion is demonstrated, leading also to natural modifications of the latter. The FRIC and AFRIC strategies are illustrated for real data.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.16463v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16463v1",
      "doi": null
    },
    {
      "id": "2602.16376",
      "title": "Two-way Clustering Robust Variance Estimator in Quantile Regression Models",
      "authors": [
        "Ulrich Hounyo",
        "Jiahao Lin"
      ],
      "abstract": "We study inference for linear quantile regression with two-way clustered data. Using a separately exchangeable array framework and a projection decomposition of the quantile score, we characterize regime-dependent convergence rates and establish a self-normalized Gaussian approximation. We propose a two-way cluster-robust sandwich variance estimator with a kernel-based density ``bread'' and a projection-matched ``meat'', and prove consistency and validity of inference in Gaussian regimes. We also show an impossibility result for uniform inference in a non-Gaussian interaction regime.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "econ.EM",
        "stat.AP"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.16376v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16376v1",
      "doi": null
    },
    {
      "id": "2602.16357",
      "title": "Optical Inversion and Spectral Unmixing of Spectroscopic Photoacoustic Images with Physics-Informed Neural Networks",
      "authors": [
        "Sarkis Ter Martirosyan",
        "Xinyue Huang",
        "David Qin",
        "Anthony Yu",
        "Stanislav Emelianov"
      ],
      "abstract": "Accurate estimation of the relative concentrations of chromophores in a spectroscopic photoacoustic (sPA) image can reveal immense structural, functional, and molecular information about physiological processes. However, due to nonlinearities and ill-posedness inherent to sPA imaging, concentration estimation is intractable. The Spectroscopic Photoacoustic Optical Inversion Autoencoder (SPOI-AE) aims to address the sPA optical inversion and spectral unmixing problems without assuming linearity. Herein, SPOI-AE was trained and tested on \\textit{in vivo} mouse lymph node sPA images with unknown ground truth chromophore concentrations. SPOI-AE better reconstructs input sPA pixels than conventional algorithms while providing biologically coherent estimates for optical parameters, chromophore concentrations, and the percent oxygen saturation of tissue. SPOI-AE's unmixing accuracy was validated using a simulated mouse lymph node phantom ground truth.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "cs.LG",
        "q-bio.QM"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2602.16357v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16357v1",
      "doi": null
    },
    {
      "id": "2602.16328",
      "title": "A general framework for modeling Gaussian process with qualitative and quantitative factors",
      "authors": [
        "Linsui Deng",
        "C. F. Jeff Wu"
      ],
      "abstract": "Computer experiments involving both qualitative and quantitative (QQ) factors have attracted increasing attention. Gaussian process (GP) models have proven effective in this context by choosing specialized covariance functions for QQ factors. In this work, we extend the latent variable-based GP approach, which maps qualitative factors into a continuous latent space, by establishing a general framework to apply standard kernel functions to continuous latent variables. This approach provides a novel perspective for interpreting some existing GP models for QQ factors and introduces new covariance structures in some situations. The ordinal structure can be incorporated naturally and seamlessly in this framework. Furthermore, the Bayesian information criterion and leave-one-out cross-validation are employed for model selection and model averaging. The performance of the proposed method is comprehensively studied on several examples.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.16328v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16328v1",
      "doi": null
    },
    {
      "id": "2602.16310",
      "title": "Introducing the b-value: combining unbiased and biased estimators from a sensitivity analysis perspective",
      "authors": [
        "Zhexiao Lin",
        "Peter J. Bickel",
        "Peng Ding"
      ],
      "abstract": "In empirical research, when we have multiple estimators for the same parameter of interest, a central question arises: how do we combine unbiased but less precise estimators with biased but more precise ones to improve the inference? Under this setting, the point estimation problem has attracted considerable attention. In this paper, we focus on a less studied inference question: how can we conduct valid statistical inference in such settings with unknown bias? We propose a strategy to combine unbiased and biased estimators from a sensitivity analysis perspective. We derive a sequence of confidence intervals indexed by the magnitude of the bias, which enable researchers to assess how conclusions vary with the bias levels. Importantly, we introduce the notion of the b-value, a critical value of the unknown maximum relative bias at which combining estimators does not yield a significant result. We apply this strategy to three canonical combined estimators: the precision-weighted estimator, the pretest estimator, and the soft-thresholding estimator. For each estimator, we characterize the sequence of confidence intervals and determine the bias threshold at which the conclusion changes. Based on the theory, we recommend reporting the b-value based on the soft-thresholding estimator and its associated confidence intervals, which are robust to unknown bias and achieve the lowest worst-case risk among the alternatives.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "stat.ME",
        "econ.EM",
        "math.ST",
        "stat.AP"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.16310v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16310v1",
      "doi": null
    },
    {
      "id": "2602.16259",
      "title": "HAL-MLE Log-Splines Density Estimation (Part I: Univariate)",
      "authors": [
        "Yilong Hou",
        "Zhengpu Zhao",
        "Yi Li",
        "Mark van der Laan"
      ],
      "abstract": "We study nonparametric maximum likelihood estimation of probability densities under a total variation (TV) type penalty, sectional variation norm (also named as Hardy-Krause variation). TV regularization has a long history in regression and density estimation, including results on $L^2$ and KL divergence convergence rates. Here, we revisit this task using the Highly Adaptive Lasso (HAL) framework. We formulate a HAL-based maximum likelihood estimator (HAL-MLE) using the log-spline link function from \\citet{kooperberg1992logspline}, and show that in the univariate setting the bounded sectional variation norm assumption underlying HAL coincides with the classical bounded TV assumption. This equivalence directly connects HAL-MLE to existing TV-penalized approaches such as local adaptive splines \\citep{mammen1997locally}. We establish three new theoretical results: (i) the univariate HAL-MLE is asymptotically linear, (ii) it admits pointwise asymptotic normality, and (iii) it achieves uniform convergence at rate $n^{-(k+1)/(2k+3)}$ up to logarithmic factors for the smoothness order $k \\geq 1$. These results extend existing results from \\citet{van2017uniform}, which previously guaranteed only uniform consistency without rates when $k=0$. We will include the uniform convergence for general dimension $d$ in the follow-up work of this paper. The intention of this paper is to provide a unified framework for the TV-penalized density estimation methods, and to connect the HAL-MLE to the existing TV-penalized methods in the univariate case, despite that the general HAL-MLE is defined for multivariate cases.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "math.ST",
        "stat.CO",
        "stat.ME"
      ],
      "primaryCategory": "math.ST",
      "pdfUrl": "https://arxiv.org/pdf/2602.16259v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16259v1",
      "doi": null
    },
    {
      "id": "2602.16195",
      "title": "Phase Transitions in Collective Damage of Civil Structures under Natural Hazards",
      "authors": [
        "Sebin Oh",
        "Jinyan Zhao",
        "Raul Rincon",
        "Jamie E. Padgett",
        "Ziqi Wang"
      ],
      "abstract": "The fate of cities under natural hazards depends not only on hazard intensity but also on the coupling of structural damage, a collective process that remains poorly understood. Here we show that urban structural damage exhibits phase-transition phenomena. As hazard intensity increases, the system can shift abruptly from a largely safe to a largely damaged state, analogous to a first-order phase transition in statistical physics. Higher diversity in the building portfolio smooths this transition, but multiscale damage clustering traps the system in an extended critical-like regime (analogous to a Griffiths phase), suppressing the emergence of a more predictable disordered (Gaussian) phase. These phenomenological patterns are characterized by a random-field Ising model, with the external field, disorder strength, and temperature interpreted as the effective hazard demand, structural diversity, and modeling uncertainty, respectively. Applying this framework to real urban inventories reveals that widely used engineering modeling practices can shift urban damage patterns between synchronized and volatile regimes, systematically biasing exceedance-based risk metrics by up to 50% under moderate earthquakes ($M_w \\approx 5.5$--$6.0$), equivalent to a several-fold gap in repair costs. This phase-aware description turns the collective behavior of civil infrastructure damage into actionable diagnostics for urban risk assessment and planning.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "stat.AP"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.16195v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16195v1",
      "doi": null
    },
    {
      "id": "2602.16749",
      "title": "U-FedTomAtt: Ultra-lightweight Federated Learning with Attention for Tomato Disease Recognition",
      "authors": [
        "Romiyal George",
        "Sathiyamohan Nishankar",
        "Selvarajah Thuseethan",
        "Chathrie Wimalasooriya",
        "Yakub Sebastian",
        "Roshan G. Ragel",
        "Zhongwei Liang"
      ],
      "abstract": "Federated learning has emerged as a privacy-preserving and efficient approach for deploying intelligent agricultural solutions. Accurate edge-based diagnosis across geographically dispersed farms is crucial for recognising tomato diseases in sustainable farming. Traditional centralised training aggregates raw data on a central server, leading to communication overhead, privacy risks and latency. Meanwhile, edge devices require lightweight networks to operate effectively within limited resources. In this paper, we propose U-FedTomAtt, an ultra-lightweight federated learning framework with attention for tomato disease recognition in resource-constrained and distributed environments. The model comprises only 245.34K parameters and 71.41 MFLOPS. First, we propose an ultra-lightweight neural network with dilated bottleneck (DBNeck) modules and a linear transformer to minimise computational and memory overhead. To mitigate potential accuracy loss, a novel local-global residual attention (LoGRA) module is incorporated. Second, we propose the federated dual adaptive weight aggregation (FedDAWA) algorithm that enhances global model accuracy. Third, our framework is validated using three benchmark datasets for tomato diseases under simulated federated settings. Experimental results show that the proposed method achieves 0.9910% and 0.9915% Top-1 accuracy and 0.9923% and 0.9897% F1-scores on SLIF-Tomato and PlantVillage tomato datasets, respectively.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "q-bio.QM",
        "cs.LG"
      ],
      "primaryCategory": "q-bio.QM",
      "pdfUrl": "https://arxiv.org/pdf/2602.16749v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16749v1",
      "doi": null
    },
    {
      "id": "2602.16146",
      "title": "Uncertainty-Aware Neural Multivariate Geostatistics",
      "authors": [
        "Yeseul Jeon",
        "Aaron Scheffler",
        "Rajarshi Guhaniyogi"
      ],
      "abstract": "We propose Deep Neural Coregionalization, a scalable framework for uncertainty-aware multivariate geostatistics. DNC models multivariate spatial effects through spatially varying latent factors and loadings, assigning deep Gaussian process (DGP) priors to both the factors and the entries of the loading matrix. This joint construction learns shared latent spatial structure together with response-specific, location-dependent mixing weights, enabling flexible nonlinear and space-dependent associations within and across variables. A key contribution is a variational formulation that makes the DGP to deep neural network (DNN) correspondence explicit: maximizing the DGP evidence lower bound (ELBO) is equivalent to training DNNs with weight decay and Monte Carlo (MC) dropout. This yields fast mini-batch stochastic optimization without Markov Chain Monte Carlo (MCMC), while providing principled uncertainty quantification through MC-dropout forward passes as approximate posterior draws, producing calibrated credible surfaces for prediction and spatial effect estimation. Across simulations, DNC is competitive with existing spatial factor models, particularly under strong nonstationarity and complex cross-dependence, while delivering substantial computational gains. In a multivariate environmental case study, DNC captures spatially varying cross-variable interactions, produces interpretable maps of multivariate outcomes, and scales uncertainty quantification to large datasets with orders-of-magnitude reductions in runtime.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.16146v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16146v1",
      "doi": null
    },
    {
      "id": "2602.16137",
      "title": "Experimental Assortments for Choice Estimation and Nest Identification",
      "authors": [
        "Xintong Yu",
        "Will Ma",
        "Michael Zhao"
      ],
      "abstract": "What assortments (subsets of items) should be offered, to collect data for estimating a choice model over $n$ total items? We propose a structured, non-adaptive experiment design requiring only $O(\\log n)$ distinct assortments, each offered repeatedly, that consistently outperforms randomized and other heuristic designs across an extensive numerical benchmark that estimates multiple different choice models under a variety of (possibly mis-specified) ground truths. We then focus on Nested Logit choice models, which cluster items into \"nests\" of close substitutes. Whereas existing Nested Logit estimation procedures assume the nests to be known and fixed, we present a new algorithm to identify nests based on collected data, which when used in conjunction with our experiment design, guarantees correct identification of nests under any Nested Logit ground truth. Our experiment design was deployed to collect data from over 70 million users at Dream11, an Indian fantasy sports platform that offers different types of betting contests, with rich substitution patterns between them. We identify nests based on the collected data, which lead to better out-of-sample choice prediction than ex-ante clustering from contest features. Our identified nests are ex-post justifiable to Dream11 management.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.16137v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16137v1",
      "doi": null
    },
    {
      "id": "2602.16120",
      "title": "Feature-based morphological analysis of shape graph data",
      "authors": [
        "Murad Hossen",
        "Demetrio Labate",
        "Nicolas Charon"
      ],
      "abstract": "This paper introduces and demonstrates a computational pipeline for the statistical analysis of shape graph datasets, namely geometric networks embedded in 2D or 3D spaces. Unlike traditional abstract graphs, our purpose is not only to retrieve and distinguish variations in the connectivity structure of the data but also geometric differences of the network branches. Our proposed approach relies on the extraction of a specifically curated and explicit set of topological, geometric and directional features, designed to satisfy key invariance properties. We leverage the resulting feature representation for tasks such as group comparison, clustering and classification on cohorts of shape graphs. The effectiveness of this representation is evaluated on several real-world datasets including urban road/street networks, neuronal traces and astrocyte imaging. These results are benchmarked against several alternative methods, both feature-based and not.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "cs.LG",
        "stat.AP",
        "stat.ML"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2602.16120v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16120v1",
      "doi": null
    },
    {
      "id": "2602.16111",
      "title": "Surrogate-Based Prevalence Measurement for Large-Scale A/B Testing",
      "authors": [
        "Zehao Xu",
        "Tony Paek",
        "Kevin O'Sullivan",
        "Attila Dobi"
      ],
      "abstract": "Online media platforms often need to measure how frequently users are exposed to specific content attributes in order to evaluate trade-offs in A/B experiments. A direct approach is to sample content, label it using a high-quality rubric (e.g., an expert-reviewed LLM prompt), and estimate impression-weighted prevalence. However, repeatedly running such labeling for every experiment arm and segment is too costly and slow to serve as a default measurement at scale. We present a scalable \\emph{surrogate-based prevalence measurement} framework that decouples expensive labeling from per-experiment evaluation. The framework calibrates a surrogate signal to reference labels offline and then uses only impression logs to estimate prevalence for arbitrary experiment arms and segments. We instantiate this framework using \\emph{score bucketing} as the surrogate: we discretize a model score into buckets, estimate bucket-level prevalences from an offline labeled sample, and combine these calibrated bucket level prevalences with the bucket distribution of impressions in each arm to obtain fast, log-based estimates. Across multiple large-scale A/B tests, we validate that the surrogate estimates closely match the reference estimates for both arm-level prevalence and treatment--control deltas. This enables scalable, low-latency prevalence measurement in experimentation without requiring per-experiment labeling jobs.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "stat.AP",
        "cs.AI"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.16111v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16111v1",
      "doi": null
    },
    {
      "id": "2602.16099",
      "title": "Quantifying and Attributing Submodel Uncertainty in Stochastic Simulation Models and Digital Twins",
      "authors": [
        "Mohammadmahdi Ghasemloo",
        "David J. Eckman",
        "Yaxian Li"
      ],
      "abstract": "Stochastic simulation is widely used to study complex systems composed of various interconnected subprocesses, such as input processes, routing and control logic, optimization routines, and data-driven decision modules. In practice, these subprocesses may be inherently unknown or too computationally intensive to directly embed in the simulation model. Replacing these elements with estimated or learned approximations introduces a form of epistemic uncertainty that we refer to as submodel uncertainty. This paper investigates how submodel uncertainty affects the estimation of system performance metrics. We develop a framework for quantifying submodel uncertainty in stochastic simulation models and extend the framework to digital-twin settings, where simulation experiments are repeatedly conducted with the model initialized from observed system states. Building on approaches from input uncertainty analysis, we leverage bootstrapping and Bayesian model averaging to construct quantile-based confidence or credible intervals for key performance indicators. We propose a tree-based method that decomposes total output variability and attributes uncertainty to individual submodels in the form of importance scores. The proposed framework is model-agnostic and accommodates both parametric and nonparametric submodels under frequentist and Bayesian modeling paradigms. A synthetic numerical experiment and a more realistic digital-twin simulation of a contact center illustrate the importance of understanding how and how much individual submodels contribute to overall uncertainty.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "stat.CO",
        "stat.ME",
        "stat.ML"
      ],
      "primaryCategory": "stat.CO",
      "pdfUrl": "https://arxiv.org/pdf/2602.16099v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16099v1",
      "doi": null
    },
    {
      "id": "2602.16062",
      "title": "Harnessing Implicit Cooperation: A Multi-Agent Reinforcement Learning Approach Towards Decentralized Local Energy Markets",
      "authors": [
        "Nelson Salazar-Pena",
        "Alejandra Tabares",
        "Andres Gonzalez-Mancera"
      ],
      "abstract": "This paper proposes implicit cooperation, a framework enabling decentralized agents to approximate optimal coordination in local energy markets without explicit peer-to-peer communication. We formulate the problem as a decentralized partially observable Markov decision problem that is solved through a multi-agent reinforcement learning task in which agents use stigmergic signals (key performance indicators at the system level) to infer and react to global states. Through a 3x3 factorial design on an IEEE 34-node topology, we evaluated three training paradigms (CTCE, CTDE, DTDE) and three algorithms (PPO, APPO, SAC). Results identify APPO-DTDE as the optimal configuration, achieving a coordination score of 91.7% relative to the theoretical centralized benchmark (CTCE). However, a critical trade-off emerges between efficiency and stability: while the centralized benchmark maximizes allocative efficiency with a peer-to-peer trade ratio of 0.6, the fully decentralized approach (DTDE) demonstrates superior physical stability. Specifically, DTDE reduces the variance of grid balance by 31% compared to hybrid architectures, establishing a highly predictable, import-biased load profile that simplifies grid regulation. Furthermore, topological analysis reveals emergent spatial clustering, where decentralized agents self-organize into stable trading communities to minimize congestion penalties. While SAC excelled in hybrid settings, it failed in decentralized environments due to entropy-driven instability. This research proves that stigmergic signaling provides sufficient context for complex grid coordination, offering a robust, privacy-preserving alternative to expensive centralized communication infrastructure.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "categories": [
        "eess.SY",
        "cs.CE",
        "cs.LG",
        "cs.MA",
        "stat.AP"
      ],
      "primaryCategory": "eess.SY",
      "pdfUrl": "https://arxiv.org/pdf/2602.16062v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16062v1",
      "doi": null
    }
  ],
  "metadata": {
    "lastUpdated": "2026-02-22T02:53:10.112Z",
    "totalPapers": 50,
    "categories": [
      "stat.ME",
      "stat.AP",
      "econ.EM",
      "q-bio.QM"
    ],
    "queryDate": "2026-02-22"
  }
}