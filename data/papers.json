{
  "papers": [
    {
      "id": "2602.18396",
      "title": "PRISM-FCP: Byzantine-Resilient Federated Conformal Prediction via Partial Sharing",
      "authors": [
        "Ehsan Lari",
        "Reza Arablouei",
        "Stefan Werner"
      ],
      "abstract": "We propose PRISM-FCP (Partial shaRing and robust calIbration with Statistical Margins for Federated Conformal Prediction), a Byzantine-resilient federated conformal prediction framework that utilizes partial model sharing to improve robustness against Byzantine attacks during both model training and conformal calibration. Existing approaches address adversarial behavior only in the calibration stage, leaving the learned model susceptible to poisoned updates. In contrast, PRISM-FCP mitigates attacks end-to-end. During training, clients partially share updates by transmitting only $M$ of $D$ parameters per round. This attenuates the expected energy of an adversary's perturbation in the aggregated update by a factor of $M/D$, yielding lower mean-square error (MSE) and tighter prediction intervals. During calibration, clients convert nonconformity scores into characterization vectors, compute distance-based maliciousness scores, and downweight or filter suspected Byzantine contributions before estimating the conformal quantile. Extensive experiments on both synthetic data and the UCI Superconductivity dataset demonstrate that PRISM-FCP maintains nominal coverage guarantees under Byzantine attacks while avoiding the interval inflation observed in standard FCP with reduced communication, providing a robust and communication-efficient approach to federated uncertainty quantification.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "cs.LG",
        "eess.SP",
        "math.PR",
        "stat.AP",
        "stat.ML"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2602.18396v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18396v1",
      "doi": null
    },
    {
      "id": "2602.18383",
      "title": "Design-based inference for generalized causal effects in randomized experiments",
      "authors": [
        "Xinyuan Chen",
        "Fan Li"
      ],
      "abstract": "Generalized causal effect estimands, including the Mann-Whitney parameter and causal net benefit, provide flexible summaries of treatment effects in randomized experiments with non-Gaussian or multivariate outcomes. We develop a unified design-based inference framework for regression adjustment and variance estimation of a broad class of generalized causal effect estimands defined through pairwise contrast functions. Leveraging the theory of U-statistics and finite-population asymptotics, we establish the consistency and asymptotic normality of regression estimators constructed from individual pairs and per-unit pair averages, even when the working models are misspecified. Consequently, these estimators are model-assisted rather than model-based. In contrast to classical average treatment effect estimands, we show that for nonlinear contrast functions, covariate adjustment preserves consistency but does not admit a universal efficiency guarantee. For inference, we demonstrate that standard heteroskedasticity-robust and cluster-robust variance estimators are generally inconsistent in this setting. As a remedy, we prove that a complete two-way cluster-robust variance estimator, which fully accounts for pairwise dependence and reverse comparisons, is consistent.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.18383v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18383v1",
      "doi": null
    },
    {
      "id": "2602.18369",
      "title": "Hidden multistate models to study multimorbidity trajectories",
      "authors": [
        "Valentina Manzoni",
        "Francesca Ieva",
        "Amaia Calderón-Larrañaga",
        "Davide Liborio Vetrano",
        "Caterina Gregorio"
      ],
      "abstract": "Multimorbidity in older adults is common, heterogeneous, and highly dynamic, and it is strongly associated with disability and increased healthcare utilization. However, existing approaches to studying multimorbidity trajectories are largely descriptive or rely on discrete-time models, which struggle to handle irregular observation intervals and right-censoring. We developed a continuous-time hidden multistate modeling framework to capture transitions among latent multimorbidity patterns while accounting for interval censoring and misclassification. A simulation study compared alternative model specifications under varying sample sizes and follow-up schemes, and the best-performing specification was applied to longitudinal data from the Swedish National study on Aging and Care-Kungsholmen (SNAC-K), including 2,716 multimorbid participants followed for up to 18 years. Simulation results showed that hidden multistate models substantially reduced bias in transition hazard estimates compared to non-hidden models, with fully time-inhomogeneous models outperforming piecewise approximations. Application to SNAC-K confirmed the feasibility and practical utility of this framework, enabling identification of risk factors for accelerated progression toward complex multimorbidity and revealing a gradient of mortality risk across patterns. Continuous-time hidden multistate models provide a robust alternative to traditional approaches, supporting individualized predictions and informing targeted interventions and secondary prevention strategies for multimorbidity in aging populations.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.AP",
        "stat.ME"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.18369v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18369v1",
      "doi": null
    },
    {
      "id": "2602.18358",
      "title": "Forecasting the Evolving Composition of Inbound Tourism Demand: A Bayesian Compositional Time Series Approach Using Platform Booking Data",
      "authors": [
        "Harrison Katz"
      ],
      "abstract": "Understanding how the composition of guest origin markets evolves over time is critical for destination marketing organizations, hospitality businesses, and tourism planners. We develop and apply Bayesian Dirichlet autoregressive moving average (BDARMA) models to forecast the compositional dynamics of guest origin market shares using proprietary Airbnb booking data spanning 2017--2024 across four major destination regions. Our analysis reveals substantial pandemic-induced structural breaks in origin composition, with heterogeneous recovery patterns across markets. The BDARMA framework achieves the lowest average forecast error across all destination regions, outperforming standard benchmarks including naïve forecasts, exponential smoothing, and SARIMA on log-ratio transformed data. For EMEA destinations, BDARMA achieves 23% lower forecast error than naive methods, with statistically significant improvements. By modeling compositions directly on the simplex with a Dirichlet likelihood and incorporating seasonal variation in both mean and precision parameters, our approach produces coherent forecasts that respect the unit-sum constraint while capturing complex temporal dependencies. The methodology provides destination stakeholders with probabilistic forecasts of source market shares, enabling more informed strategic planning for marketing resource allocation, infrastructure investment, and crisis response.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.AP",
        "q-fin.ST"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.18358v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18358v1",
      "doi": null
    },
    {
      "id": "2602.18271",
      "title": "Two-Stage Multiple Test Procedures Controlling False Discovery Rate with auxiliary variable and their Application to Set4Delta Mutant Data",
      "authors": [
        "Seohwa Hwang",
        "Mark Louie Ramos",
        "DoHwan Park",
        "Junyong Park",
        "Johan Lim",
        "Erin Green"
      ],
      "abstract": "In this paper, we present novel methodologies that incorporate auxiliary variables for multiple hypotheses testing related to the main point of interest while effectively controlling the false discovery rate. When dealing with multiple tests concerning the primary variable of interest, researchers can use auxiliary variables to set preconditions for the significance of primary variables, thereby enhancing test efficacy. Depending on the auxiliary variable's role, we propose two approaches: one terminates testing of the primary variable if it does not meet predefined conditions, and the other adjusts the evaluation criteria based on the auxiliary variable. Employing the copula method, we elucidate the dependence between the auxiliary and primary variables by deriving their joint distribution from individual marginal distributions.Our numerical studies, compared with existing methods, demonstrate that the proposed methodologies effectively control the FDR and yield greater statistical power than previous approaches solely based on the primary variable. As an illustrative example, we apply our methods to the Set4$Δ$ mutant dataset. Our findings highlight the distinctions between our methodologies and traditional approaches, emphasising the potential advantages of our methods in introducing the auxiliary variable for selecting more genes.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.18271v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18271v1",
      "doi": null
    },
    {
      "id": "2602.18241",
      "title": "Online FDR Controlling procedures for statistical SIS Model and its application to COVID19 data",
      "authors": [
        "Seohwa Hwang",
        "Junyong Park"
      ],
      "abstract": "We propose an online false discovery rate (FDR) controlling method based on conditional local FDR (LIS), designed for infectious disease datasets that are discrete and exhibit complex dependencies. Unlike existing online FDR methods, which often assume independence or suffer from low statistical power in dependent settings, our approach effectively controls FDR while maintaining high detection power in realistic epidemic scenarios. For disease modeling, we establish a Dynamic Bayesian Network (DBN) structure within the Susceptible-Infected-Susceptible (SIS) model, a widely used epidemiological framework for infectious diseases. Our method requires no additional tuning parameters apart from the width of the sliding window, making it practical for real-time disease monitoring. From a statistical perspective, we prove that our method ensures valid FDR control under stationary and ergodic dependencies, extending online hypothesis testing to a broader range of dependent and discrete datasets. Additionally, our method achieves higher statistical power than existing approaches by leveraging LIS, which has been shown to be more powerful than traditional $p$-value-based methods. We validate our method through extensive simulations and real-world applications, including the analysis of infectious disease incidence data. Our results demonstrate that the proposed approach outperforms existing methods by achieving higher detection power while maintaining rigorous FDR control.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.18241v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18241v1",
      "doi": null
    },
    {
      "id": "2602.18210",
      "title": "Semiparametric Uncertainty Quantification via Isotonized Posterior for Deconvolutions",
      "authors": [
        "Francesco Gili",
        "Geurt Jongbloed"
      ],
      "abstract": "We address the problem of uncertainty quantification for the deconvolution model \\(Z = X + Y\\), where \\(X\\) and \\(Y\\) are nonnegative random variables and the goal is to estimate the signal's distribution of \\(X \\sim F_0\\) supported on~\\([0,\\infty)\\), from observations where the noise distribution is known. Existing frequentist methods often produce confidence intervals for $F_0(x)$ that depend on unknown nuisance parameters, such as the density of \\(X\\) and its derivative, which are difficult to estimate in practice. This paper introduces a novel and computationally efficient nonparametric Bayesian approach, based on projecting the posterior, to overcome this limitation. Our method leverages the solution \\(p\\) to a specific Volterra integral equation as in \\cite{74}, which relates the cumulative distribution function (CDF) of the signal, \\(F_0\\), to the distribution of the observables. We place a Dirichlet Process prior directly on the distribution of the observed data $Z$, yielding a simple, conjugate posterior. To ensure the resulting estimates for \\(F_0\\) are valid CDFs, we isotonize posterior draws taking the Greatest Convex Majorant of the primitive of the posterior draws and defining what we term the Isotonic Inverse Posterior. We show that this framework yields posterior credible sets for \\(F_0\\) that are not only computationally fast to generate but also possess asymptotically correct frequentist coverage after a straightforward recalibration technique for the so-called Bayes Chernoff distribution introduced in \\cite{54}. Our approach thus does not require the estimation of nuisance parameters to deliver uncertainty quantification for the parameter of interest $F_0(x)$. The practical effectiveness and robustness of the method are demonstrated through a simulation study with various noise distributions for $Y$.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.18210v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18210v1",
      "doi": null
    },
    {
      "id": "2602.18184",
      "title": "Kolmogorov-Type Maximal Inequalities for Independent and Dependent Negative Binomial Random Variables: Sharp Bounds, Sub-Exponential Refinements, and Applications to Overdispersed Count Data",
      "authors": [
        "Aristides V. Doumas",
        "S. Spektor"
      ],
      "abstract": "This paper develops Kolmogorov-type maximal inequalities for sums of Negative Binomial random variables under both independence and dependence structures. For independent heterogeneous Negative Binomial variables we derive sharp Markov-type deviation inequalities and Kolmogorov-type bounds expressed in terms of Tweedie dispersion parameters, providing explicit control limits for NB2 generalized linear model monitoring. For dependent count data arising through a shared Gamma mixing variable, we establish a \\emph{sub-exponential Bernstein-type refinement} that exploits the Poisson-Gamma hierarchical structure to yield exponentially decaying tail probabilities -- this refinement is new in the literature. Through moment-matched Monte Carlo experiments ($n=20$, 2{,}000 replications), we document a 55\\% reduction in mean maximum deviation under appropriate dependence structures, a stabilization effect we explain analytically. A concrete epidemiological application with NB2 parameters calibrated from COVID-19 surveillance data demonstrates practical utility. These results materially advance the applicability of classical maximal inequalities to overdispersed and dependent count data prevalent in public health, insurance, and ecological modeling.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "math.ST",
        "math.PR",
        "stat.ME"
      ],
      "primaryCategory": "math.ST",
      "pdfUrl": "https://arxiv.org/pdf/2602.18184v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18184v1",
      "doi": null
    },
    {
      "id": "2602.18170",
      "title": "Minimum L2 and robust Kullback-Leibler estimation",
      "authors": [
        "Nils Lid Hjort"
      ],
      "abstract": "This paper introduces two new robust methods for estimation of parameters in a given parametric family. The first method is that of `minimum weighted L2', effectively minimising an estimate of the integrated (and possibly weighted) squared error. The second is `robust Kullback-Leibler', consisting of minimising a robust version of the empirical Kullback-Leibler distance, and can be viewed as a general robust modification of the maximum likelihood procedure. This second method is also related to recent local likelihood ideas for semiparametric density estimation. The methods are described, influence functions are found, as are formulae for asymptotic variances. In particular large-sample efficiencies are computed under the home turf conditions of the underlying parametric model. The methods and formulae are illustrated for the normal model.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.18170v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18170v1",
      "doi": null
    },
    {
      "id": "2602.18161",
      "title": "Equal Marginal Power for Co-Primary Endpoints",
      "authors": [
        "Simon Bond"
      ],
      "abstract": "The choice of sample size in the context of co-primary endpoints for a randomised trial is discussed. Current guidance can leave endpoints with unequal marginal power. A method is provided to achieve equal marginal power by using the flexibility provided in multiple testing procedures. A comparison is made to several choices of rule to determine the sample size, in terms of the study design and its operating characteristics.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.18161v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18161v1",
      "doi": null
    },
    {
      "id": "2602.18150",
      "title": "Inclusive Ranking of Indian States via Bayesian Bradley-Terry Model",
      "authors": [
        "Arshi Rizvi",
        "Rahul Singh"
      ],
      "abstract": "Evaluating the performance of different administrative regions within a country is crucial for its development and policy formulation. The performance evaluators are mostly based on health, education, per capita income, awareness, family planning and so on. Not only evaluating regions, but also ranking them is a crucial step, and various methods have been proposed to date. We aim to provide a ranking system for Indian states that uses a Bayesian approach via the famous Bradley-Terry model for paired comparisons. The ranking method uses indicators from the NFHS-5 dataset with the prior information of per-capita incomes of the states/UTs, thus leading to a holistic ranking, which not only includes human development factors but also take account the economic background of the states. We also carried out various Markov chain Monte Carlo diagnostics required for the reliability of the estimates of merits for these states. These merits thus provide a ranking for the states/UTs and can further be utilised to make informed policy decisions.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.18150v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18150v1",
      "doi": null
    },
    {
      "id": "2602.18087",
      "title": "Optimal inference via confidence distributions for two-by-two tables modelled as Poisson pairs: fixed and random effects",
      "authors": [
        "Céline Cunen",
        "Nils Lid Hjort"
      ],
      "abstract": "This paper presents methods for meta-analysis of $2 \\times 2$ tables, both with and without allowing heterogeneity in the treatment effects. Meta-analysis is common in medical research, but most existing methods are unsuited for $2 \\times 2$ tables with rare events. Usually the tables are modelled as pairs of binomial variables, but we will model them as Poisson pairs. The methods presented here are based on confidence distributions, and offer optimal inference for the treatment effect parameter. We also propose an optimal method for inference on the ratio between treatment effects, and illustrate our methods on a real dataset.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.18087v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18087v1",
      "doi": null
    },
    {
      "id": "2602.18058",
      "title": "Probabilistic Methods for Initial Orbit Determination and Orbit Determination in Cislunar Space",
      "authors": [
        "Ishan Paranjape",
        "Tarun Hejmadi",
        "Suman Chakravorty"
      ],
      "abstract": "In orbital mechanics, Gauss's method for orbit determination (OD) is a popular, minimal assumption solution for obtaining the initial state estimate of a passing resident space object (RSO). Since much of the cislunar domain relies on three-body dynamics, a key assumption of Gauss's method is rendered incompatible, creating a need for a new, minimal assumption method for initial orbit determination (IOD). In this work, we present a framework for short and long term probabilistic target tracking in cislunar space which produces an initial state estimate with as few assumptions as possible. Specifically, we propose an IOD method involving the kinematic fitting of several series of noisy, consecutive ground-based observations. Once a probabilistic initial state estimate in the form of a particle cloud is formed, we apply the powerful Particle Gaussian Mixture (PGM) Filter to reduce the uncertainty of our state estimate over time. This combined IOD/OD framework is demonstrated for several classes of trajectories in cislunar space and compared to better-known filtering frameworks.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "astro-ph.EP",
        "astro-ph.IM",
        "eess.SY",
        "math.OC",
        "physics.data-an",
        "physics.space-ph",
        "stat.AP"
      ],
      "primaryCategory": "astro-ph.EP",
      "pdfUrl": "https://arxiv.org/pdf/2602.18058v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18058v1",
      "doi": null
    },
    {
      "id": "2602.18045",
      "title": "Conformal Tradeoffs: Guarantees Beyond Coverage",
      "authors": [
        "Petrus H. Zwart"
      ],
      "abstract": "Deployed conformal predictors are long-lived decision infrastructure operating over finite operational windows. The real-world question is not only ``Does the true label lie in the prediction set at the target rate?'' (marginal coverage), but ``How often does the system commit versus defer? What error exposure does it induce when it acts? How do these rates trade off?'' Marginal coverage does not determine these deployment-facing quantities: the same calibrated thresholds can yield different operational profiles depending on score geometry. We provide a framework for operational certification and planning beyond coverage with three contributions. (1) Small-Sample Beta Correction (SSBC): we invert the exact finite-sample Beta/rank law for split conformal to map a user request $(α^\\star,δ)$ to a calibrated grid point with PAC-style semantics, yielding explicit finite-window coverage guarantees. (2) Calibrate-and-Audit: since no distribution-free pivot exists for rates beyond coverage, we introduce a two-stage design in which an independent audit set produces a reusable region -- label table and certified finite-window envelopes (Binomial/Beta-Binomial) for operational quantities -- commitment frequency, deferral, decisive error exposure, and commit purity -- via linear projection. (3) Geometric characterization: we describe feasibility constraints, regime boundaries (hedging vs.\\ rejection), and cost-coherence conditions induced by a fixed conformal partition, explaining why operational rates are coupled and how calibration navigates their trade-offs. The output is an auditable operational menu: for a fixed scoring model, we trace attainable operational profiles across calibration settings and attach finite-window uncertainty envelopes. We demonstrate the approach on Tox21 toxicity prediction (12 endpoints) and aqueous solubility screening using AquaSolDB.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.ME",
        "cs.AI"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.18045v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18045v1",
      "doi": null
    },
    {
      "id": "2602.18039",
      "title": "A context-specific causal model for estimating the effect of extended length of overnight stay on traveller's total expenditure",
      "authors": [
        "Lauri Valkonen",
        "Juha Karvanen"
      ],
      "abstract": "Tourism significantly affects the economies of many countries. Understanding the causal relationship between the length of overnight stay and traveller's expenditure is crucial for stakeholders to characterize spending profiles and to design marketing strategies. Causal mechanisms differ between personal and work-related travel because the decision-making processes have different drivers and constraints. We apply context-specific independence relations to model causal mechanisms in contexts specified by trip purpose and identify the causal effect of the length of stay on expenditure. Using the international visitor survey data on foreign travellers to Finland, we fit a hierarchical Bayesian model to estimate the posterior distribution of the counterfactual expenditure due to extending the length of stay by one night. We also perform a Bayesian sensitivity analysis of the estimated causal effect with respect to omitted variable bias.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.AP"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.18039v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18039v1",
      "doi": null
    },
    {
      "id": "2602.18004",
      "title": "Preconditioned Robust Neural Posterior Estimation for Misspecified Simulators",
      "authors": [
        "Ryan P. Kelly",
        "David T. Frazier",
        "David J. Warne",
        "Christopher C. Drovandi"
      ],
      "abstract": "Simulation-based inference (SBI) enables parameter estimation for complex stochastic models with intractable likelihoods when model simulation is feasible. Neural posterior estimation (NPE) is a popular SBI approach that often achieves accurate inference with far fewer simulations than classical approaches. But in practice, neural approaches can be unreliable for two reasons: incompatible data summaries arising from model misspecification yield unreliable posteriors due to extrapolation, and prior-predictive draws can produce extreme summaries that lead to difficulties in obtaining an accurate posterior for the observed data of interest. Existing preconditioning schemes target well-specified settings, and their behaviour under misspecification remains unexplored. We study preconditioning under misspecification and propose preconditioned robust neural posterior estimation, which computes data-dependent weights that focus training near the observed summaries and fits a robust neural posterior approximation. We also introduce a forest-proximity preconditioning approach that uses tree-based proximity scores to down-weight outlying simulations and concentrate computation around the observed dataset. Across two synthetic examples and one real example with incompatible summaries and extreme prior-predictive behaviour, we demonstrate that preconditioning combined with robust NPE increases stability and improves accuracy, calibration, and posterior-predictive fit over standard baseline methods.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.ME",
        "stat.CO",
        "stat.ML"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.18004v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18004v1",
      "doi": null
    },
    {
      "id": "2602.17995",
      "title": "Hybrid Non-informative and Informative Prior Model-assisted Designs for Mid-trial Dose Insertion",
      "authors": [
        "Kana Yamada",
        "Hisato Sunami",
        "Kentaro Takeda",
        "Keisuke Hanada",
        "Masahiro Kojima"
      ],
      "abstract": "In oncology phase I trials, model-assisted designs have been increasingly adopted because they enable adaptive yet operationally simple dose adjustment based on accumulating safety data, leading to a paradigm shift in dose-escalation methodology. In practice, a single mid-trial dose insertion may be considered to examine safer doses and/or to collect more informative efficacy data. In this study, we investigate methods to improve dose assignment and the selection of the maximum tolerated dose (MTD) or the optimal biological dose (OBD) when a new dose level is added during an ongoing trial under a model-assisted framework, by assigning informative prior information to the inserted dose. We propose a hybrid design that uses a non-informative model-assisted design at trial initiation and, upon dose insertion, applies an informative-prior extension only to the newly added dose. In addition, to address potential skeleton misspecification, we propose two adaptive extensions: (i) an online-weighting approach that updates the skeleton over time, and (ii) a Bayesian-mixture approach that robustly combines multiple candidate skeletons. We evaluate the proposed methods through simulation studies.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.ME",
        "stat.AP"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.17995v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17995v1",
      "doi": null
    },
    {
      "id": "2602.17984",
      "title": "Developing Performance-Guaranteed Biomarker Combination Rules with Integrated External Information under Practical Constraint",
      "authors": [
        "Albert Osom",
        "Camden Lopez",
        "Ashley Alexander",
        "Suresh Chari",
        "Ziding Feng",
        "Ying-Qi Zhao"
      ],
      "abstract": "In clinical practice, there is significant interest in integrating novel biomarkers with existing clinical data to construct interpretable and robust decision rules. Motivated by the need to improve decision-making for early disease detection, we propose a framework for developing an optimal biomarker-based clinical decision rule that is both clinically meaningful and practically feasible. Specifically, our procedure constructs a linear decision rule designed to achieve optimal performance among class of linear rules by maximizing the true positive rate while adhering to a pre-specified positive predictive value constraint. Additionally, our method can adaptively incorporate individual risk information from external source to enhance performance when such information is beneficial. We establish the asymptotic properties of our proposed estimator and compare to the standard approach used in practice through extensive simulation studies. Results indicate that our approach offers strong finite-sample performance. We also apply the proposed methods to develop biomarker-based screening rules for pancreatic ductal adenocarcinoma (PDAC) among new-onset diabetes (NOD) patients.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.17984v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17984v1",
      "doi": null
    },
    {
      "id": "2602.17967",
      "title": "Minimax optimal adaptive structured transfer learning through semi-parametric domain-varying coefficient model",
      "authors": [
        "Hanxiao Chen",
        "Debarghya Mukherjee"
      ],
      "abstract": "Transfer learning aims to improve inference in a target domain by leveraging information from related source domains, but its effectiveness critically depends on how cross-domain heterogeneity is modeled and controlled. When the conditional mechanism linking covariates and responses varies across domains, indiscriminate information pooling can lead to negative transfer, degrading performance relative to target-only estimation. We study a multi-source, single-target transfer learning problem under conditional distributional drift and propose a semiparametric domain-varying coefficient model (DVCM), in which domain-relatedness is encoded through an observable domain identifier. This framework generalizes classical varying-coefficient models to structured transfer learning and interpolates between invariant and fully heterogeneous regimes. Building on this model, we develop an adaptive transfer learning estimator that selectively borrows strength from informative source domains while provably safeguarding against negative transfer. Our estimator is computationally efficient and easy to implement; we also show that it is minimax rate-optimal and derive its asymptotic distribution, enabling valid uncertainty quantification and hypothesis testing despite data-adaptive pooling and shrinkage. Our results precisely characterize the interplay among domain heterogeneity, the smoothness of the underlying mean function, and the number of source domains and are corroborated by comprehensive numerical experiments and two real-data applications.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "math.ST",
        "stat.ME",
        "stat.ML"
      ],
      "primaryCategory": "math.ST",
      "pdfUrl": "https://arxiv.org/pdf/2602.17967v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17967v1",
      "doi": null
    },
    {
      "id": "2602.17960",
      "title": "Anisotropic local law for non-separable sample covariance matrices",
      "authors": [
        "Zhou Fan",
        "Renyuan Ma",
        "Elliot Paquette",
        "Zhichao Wang"
      ],
      "abstract": "We establish local laws for sample covariance matrices $K = N^{-1}\\sum_{i=1}^N \\g_i\\g_i^*$ where the random vectors $\\g_1, \\ldots, \\g_N \\in \\R^n$ are independent with common covariance $Σ$. Previous work has largely focused on the separable model $\\g = Σ^{1/2}\\w$ with $\\w$ having independent entries, but this structure is rarely present in statistical applications involving dependent or nonlinearly transformed data. Under a concentration assumption for quadratic forms $\\g^*A\\g$, we prove an optimal averaged local law showing that the Stieltjes transform of $K$ converges to its deterministic limit uniformly down to the optimal scale $η\\geq N^{-1+\\eps}$. Under an additional structural assumption on the cumulant tensors of $\\g$ -- which interpolates between the highly structured case of independent entries and generic dependence -- we establish the full anisotropic local law, providing entrywise control of the resolvent $(K-zI)^{-1}$ in arbitrary directions. We discuss several classes of non-separable examples satisfying our assumptions, including conditionally mean-zero distributions, the random features model $\\g = σ(X\\w)$ arising in machine learning, and Gaussian measures with nonlinear tilting. The proofs introduce a tensor network framework for analyzing fluctuation averaging in the presence of higher-order cumulant structure.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "math.PR",
        "stat.AP",
        "stat.ML"
      ],
      "primaryCategory": "math.PR",
      "pdfUrl": "https://arxiv.org/pdf/2602.17960v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17960v1",
      "doi": null
    },
    {
      "id": "2602.17956",
      "title": "A variational framework for modal estimation",
      "authors": [
        "Tâm LeMinh",
        "Julyan Arbel",
        "Florence Forbes",
        "Hien Duy Nguyen"
      ],
      "abstract": "We approach multivariate mode estimation through Gibbs distributions and introduce GERVE (Gibbs-measure Entropy-Regularised Variational Estimation), a likelihood-free framework that approximates Gibbs measures directly from samples by maximizing an entropy-regularised variational objective with natural-gradient updates. GERVE brings together kernel density estimation, mean-shift, variational inference, and annealing in a single platform for mode estimation. It fits Gaussian mixtures that concentrate on high-density regions and yields cluster assignments from responsibilities, with reduced sensitivity to the chosen number of components. We provide theory in two regimes: as the Gibbs temperature approaches zero, mixture components converge to population modes; at fixed temperature, maximisers of the empirical objective exist, are consistent, and are asymptotically normal. We also propose a bootstrap procedure for per-mode confidence ellipses and stability scores. Simulation and real-data studies show accurate mode recovery and emergent clustering, robust to mixture overspecification. GERVE is a practical likelihood-free approach when the number of modes or groups is unknown and full density estimation is impractical.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.CO"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.17956v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17956v1",
      "doi": null
    },
    {
      "id": "2602.17923",
      "title": "Model Error Embedding with Orthogonal Gaussian Processes",
      "authors": [
        "Mridula Kuppa",
        "Khachik Sargsyan",
        "Marco Panesi",
        "Habib N. Najm"
      ],
      "abstract": "Computational models of complex physical systems often rely on simplifying assumptions which inevitably introduce model error, with consequent predictive errors. Given data on model observables, the estimation of parameterized model-error representations, along with other model parameters, would be ideally done while separating the contributions of each of the two sets of parameters, in order to ensure meaningful stand-alone model predictions. This work builds an embedded model error framework using a weight-space representation of Gaussian processes (GPs) to flexibly capture model-error spatiotemporal correlations and enable inference with GP-embedding in non-linear models. To disambiguate model and model-error/bias parameters, we extend an existing orthogonal GP method to the embedded model-error setting and derive appropriate orthogonality constraints. To address the increased dimensionality introduced by the GP representation, we employ the likelihood-informed subspace method. The construction is demonstrated on linear and non-linear examples, where it effectively corrects model predictions to match data trends. Extrapolation beyond the training data recovers the prior predictive distribution, and the orthogonality constraints lead to meaningful stand-alone model predictions and nearly uncorrelated posteriors between model and model-error parameters.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.ME",
        "math.NA",
        "stat.CO"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.17923v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17923v1",
      "doi": null
    },
    {
      "id": "2602.17922",
      "title": "Data-driven configuration tuning of glmnet to balance accuracy and computation time",
      "authors": [
        "Shuhei Muroya",
        "Kei Hirose"
      ],
      "abstract": "glmnet is a widely adopted R package for lasso estimation due to its computational efficiency. Despite its popularity, glmnet sometimes yields solutions that are substantially different from the true ones because of the inappropriate default configuration of the algorithm. The accuracy of the obtained solutions can be improved by appropriately tuning the configuration. However, improving accuracy typically increases computational time, resulting in a trade-off between accuracy and computational efficiency. Therefore, it is essential to establish a systematic approach to determine appropriate configuration. To address this need, we propose a unified data-driven framework specifically designed to optimize the configuration by balancing the trade-off between accuracy and computational efficiency. We generate large-scale simulated datasets and apply glmnet under various configurations to obtain accuracy and computation time. Based on these results, we construct neural networks that predict accuracy and computation time from data characteristics and configuration. Given a new dataset, our framework uses the neural networks to explore the configuration space and derive a Pareto front that represents the trade-off between accuracy and computational cost. This front allows us to automatically identify the configuration that maximize accuracy under a user-specified time constraint. The proposed method is implemented in the R package 'glmnetconf', available at https://github.com/Shuhei-Muroya/glmnetconf.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.CO",
        "stat.ME",
        "stat.ML"
      ],
      "primaryCategory": "stat.CO",
      "pdfUrl": "https://arxiv.org/pdf/2602.17922v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17922v1",
      "doi": null
    },
    {
      "id": "2602.17792",
      "title": "Spatial Confounding: A review of concepts, challenges, and current approaches",
      "authors": [
        "Isaque Vieira Machado Pim",
        "Luiz Max Fagundes de Carvalho",
        "Marcos Oliveira Prates"
      ],
      "abstract": "Spatial confounding is a persistent challenge in spatial statistics, influencing the validity of statistical inference in models that analyze spatially-structured data. The concept has been interpreted in various ways but is broadly defined as bias in estimates arising from unmeasured spatial variation. In this paper we review definitions, classical spatial models, and recent methodological advances, including approaches from spatial statistics and causal inference. We provide an unified view of the many available approaches for areal as well as geostatistical data and discuss their relative merits both theoretically and empirically with a head-to-head comparison on real datasets. Finally, we leverage the results of the empirical comparisons to discuss directions for future research.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.17792v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17792v1",
      "doi": null
    },
    {
      "id": "2602.17772",
      "title": "Sparse Bayesian Modeling of EEG Channel Interactions Improves P300 Brain-Computer Interface Performance",
      "authors": [
        "Guoxuan Ma",
        "Yuan Zhong",
        "Moyan Li",
        "Yuxiao Nie",
        "Jian Kang"
      ],
      "abstract": "Electroencephalography (EEG)-based P300 brain-computer interfaces (BCIs) enable communication without physical movement by detecting stimulus-evoked neural responses. Accurate and efficient decoding remains challenging due to high dimensionality, temporal dependence, and complex interactions across EEG channels. Most existing approaches treat channels independently or rely on black-box machine learning models, limiting interpretability and personalization. We propose a sparse Bayesian time-varying regression framework that explicitly models pairwise EEG channel interactions while performing automatic temporal feature selection. The model employs a relaxed-thresholded Gaussian process prior to induce structured sparsity in both channel-specific and interaction effects, enabling interpretable identification of task-relevant channels and channel pairs. Applied to a publicly available P300 speller dataset of 55 participants, the proposed method achieves a median character-level accuracy of 100\\% using all stimulus sequences and attains the highest overall decoding performance among competing statistical and deep learning approaches. Incorporating channel interactions yields subgroup-specific gains of up to 7\\% in character-level accuracy, particularly among participants who abstained from alcohol (up to 18\\% improvement). Importantly, the proposed method improves median BCI-Utility by approximately 10\\% at its optimal operating point, achieving peak throughput after only seven stimulus sequences. These results demonstrate that explicitly modeling structured EEG channel interactions within a principled Bayesian framework enhances predictive accuracy, improves user-centric throughput, and supports personalization in P300 BCI systems.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.ME",
        "cs.LG"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.17772v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17772v1",
      "doi": null
    },
    {
      "id": "2602.17640",
      "title": "huff: A Python package for Market Area Analysis",
      "authors": [
        "Thomas Wieland"
      ],
      "abstract": "Market area models, such as the Huff model and its extensions, are widely used to estimate regional market shares and customer flows of retail and service locations. Another, now very common, area of application is the analysis of catchment areas, supply structures and the accessibility of healthcare locations. The huff Python package provides a complete workflow for market area analysis, including data import, construction of origin-destination interaction matrices, basic model analysis, parameter estimation from empirical data, calculation of distance or travel time indicators, and map visualization. Additionally, the package provides several methods of spatial accessibility analysis. The package is modular and object-oriented. It is intended for researchers in economic geography, regional economics, spatial planning, marketing, geoinformation science, and health geography. The software is openly available via the [Python Package Index (PyPI)](https://pypi.org/project/huff/); its development and version history are managed in a public [GitHub Repository](https://github.com/geowieland/huff_official) and archived at [Zenodo](https://doi.org/10.5281/zenodo.18639559).",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.AP",
        "cs.SE"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.17640v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17640v1",
      "doi": null
    },
    {
      "id": "2602.17603",
      "title": "SOLVAR: Fast covariance-based heterogeneity analysis with pose refinement for cryo-EM",
      "authors": [
        "Roey Yadgar",
        "Roy R. Lederman",
        "Yoel Shkolnisky"
      ],
      "abstract": "Cryo-electron microscopy (cryo-EM) has emerged as a powerful technique for resolving the three-dimensional structures of macromolecules. A key challenge in cryo-EM is characterizing continuous heterogeneity, where molecules adopt a continuum of conformational states. Covariance-based methods offer a principled approach to modeling structural variability. However, estimating the covariance matrix efficiently remains a challenging computational task. In this paper, we present SOLVAR (Stochastic Optimization for Low-rank Variability Analysis), which leverages a low-rank assumption on the covariance matrix to provide a tractable estimator for its principal components, despite the apparently prohibitive large size of the covariance matrix. Under this low-rank assumption, our estimator can be formulated as an optimization problem that can be solved quickly and accurately. Moreover, our framework enables refinement of the poses of the input particle images, a capability absent from most heterogeneity-analysis methods, and all covariance-based methods. Numerical experiments on both synthetic and experimental datasets demonstrate that the algorithm accurately captures dominant components of variability while maintaining computational efficiency. SOLVAR achieves state-of-the-art performance across multiple datasets in a recent heterogeneity benchmark. The code of the algorithm is freely available at https://github.com/RoeyYadgar/SOLVAR.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.ML",
        "stat.AP"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2602.17603v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17603v1",
      "doi": null
    },
    {
      "id": "2602.17592",
      "title": "BMW: Bayesian Model-Assisted Adaptive Phase II Clinical Trial Design for Win Ratio Statistic",
      "authors": [
        "Di Zhu",
        "Yong Zang"
      ],
      "abstract": "The win ratio (WR) statistic is increasingly used to evaluate treatment effects based on prioritized composite endpoints, yet existing Bayesian adaptive designs are not directly applicable because the WR is a summary statistic derived from pairwise comparisons and does not correspond to a unique data-generating mechanism. We propose a Bayesian model-assisted adaptive design for randomized phase II clinical trials based on the WR statistic, referred to as the BMW design. The proposed design uses the joint asymptotic distribution of WR test statistics across interim and final analyses to compute posterior probabilities without specifying the underlying outcome distribution. The BMW design allows flexible interim monitoring with early stopping for futility or superiority and is extended to jointly evaluate efficacy and toxicity using a graphical testing procedure that controls the family-wise error rate (FWER). Simulation studies demonstrate that the BMW design maintains valid type I error and FWER control, achieves power comparable to conventional methods, and substantially reduces expected sample size. An R Shiny application is provided to facilitate practical implementation.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.ME",
        "stat.AP"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.17592v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17592v1",
      "doi": null
    },
    {
      "id": "2602.17543",
      "title": "genriesz: A Python Package for Automatic Debiased Machine Learning with Generalized Riesz Regression",
      "authors": [
        "Masahiro Kato"
      ],
      "abstract": "Efficient estimation of causal and structural parameters can be automated using the Riesz representation theorem and debiased machine learning (DML). We present genriesz, an open-source Python package that implements automatic DML and generalized Riesz regression, a unified framework for estimating Riesz representers by minimizing empirical Bregman divergences. This framework includes covariate balancing, nearest-neighbor matching, calibrated estimation, and density ratio estimation as special cases. A key design principle of the package is automatic regressor balancing (ARB): given a Bregman generator $g$ and a representer model class, genriesz} automatically constructs a compatible link function so that the generalized Riesz regression estimator satisfies balancing (moment-matching) optimality conditions in a user-chosen basis. The package provides a modulr interface for specifying (i) the target linear functional via a black-box evaluation oracle, (ii) the representer model via basis functions (polynomial, RKHS approximations, random forest leaf encodings, neural embeddings, and a nearest-neighbor catchment basis), and (iii) the Bregman generator, with optional user-supplied derivatives. It returns regression adjustment (RA), Riesz weighting (RW), augmented Riesz weighting (ARW), and TMLE-style estimators with cross-fitting, confidence intervals, and $p$-values. We highlight representative workflows for estimation problems such as the average treatment effect (ATE), ATE on treated (ATT), and average marginal effect estimation. The Python package is available at https://github.com/MasaKat0/genriesz and on PyPI.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.ML",
        "cs.LG",
        "econ.EM",
        "math.ST",
        "stat.ME"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2602.17543v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17543v1",
      "doi": null
    },
    {
      "id": "2602.17503",
      "title": "An extension to reversible jump Markov chain Monte Carlo for change point problems with heterogeneous temporal dynamics",
      "authors": [
        "Emily Gribbin",
        "Benjamin Davis",
        "Daniel Rolfe",
        "Hannah Mitchell"
      ],
      "abstract": "Detecting brief changes in time-series data remains a major challenge in fields where short-lived states carry meaning. In single-molecule localisation microscopy, this problem is particularly acute as fluorescent molecules used to tag protein oligomers display heterogenous photophysical behaviour that can complicate photobleach step analysis; a key step in resolving nanoscale protein organisation. Existing methods often require extensive filtering or prior calibration, and can fail to accurately account for blinking or reversible dark states that may contaminate downstream analysis. In this paper, an extension to RJMCMC is proposed for change point detection with heterogeneous temporal dynamics. This approach is applied to the problem of estimating per-frame active fluorophore counts from one-dimensional integrated intensity traces derived from Fluorescence Localisation Imaging with Photobleaching (FLImP), where compound change point pair moves are introduced to better account for short-lived events known as blinking and dark states. The approach is validated using simulated and experimental data, demonstrating improved accuracy and robustness when compared with current photobleach step analysis methods and with the existing analysis approach for FLImP data. This Compound RJMCMC (CRJMCMC) algorithm performs reliably across a wide range of fluorophore counts and signal-to-noise conditions, with signal-to-noise ratio (SNR) down to 0.001 and counts as high as seventeen fluorophores, while also effectively estimating low counts observed when studying EGFR oligomerisation. Beyond single molecule imaging, this work has applications for a variety of time series change point detection problems with heterogeneous state persistence. For example, electrocorticography brain-state segmentation, fault detection in industrial process monitoring and realised volatility in financial time series.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.17503v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17503v1",
      "doi": null
    },
    {
      "id": "2602.17414",
      "title": "Nested Sampling with Slice-within-Gibbs: Efficient Evidence Calculation for Hierarchical Bayesian Models",
      "authors": [
        "David Yallup"
      ],
      "abstract": "We present Nested Sampling with Slice-within-Gibbs (NS-SwiG), an algorithm for Bayesian inference and evidence estimation in high-dimensional models whose likelihood admits a factorization, such as hierarchical Bayesian models. We construct a procedure to sample from the likelihood-constrained prior using a Slice-within-Gibbs kernel: an outer update of hyperparameters followed by inner block updates over local parameters. A likelihood-budget decomposition caches per-block contributions so that each local update checks feasibility in constant time rather than recomputing the global constraint at linearly growing cost. This reduces the per-replacement cost from quadratic to linear in the number of groups, and the overall algorithmic complexity from cubic to quadratic under standard assumptions. The decomposition extends naturally beyond independent observations, and we demonstrate this on Markov-structured latent variables. We evaluate NS-SwiG on challenging benchmarks, demonstrating scalability to thousands of dimensions and accurate evidence estimates even on posterior geometries where state-of-the-art gradient-based samplers can struggle.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.CO",
        "astro-ph.IM",
        "stat.ME"
      ],
      "primaryCategory": "stat.CO",
      "pdfUrl": "https://arxiv.org/pdf/2602.17414v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17414v1",
      "doi": null
    },
    {
      "id": "2602.17272",
      "title": "Estimating Zero-inflated Negative Binomial GAMLSS via a Balanced Gradient Boosting Approach with an Application to Antenatal Care Data from Nigeria",
      "authors": [
        "Alexandra Daub",
        "Elisabeth Bergherr"
      ],
      "abstract": "Statistical boosting algorithms are renowned for their intrinsic variable selection and enhanced predictive performance compared to classical statistical methods, making them especially useful for complex models such as generalized additive models for location scale and shape (GAMLSS). Boosting this model class can suffer from imbalanced updates across the distribution parameters as well as long computation times. Shrunk optimal step lengths have been shown to address these issues. To examine the influence of socio-economic factors on the distribution of the number of antenatal care visits in Nigeria, we generalize boosting of GAMLSS with shrunk optimal step lengths to base-learners beyond simple linear models and to a more complex response variable distribution. In an extensive simulation study and in the application we demonstrate that shrunk optimal step lengths yield a more balanced regularization of the overall model and enhance computational efficiency across diverse settings, in particular in the presence of base-learners penalizing the size of the fit.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.17272v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17272v1",
      "doi": null
    },
    {
      "id": "2602.17262",
      "title": "Quantifying and Mitigating Socially Desirable Responding in LLMs: A Desirability-Matched Graded Forced-Choice Psychometric Study",
      "authors": [
        "Kensuke Okada",
        "Yui Furukawa",
        "Kyosuke Bunji"
      ],
      "abstract": "Human self-report questionnaires are increasingly used in NLP to benchmark and audit large language models (LLMs), from persona consistency to safety and bias assessments. Yet these instruments presume honest responding; in evaluative contexts, LLMs can instead gravitate toward socially preferred answers-a form of socially desirable responding (SDR)-biasing questionnaire-derived scores and downstream conclusions. We propose a psychometric framework to quantify and mitigate SDR in questionnaire-based evaluation of LLMs. To quantify SDR, the same inventory is administered under HONEST versus FAKE-GOOD instructions, and SDR is computed as a direction-corrected standardized effect size from item response theory (IRT)-estimated latent scores. This enables comparisons across constructs and response formats, as well as against human instructed-faking benchmarks. For mitigation, we construct a graded forced-choice (GFC) Big Five inventory by selecting 30 cross-domain pairs from an item pool via constrained optimization to match desirability. Across nine instruction-tuned LLMs evaluated on synthetic personas with known target profiles, Likert-style questionnaires show consistently large SDR, whereas desirability-matched GFC substantially attenuates SDR while largely preserving the recovery of the intended persona profiles. These results highlight a model-dependent SDR-recovery trade-off and motivate SDR-aware reporting practices for questionnaire-based benchmarking and auditing of LLMs.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "cs.CL",
        "stat.ME"
      ],
      "primaryCategory": "cs.CL",
      "pdfUrl": "https://arxiv.org/pdf/2602.17262v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17262v1",
      "doi": null
    },
    {
      "id": "2602.17261",
      "title": "Parametric or nonparametric: the FIC approach for stationary time series",
      "authors": [
        "Gudmund Hermansen",
        "Nils Lid Hjort",
        "Martin Jullum"
      ],
      "abstract": "We seek to narrow the gap between parametric and nonparametric modelling of stationary time series processes. The approach is inspired by recent advances in focused inference and model selection techniques. The paper generalises and extends recent work by developing a new version of the focused information criterion (FIC), directly comparing the performance of parametric time series models with a nonparametric alternative. For a pre-specified focused parameter, for which scrutiny is considered valuable, this is achieved by comparing the mean squared error of the model-based estimators of this quantity. In particular, this yields FIC formulae for covariances or correlations at specified lags, for the probability of reaching a threshold, etc. Suitable weighted average versions, the AFIC, also lead to model selection strategies for finding the best model for the purpose of estimating e.g.~a sequence of correlations.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.17261v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17261v1",
      "doi": null
    },
    {
      "id": "2602.17255",
      "title": "Selection and Collider Restriction Bias Due to Predictor Availability in Prognostic Models",
      "authors": [
        "Marc Delord"
      ],
      "abstract": "This methodological note investigates and discuss possible selection and collider restriction bias due to predictor availability in prognostic models.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.ME",
        "stat.AP"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.17255v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17255v1",
      "doi": null
    },
    {
      "id": "2602.17225",
      "title": "Wide-Surface Furnace for In Situ X-Ray Diffraction of Combinatorial Samples using a High-Throughput Approach",
      "authors": [
        "Giulio Cordaro",
        "Juande Sirvent",
        "Cristian Mocuta",
        "Fjorelo Buzi",
        "Thierry Martin",
        "Federico Baiutti",
        "Alex Morata",
        "Albert Tarancòn",
        "Dominique Thiaudière",
        "Guilhem Dezanneau"
      ],
      "abstract": "The combinatorial approach applied to functional oxides has enabled the production of material libraries that formally contain infinite compositions. A complete ternary diagram can be obtained by pulsed laser deposition (PLD) on 100 mm silicon wafers. However, interest in such materials libraries is only meaningful if high-throughput characterization enables the information extraction from the as-deposited library in a reasonable time. While much commercial equipment allows for XY-resolved characterization at room temperature, very few sample holders have been made available to investigate structural, chemical, and functional properties at high temperatures in controlled atmospheres. In the present work, we present a furnace that enables the study of 100 mm wafers as a function of temperature. This furnace has a dome to control the atmosphere, typically varying from nitrogen gas to pure oxygen atmosphere with external control. We present the design of such a furnace and an example of X-ray diffraction (XRD) and fluorescence (XRF) measurements performed at the DiffAbs beamline of the SOLEIL synchrotron. We apply this high-throughput approach to a combinatorial library up to 735 {\\textdegree}C in nitrogen and calculate the thermal expansion coefficients (TEC) of the ternary system using custom-made MATLAB codes. The TEC analysis revealed the potential limitations of Vegard's law in predicting lattice variations for high-entropy materials.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "cond-mat.mtrl-sci",
        "physics.data-an",
        "stat.ME"
      ],
      "primaryCategory": "cond-mat.mtrl-sci",
      "pdfUrl": "https://arxiv.org/pdf/2602.17225v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17225v1",
      "doi": null
    },
    {
      "id": "2602.17161",
      "title": "Dynamic likelihood hazard rate estimation",
      "authors": [
        "Nils Lid Hjort"
      ],
      "abstract": "The best known methods for estimating hazard rate functions in survival analysis models are either purely parametric or purely nonparametric. The parametric ones are sometimes too biased while the nonparametric ones are sometimes too variable. In the present paper a certain semiparametric approach to hazard rate estimation, proposed in Hjort (1991), is developed further, aiming to combine parametric and nonparametric features. It uses a dynamic local likelihood approach to fit the locally most suitable member in a given parametric class of hazard rates, and amounts to a version of nonparametric parameter smoothing within the parametric class. Thus the parametric hazard rate estimate at time $s$ inserts a parameter estimate that also depends on $s$. We study bias and variance properties of the resulting estimator and methods for choosing the local smoothing parameter. It is shown that dynamic likelihood estimation often leads to better performance than the purely nonparametric methods, while also having capacity for not losing much to the parametric methods in cases where the model being smoothed is adequate.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.17161v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17161v1",
      "doi": null
    },
    {
      "id": "2602.17079",
      "title": "Environmental policy in the context of complex systems: Statistical optimization and sensitivity analysis for ABMs",
      "authors": [
        "Dylan Munson",
        "Arijit Dey",
        "Simon Mak"
      ],
      "abstract": "Coupled human-environment systems are increasingly being understood as complex adaptive systems (CAS), in which micro-level interactions between components lead to emergent behavior. Agent-based models (ABMs) hold great promise for environmental policy design by capturing such complex behavior, enabling a sophisticated understanding of potential interventions. One limitation, however, is that ABMs can be computationally costly to simulate, which hinders their use for policy optimization. To address this, we propose a new statistical framework that exploits machine learning techniques to accelerate policy optimization with costly ABMs. We first develop a statistical approach for sensitivity testing of the optimal policy, then leverage a reinforcement learning method for efficient policy optimization. We test this framework on the classic ``Sugarscape'' model, an ABM for resource harvesting. We show that our approach can quickly identify optimal and interpretable policies that improve upon baseline techniques, with insightful sensitivity and dynamic analyses that connect back to economic theory.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.AP"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.17079v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17079v1",
      "doi": null
    },
    {
      "id": "2602.17070",
      "title": "General sample size analysis for probabilities of causation: a delta method approach",
      "authors": [
        "Tianyuan Cheng",
        "Ruirui Mao",
        "Judea Pearl",
        "Ang Li"
      ],
      "abstract": "Probabilities of causation (PoCs), such as the probability of necessity and sufficiency (PNS), are important tools for decision making but are generally not point identifiable. Existing work has derived bounds for these quantities using combinations of experimental and observational data. However, there is very limited research on sample size analysis, namely, how many experimental and observational samples are required to achieve a desired margin of error. In this paper, we propose a general sample size framework based on the delta method. Our approach applies to settings in which the target bounds of PoCs can be expressed as finite minima or maxima of linear combinations of experimental and observational probabilities. Through simulation studies, we demonstrate that the proposed sample size calculations lead to stable estimation of these bounds.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.ME",
        "cs.AI"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.17070v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17070v1",
      "doi": null
    },
    {
      "id": "2602.17052",
      "title": "Generative modeling for the bootstrap",
      "authors": [
        "Leon Tran",
        "Ting Ye",
        "Peng Ding",
        "Fang Han"
      ],
      "abstract": "Generative modeling builds on and substantially advances the classical idea of simulating synthetic data from observed samples. This paper shows that this principle is not only natural but also theoretically well-founded for bootstrap inference: it yields statistically valid confidence intervals that apply simultaneously to both regular and irregular estimators, including settings in which Efron's bootstrap fails. In this sense, the generative modeling-based bootstrap can be viewed as a modern version of the smoothed bootstrap: it could mitigate the curse of dimensionality and remain effective in challenging regimes where estimators may lack root-$n$ consistency or a Gaussian limit.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.ME",
        "econ.EM"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.17052v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17052v1",
      "doi": null
    },
    {
      "id": "2602.17043",
      "title": "Quantifying the limits of human athletic performance: A Bayesian analysis of elite decathletes",
      "authors": [
        "Paul-Hieu V. Nguyen",
        "James M. Smoliga",
        "Benton Lindaman",
        "Sameer K. Deshpande"
      ],
      "abstract": "Because the decathlon tests many facets of athleticism, including sprinting, throwing, jumping, and endurance, many consider it to be the ultimate test of athletic ability. On this view, estimating the maximal decathlon score and understanding what it would take to achieve that score provides insight into the upper limits of human athletic potential. To this end, we develop a Bayesian composition model for forecasting how individual athletes perform in each of the 10 decathlon events of time. Besides capturing potential non-linear temporal trends in performance, our model carefully captures the dependence between performance in an event and all preceding events. Using our model, we can simulate and evaluate the distribution of the maximal possible scores and identify profiles of athletes who could realistically attain scores approaching this limit.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.AP"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.17043v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17043v1",
      "doi": null
    },
    {
      "id": "2602.17041",
      "title": "Reframing Population-Adjusted Indirect Comparisons as a Transportability Problem: An Estimand-Based Perspective and Implications for Health Technology Assessment",
      "authors": [
        "Conor Chandler",
        "Jack Ishak"
      ],
      "abstract": "Population-adjusted indirect comparisons (PAICs) are widely used to synthesize evidence when randomized controlled trials enroll different patient populations and head-to-head comparisons are unavailable. Although PAICs adjust for observed population differences across trials, adjustment alone does not ensure transportability of estimated effects to decision-relevant populations for health technology assessment (HTA). We examine and formalize transportability in PAICs from an estimand-based perspective. We distinguish conditional and marginal treatment effect estimands and show how transportability depends on effect modification, collapsibility, and alignment between the scale of effect modification and the effect measure. Using illustrative examples, we demonstrate that even when effect modifiers are shared across treatments, marginal effects are generally population-dependent for commonly used non-collapsible measures, including hazard ratios and odds ratios. Conversely, collapsible and conditional effects defined on the linear predictor scale exhibit more favorable transportability properties. We further show that pairwise PAIC approaches typically identify effects defined in the comparator population and that applying these estimates to other populations entails an additional, often implicit, transport step requiring further assumptions. This has direct implications for HTA, where PAIC-derived effects are routinely applied within cost-effectiveness and decision models defined for different target populations. Our results clarify when applying PAIC-derived treatment effects to desired target populations is justified, when doing so requires additional assumptions, and when results should instead be interpreted as population-specific rather than decision-relevant, supporting more transparent and principled use of indirect evidence in HTA and related decision-making contexts.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.17041v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17041v1",
      "doi": null
    },
    {
      "id": "2602.17034",
      "title": "Using Time Series Measures to Explore Family Planning Survey Data and Model-based Estimates",
      "authors": [
        "Oluwayomi Akinfenwa",
        "Niamh Cahill",
        "Catherine Hurley"
      ],
      "abstract": "Family planning is a global development priority and a key indicator of reproductive health. Monitoring progress is challenged by gaps in survey data across countries. The United Nations Population Division addresses this with the Family Planning Estimation Model (FPEM), a Bayesian hierarchical time series model producing annual estimates of modern contraceptive use while sharing information across countries and regions. This paper evaluates how well FPEM estimates align with survey data using time series diagnostic indices from the wdiexplorer R package, which account for countries nested within sub-regions. Visualisation of survey data, modelled trajectories, and diagnostics enables assessment of model performance, highlighting where trends align and where discrepancies occur.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.AP"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.17034v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17034v1",
      "doi": null
    },
    {
      "id": "2602.16992",
      "title": "Modeling Multivariate Missingness with Tree Graphs and Conjugate Odds",
      "authors": [
        "Daniel Suen",
        "Yen-Chi Chen"
      ],
      "abstract": "In this paper, we analyze a specific class of missing not at random (MNAR) assumptions called tree graphs, extending upon the work of pattern graphs. We build off previous work by introducing the idea of a conjugate odds family in which certain parametric models on the selection odds can preserve the data distribution family across all missing data patterns. Under a conjugate odds family and a tree graph assumption, we are able to model the full data distribution elegantly in the sense that for the observed data, we obtain a model that is conjugate from the complete-data, and for the missing entries, we create a simple imputation model. In addition, we investigate the problem of graph selection, sensitivity analysis, and statistical inference. Using both simulations and real data, we illustrate the applicability of our method.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.16992v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16992v1",
      "doi": null
    },
    {
      "id": "2602.16970",
      "title": "Temperature and Respiratory Emergency Department Visits: A Mediation Analysis with Ambient Ozone Exposure",
      "authors": [
        "Chen Li",
        "Thomas W. Hsiao",
        "Stefanie Ebelt",
        "Rebecca H. Zhang",
        "Howard H. Chang"
      ],
      "abstract": "High temperatures are associated with adverse respiratory health outcomes and increases in ambient air pollution. Limited research has quantified air pollution's mediating role in the relationship between temperature and respiratory morbidity, such as emergency department (ED) visits. In this study, we conducted a causal mediation analysis to decompose the total effect of daily temperature on respiratory ED visits in Los Angeles from 2005 to 2016. We focused on ambient ozone as a mediator because its precursors and formation are directly driven by sunlight and temperature. We estimated natural direct, indirect, and total effects on the relative risk scale across deciles of temperature exposure compared to the median. We utilized Bayesian additive regression trees (BART) to flexibly characterize the nonlinear relationship between temperature and ozone and quantified uncertainty via posterior prediction and the Bayesian bootstrap. Our results showed that ozone partially mediated the association between high temperatures and respiratory ED visits, particularly at moderately high temperatures. We also validated our modeling approach through simulation studies. This study extends the existing literature by considering acute respiratory morbidity and employing a flexible modeling approach, offering new insights into the mechanisms underlying temperature-related health risks.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "stat.AP"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.16970v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16970v1",
      "doi": null
    },
    {
      "id": "2602.16933",
      "title": "M-estimation under Two-Phase Multiwave Sampling with Applications to Prediction-Powered Inference",
      "authors": [
        "Dan M. Kluger",
        "Stephen Bates"
      ],
      "abstract": "In two-phase multiwave sampling, inexpensive measurements are collected on a large sample and expensive, more informative measurements are adaptively obtained on subsets of units across multiple waves. Adaptively collecting the expensive measurements can increase efficiency but complicates statistical inference. We give valid estimators and confidence intervals for M-estimation under adaptive two-phase multiwave sampling. We focus on the case where proxies for the expensive variables -- such as predictions from pretrained machine learning models -- are available for all units and propose a Multiwave Predict-Then-Debias estimator that combines proxy information with the expensive, higher-quality measurements to improve efficiency while removing bias. We establish asymptotic linearity and normality and propose asymptotically valid confidence intervals. We also develop an approximately greedy sampling strategy that improves efficiency relative to uniform sampling. Data-based simulation studies support the theoretical results and demonstrate efficiency gains.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.ML"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.16933v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16933v1",
      "doi": null
    },
    {
      "id": "2602.16914",
      "title": "A statistical perspective on transformers for small longitudinal cohort data",
      "authors": [
        "Kiana Farhadyar",
        "Maren Hackenberg",
        "Kira Ahrens",
        "Charlotte Schenk",
        "Bianca Kollmann",
        "Oliver Tüscher",
        "Klaus Lieb",
        "Michael M. Plichta",
        "Andreas Reif",
        "Raffael Kalisch",
        "Martin Wolkewitz",
        "Moritz Hess",
        "Harald Binder"
      ],
      "abstract": "Modeling of longitudinal cohort data typically involves complex temporal dependencies between multiple variables. There, the transformer architecture, which has been highly successful in language and vision applications, allows us to account for the fact that the most recently observed time points in an individual's history may not always be the most important for the immediate future. This is achieved by assigning attention weights to observations of an individual based on a transformation of their values. One reason why these ideas have not yet been fully leveraged for longitudinal cohort data is that typically, large datasets are required. Therefore, we present a simplified transformer architecture that retains the core attention mechanism while reducing the number of parameters to be estimated, to be more suitable for small datasets with few time points. Guided by a statistical perspective on transformers, we use an autoregressive model as a starting point and incorporate attention as a kernel-based operation with temporal decay, where aggregation of multiple transformer heads, i.e. different candidate weighting schemes, is expressed as accumulating evidence on different types of underlying characteristics of individuals. This also enables a permutation-based statistical testing procedure for identifying contextual patterns. In a simulation study, the approach is shown to recover contextual dependencies even with a small number of individuals and time points. In an application to data from a resilience study, we identify temporal patterns in the dynamics of stress and mental health. This indicates that properly adapted transformers can not only achieve competitive predictive performance, but also uncover complex context dependencies in small data settings.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "stat.ME",
        "cs.LG",
        "stat.ML"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.16914v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16914v1",
      "doi": null
    },
    {
      "id": "2602.16830",
      "title": "The Impact of Formations on Football Matches Using Double Machine Learning. Is it worth parking the bus?",
      "authors": [
        "Genís Ruiz-Menárguez",
        "Llorenç Badiella"
      ],
      "abstract": "This study addresses a central tactical dilemma for football coaches: whether to employ a defensive strategy, colloquially known as \"parking the bus\", or a more offensive one. Using an advanced Double Machine Learning (DML) framework, this project provides a robust and interpretable tool to estimate the causal impact of different formations on key match outcomes such as goal difference, possession, corners, and disciplinary actions. Leveraging a dataset of over 22,000 matches from top European leagues, formations were categorized into six representative types based on tactical structure and expert consultation. A major methodological contribution lies in the adaptation of DML to handle categorical treatments, specifically formation combinations, through a novel matrix-based residualization process, allowing for a detailed estimation of formation-versus-formation effects that can inform a coach's tactical decision-making. Results show that while offensive formations like 4-3-3 and 4-2-3-1 offer modest statistical advantages in possession and corners, their impact on goals is limited. Furthermore, no evidence supports the idea that defensive formations, commonly associated with parking the bus, increase a team's winning potential. Additionally, red cards appear unaffected by formation choice, suggesting other behavioral factors dominate. Although this approach does not fully capture all aspects of playing style or team strength, it provides a valuable framework for coaches to analyze tactical efficiency and sets a precedent for future research in sports analytics.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "stat.AP",
        "cs.LG"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.16830v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16830v1",
      "doi": null
    },
    {
      "id": "2602.16789",
      "title": "First versus full or first versus last: U-statistic change-point tests under fixed and local alternatives",
      "authors": [
        "Herold Dehling",
        "Daniel Vogel",
        "Martin Wendler"
      ],
      "abstract": "The use of U-statistics in the change-point context has received considerable attention in the literature. We compare two approaches of constructing CUSUM-type change-point tests, which we call the first-vs-full and first-vs-last approach. Both have been pursued by different authors. The question naturally arises if the two tests substantially differ and, if so, which of them is better in which data situation. In large samples, both tests are similar: they are asymptotically equivalent under the null hypothesis and under sequences of local alternatives. In small samples, there may be quite noticeable differences, which is in line with a different asymptotic behavior under fixed alternatives. We derive a simple criterion for deciding which test is more powerful. We examine the examples Gini's mean difference, the sample variance, and Kendall's tau in detail. Particularly, when testing for changes in scale by Gini's mean difference, we show that the first-vs-full approach has a higher power if and only if the scale changes from a smaller to a larger value -- regardless of the population distribution or the location of the change. The asymptotic derivations are under weak dependence. The results are illustrated by numerical simulations and data examples.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.16789v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16789v1",
      "doi": null
    },
    {
      "id": "2602.16784",
      "title": "Omitted Variable Bias in Language Models Under Distribution Shift",
      "authors": [
        "Victoria Lin",
        "Louis-Philippe Morency",
        "Eli Ben-Michael"
      ],
      "abstract": "Despite their impressive performance on a wide variety of tasks, modern language models remain susceptible to distribution shifts, exhibiting brittle behavior when evaluated on data that differs in distribution from their training data. In this paper, we describe how distribution shifts in language models can be separated into observable and unobservable components, and we discuss how established approaches for dealing with distribution shift address only the former. Importantly, we identify that the resulting omitted variable bias from unobserved variables can compromise both evaluation and optimization in language models. To address this challenge, we introduce a framework that maps the strength of the omitted variables to bounds on the worst-case generalization performance of language models under distribution shift. In empirical experiments, we show that using these bounds directly in language model evaluation and optimization provides more principled measures of out-of-distribution performance, improves true out-of-distribution performance relative to standard distribution shift adjustment methods, and further enables inference about the strength of the omitted variables when target distribution labels are available.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "categories": [
        "cs.LG",
        "cs.CL",
        "stat.ME"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2602.16784v1",
      "arxivUrl": "http://arxiv.org/abs/2602.16784v1",
      "doi": null
    }
  ],
  "metadata": {
    "lastUpdated": "2026-02-23T02:53:23.027Z",
    "totalPapers": 50,
    "categories": [
      "stat.ME",
      "stat.AP",
      "econ.EM",
      "q-bio.QM"
    ],
    "queryDate": "2026-02-23"
  }
}