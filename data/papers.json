{
  "papers": [
    {
      "id": "2602.19329",
      "title": "Dynamic Elasticity Between Forest Loss and Carbon Emissions: A Subnational Panel Analysis of the United States",
      "authors": [
        "Keonvin Park"
      ],
      "abstract": "Accurate quantification of the relationship between forest loss and associated carbon emissions is critical for both environmental monitoring and policy evaluation. Although many studies have documented spatial patterns of forest degradation, there is limited understanding of the dynamic elasticity linking tree cover loss to carbon emissions at subnational scales. In this paper, we construct a comprehensive panel dataset of annual forest loss and carbon emission estimates for U.S. subnational administrative units from 2001 to 2023, based on the Hansen Global Forest Change dataset. We apply fixed effects and dynamic panel regression techniques to isolate within-region variation and account for temporal persistence in emissions. Our results show that forest loss has a significant positive short-run elasticity with carbon emissions, and that emissions exhibit strong persistence over time. Importantly, the estimated long-run elasticity, accounting for autoregressive dynamics, is substantially larger than the short-run effect, indicating cumulative impacts of repeated forest loss events. These findings highlight the importance of modeling temporal dynamics when assessing environmental responses to land cover change. The dynamic elasticity framework proposed here offers a robust and interpretable tool for analyzing environmental change processes, and can inform both regional monitoring systems and carbon accounting frameworks.",
      "published": "2026-02-22",
      "updated": "2026-02-22",
      "categories": [
        "stat.AP",
        "cs.LG"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.19329v1",
      "arxivUrl": "http://arxiv.org/abs/2602.19329v1",
      "doi": null
    },
    {
      "id": "2602.19296",
      "title": "A Causal Framework for Estimating Heterogeneous Effects of On-Demand Tutoring",
      "authors": [
        "Kirk Vanacore",
        "Danielle R Thomas",
        "Digory Smith",
        "Bibi Groot",
        "Justin Reich",
        "Rene Kizilcec"
      ],
      "abstract": "This paper introduces a scalable causal inference framework for estimating the immediate, session-level effects of on-demand human tutoring embedded within adaptive learning systems. Because students seek assistance at moments of difficulty, conventional evaluation is confounded by self-selection and time-varying knowledge states. We address these challenges by integrating principled analytic sample construction with Deep Knowledge Tracing (DKT) to estimate latent mastery, followed by doubly robust estimation using Causal Forests. Applying this framework to over 5,000 middle-school mathematics tutoring sessions, we find that requesting human tutoring increases next-problem correctness by approximately 4 percentage points and accuracy on the subsequent skill encountered by approximately 3 percentage points, suggesting that the effects of tutoring have proximal transfer across knowledge components. This effect is robust to various forms of model specification and potential unmeasured confounders. Notably, these effects exhibit significant heterogeneity across sessions and students, with session-level effect estimates ranging from $-20.25pp$ to $+19.91pp$. Our follow-up analyses suggest that typical behavioral indicators, such as student talk time, do not consistently correlate with high-impact sessions. Furthermore, treatment effects are larger for students with lower prior mastery and slightly smaller for low-SES students. This framework offers a rigorous, practical template for the evaluation and continuous improvement of on-demand human tutoring, with direct applications for emerging AI tutoring systems.",
      "published": "2026-02-22",
      "updated": "2026-02-22",
      "categories": [
        "cs.HC",
        "stat.AP"
      ],
      "primaryCategory": "cs.HC",
      "pdfUrl": "https://arxiv.org/pdf/2602.19296v1",
      "arxivUrl": "http://arxiv.org/abs/2602.19296v1",
      "doi": null
    },
    {
      "id": "2602.19295",
      "title": "Time-Varying Hazard Patterns and Co-Mutation Profiles of KRAS G12C and G12D in Real-World NSCLC",
      "authors": [
        "Robert Amevor",
        "Dennis Baidoo",
        "Emmanuel Kubuafor"
      ],
      "abstract": "Background: KRAS mutations are the largest oncogenic subset in NSCLC. While KRAS G12C is now targetable, no approved therapies exist for G12D. We examined time-to-next-treatment (TTNT) and overall survival (OS) differences between G12C and G12D, allowing for time-varying hazard effects. Methods: De-identified data from AACR Project GENIE BPC NSCLC v2.0-public were analyzed. TTNT served as a real-world surrogate for progression-free survival. Co-mutations (TP53, STK11, KEAP1, SMARCA4, MET), TMB, and PD-L1 were harmonized. Kaplan-Meier, multivariable Cox, and a pre-specified piecewise Cox model (split at median TTNT = 23 months) were applied. Schoenfeld residuals assessed proportional hazards; bootstrap resampling (B=1000) evaluated stability. Results: Among 162 TTNT-evaluable patients (G12C n=130; G12D n=32), median TTNT was 28.6 versus 32.0 months (log-rank p=0.79). Adjusted Cox regression showed no overall hazard difference (HR=0.85; 95% CI 0.53-1.37; p=0.50), but Schoenfeld testing indicated borderline non-proportionality (p=0.053). Piecewise Cox modeling revealed time-varying effects: early TTNT hazard favored G12D (HR=0.41; 95% CI 0.17-0.97; p=0.043) with significant KRAS x period interaction (HR=3.33; p=0.021) and late-period attenuation (HR=1.38; 95% CI 0.77-2.47; p=0.285). Bootstrap resampling confirmed this pattern (median HRearly=0.39; HRlate=1.41). Among 278 OS-evaluable patients (133 deaths), G12D showed improved OS (adjusted HR=0.63; 95% CI 0.39-0.99; p=0.048). G12C tumors exhibited higher TMB (9.79 vs 7.83 mut/Mb; p=0.002) and greater STK11/KEAP1 enrichment. Conclusions: KRAS G12D demonstrated early TTNT advantage and improved OS. Late-period TTNT differences were non-significant (post-hoc power: 12.3%). These exploratory findings require validation in larger cohorts but support allele-specific therapeutic development for G12D.",
      "published": "2026-02-22",
      "updated": "2026-02-22",
      "categories": [
        "q-bio.QM",
        "stat.AP",
        "stat.ME"
      ],
      "primaryCategory": "q-bio.QM",
      "pdfUrl": "https://arxiv.org/pdf/2602.19295v1",
      "arxivUrl": "http://arxiv.org/abs/2602.19295v1",
      "doi": null
    },
    {
      "id": "2602.19290",
      "title": "Distributional Discontinuity Design",
      "authors": [
        "Kyle Schindl",
        "Larry Wasserman"
      ],
      "abstract": "Regression discontinuity and kink designs are typically analyzed through mean effects, even when treatment changes the shape of the entire outcome distribution. To address this, we introduce distributional discontinuity designs, a framework for estimating causal effects for a scalar outcome at the boundary of a discontinuity in treatment assignment. Our estimand is the Wasserstein distance between limiting conditional outcome distributions; a single scale-interpretable measure of distribution shift. We show that this weakly bounds the average treatment effect, where equality holds if and only if the treatment effect is purely additive; thus, departure from equality measures effect heterogeneity. To further encode effect heterogeneity we show that the Wasserstein distance admits an orthogonal decomposition into squared differences in $L$-moments, thereby quantifying the contribution from location, scale, skewness, and higher-order shape components to the overall distributional distance. Next, we extend this framework to distributional kink designs by evaluating the Wasserstein derivative at a policy kink; this describes the flow of probability mass through the kink. In the case of fuzzy kink designs, we derive new identification results. Finally, we apply our methods on real data by re-analyzing two natural experiments to compare our distributional effects to traditional causal estimands.",
      "published": "2026-02-22",
      "updated": "2026-02-22",
      "categories": [
        "stat.ME",
        "econ.EM",
        "math.ST"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.19290v1",
      "arxivUrl": "http://arxiv.org/abs/2602.19290v1",
      "doi": null
    },
    {
      "id": "2602.19287",
      "title": "Asymptotic theory of range-based multipower variation",
      "authors": [
        "Kim Christensen",
        "Mark Podolskij"
      ],
      "abstract": "In this paper, we present a realized range-based multipower variation theory, which can be used to estimate return variation and draw jump-robust inference about the diffusive volatility component, when a high-frequency record of asset prices is available. The standard range-statistic -- routinely used in financial economics to estimate the variance of securities prices -- is shown to be biased when the price process contains jumps. We outline how the new theory can be applied to remove this bias by constructing a hybrid range-based estimator. Our asymptotic theory also reveals that when high-frequency data are sparsely sampled, as is often done in practice due to the presence of microstructure noise, the range-based multipower variations can produce significant efficiency gains over comparable subsampled return-based estimators. The analysis is supported by a simulation study and we illustrate the practical use of our framework on some recent TAQ equity data.",
      "published": "2026-02-22",
      "updated": "2026-02-22",
      "categories": [
        "econ.EM"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.19287v1",
      "arxivUrl": "http://arxiv.org/abs/2602.19287v1",
      "doi": "10.1093/jjfinec/nbr019"
    },
    {
      "id": "2602.19284",
      "title": "Localized conformal model selection",
      "authors": [
        "Yuhao Wang",
        "Tengyao Wang"
      ],
      "abstract": "We propose a localized conformal model selection framework that integrates local adaptivity with post-selection validity for distribution-free prediction. By performing model selection symmetrically across calibration points using upper and lower surrogate intervals, we construct a data-dependent safe index set that contains the oracle model and preserves exchangeability. The resulting ensemble procedure retains exact finite-sample marginal coverage while adapting to spatial heterogeneity and model complexity. Simulations demonstrate substantial reductions in interval length compared to the best fixed model, especially in heterogeneous and low-noise settings.",
      "published": "2026-02-22",
      "updated": "2026-02-22",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.19284v1",
      "arxivUrl": "http://arxiv.org/abs/2602.19284v1",
      "doi": null
    },
    {
      "id": "2602.19279",
      "title": "Distributional Effects in Censored Quantile Regressions with Endogeneity and Heteroskedasticity",
      "authors": [
        "Xi Wang"
      ],
      "abstract": "Distributional effects, characterized by quantile frameworks, are well-known to capture heterogeneous impacts of economic factors across the unobserved relative ranks. Censored outcome, endogenous regressor and heteroskedastic error are prevalent in empirical work, yet challenge the consistency of existing quantile estimation methods. This paper develops a Sequential Control Function Censored Quantile (SCFCQ) estimator for distributional effects in censored quantile models with unbounded endogenous regressors. Our method combines the sequential analysis with the control function approach, particularly adapting for conditional heteroskedasticity in the endogenous regressor. The estimation algorithm is a two-step procedure composed of series quantile regressions, thereby providing applied researchers with a computationally tractable and practically feasible tool. We apply the SCFCQ method to estimate heterogeneous income elasticities over household preferences using data from the UK Family Expenditure Survey.",
      "published": "2026-02-22",
      "updated": "2026-02-22",
      "categories": [
        "econ.EM"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.19279v1",
      "arxivUrl": "http://arxiv.org/abs/2602.19279v1",
      "doi": null
    },
    {
      "id": "2602.19263",
      "title": "Prognostics of Multisensor Systems with Unknown and Unlabeled Failure Modes via Bayesian Nonparametric Process Mixtures",
      "authors": [
        "Kani Fu",
        "Sanduni S Disanayaka Mudiyanselage",
        "Chunli Dai",
        "Minhee Kim"
      ],
      "abstract": "Modern manufacturing systems often experience multiple and unpredictable failure behaviors, yet most existing prognostic models assume a fixed, known set of failure modes with labeled historical data. This assumption limits the use of digital twins for predictive maintenance, especially in high-mix or adaptive production environments, where new failure modes may emerge, and the failure mode labels may be unavailable. To address these challenges, we propose a novel Bayesian nonparametric framework that unifies a Dirichlet process mixture module for unsupervised failure mode discovery with a neural network-based prognostic module. The key innovation lies in an iterative feedback mechanism to jointly learn two modules. These modules iteratively update one another to dynamically infer, expand, or merge failure modes as new data arrive while providing high prognostic accuracy. Experiments on both simulation and aircraft engine datasets show that the proposed approach performs competitively with or significantly better than existing approaches. It also exhibits robust online adaptation capabilities, making it well-suited for digital-twin-based system health management in complex manufacturing environments.",
      "published": "2026-02-22",
      "updated": "2026-02-22",
      "categories": [
        "stat.AP",
        "cs.LG"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.19263v1",
      "arxivUrl": "http://arxiv.org/abs/2602.19263v1",
      "doi": null
    },
    {
      "id": "2602.19236",
      "title": "CoMET: A Compressed Bayesian Mixed-Effects Model for High-Dimensional Tensors",
      "authors": [
        "Sreya Sarkar",
        "Kshitij Khare",
        "Sanvesh Srivastava"
      ],
      "abstract": "Mixed-effects models are fundamental tools for analyzing clustered and repeated-measures data, but existing high-dimensional methods largely focus on penalized estimation with vector-valued covariates. Bayesian alternatives in this regime are limited, with no sampling-based mixed-effects framework that supports tensor-valued fixed- and random-effects covariates while remaining computationally tractable. We propose the Compressed Mixed-Effects Tensor (CoMET) model for high-dimensional repeated-measures data with scalar responses and tensor-valued covariates. CoMET performs structured, mode-wise random projection of the random-effects covariance, yielding a low-dimensional covariance parameter that admits simple Gaussian prior specification and enables efficient imputation of compressed random-effects. For the mean structure, CoMET leverages a low-rank tensor decomposition and margin-structured Horseshoe priors to enable fixed-effects selection. These design choices lead to an efficient collapsed Gibbs sampler whose computational complexity grows approximately linearly with the tensor covariate dimensions. We establish high-dimensional theoretical guarantees by identifying regularity conditions under which CoMET's posterior predictive risk decays to zero. Empirically, CoMET outperforms penalized competitors across a range of simulation studies and two benchmark applications involving facial-expression prediction and music emotion modeling.",
      "published": "2026-02-22",
      "updated": "2026-02-22",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.19236v1",
      "arxivUrl": "http://arxiv.org/abs/2602.19236v1",
      "doi": null
    },
    {
      "id": "2602.19220",
      "title": "A likelihood approach to proper analysis of secondary outcomes in matched case-control studies",
      "authors": [
        "Shanshan Liu",
        "Guoqing Diao"
      ],
      "abstract": "Matched case-control studies are commonly employed in epidemiological research for their convenience and efficiency. Analysis of secondary outcomes can yield valuable insights into biological pathways and help identify genetic variants of importance. Naive analysis using standard statistical methods, such as least-squares regression for quantitative traits, can be misleading because they fail to account for unequal sampling induced by the case-control design and matching. In this paper, we propose novel statistical methods that appropriately reflect the study design and sampling scheme in the analysis of secondary outcome data. The new methods provide consistent estimation and accurate coverage probabilities for the confidence interval estimators. We demonstrate the advantages of the new methods through simulation studies and a real application with diabetes patients. R code implementing the proposed methods is publicly available.",
      "published": "2026-02-22",
      "updated": "2026-02-22",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.19220v1",
      "arxivUrl": "http://arxiv.org/abs/2602.19220v1",
      "doi": null
    },
    {
      "id": "2602.19216",
      "title": "Statistical Measures for Explainable Aspect-Based Sentiment Analysis: A Case Study on Environmental Discourse in Reddit",
      "authors": [
        "Luisa Stracqualursi",
        "Patrizia Agati"
      ],
      "abstract": "Aspect-Based Sentiment Analysis (ABSA) provides a fine-grained understanding of opinions by linking sentiment to specific aspects in text. While transformer-based models excel at this task, their black-box nature limits their interpretability, posing risks in real-world applications without labeled data. This paper introduces a statistical, model-agnostic framework to assess the behavioral transparency and trustworthiness of ABSA models. Our framework relies on several metrics, such as the entropy of polarity distributions, soft-count-based dominance scores, and sentiment divergence between sources, whose robustness is validated through bootstrap resampling and sensitivity analysis. A case study on environmentally focused Reddit communities illustrates how the proposed indicators provide interpretable diagnostics of model certainty, decisiveness, and cross-source variability. The results show that statistical indicators computed on soft outputs can complement traditional approaches, offering a computationally efficient methodology for validating, monitoring, and interpreting ABSA models in contexts where labeled data are unavailable.",
      "published": "2026-02-22",
      "updated": "2026-02-22",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.19216v1",
      "arxivUrl": "http://arxiv.org/abs/2602.19216v1",
      "doi": "10.1080/02331888.2026.2636122"
    },
    {
      "id": "2602.19203",
      "title": "Generalized entropy calibration for inference with partially observed data: A unified framework",
      "authors": [
        "Mst Moushumi Pervin",
        "Hengfang Wang",
        "Jae Kwang Kim"
      ],
      "abstract": "Missing data is an universal problem in statistics. We develop a unified framework for estimating parameters defined by general estimating equations under a missing-at-random (MAR) mechanism, based on generalized entropy calibration weighting. We construct weights by minimizing a convex entropy subject to (i) balancing constraints on a data-adaptive calibration function, estimated using flexible machine-learning predictors with cross-fitting, and (ii) a debiasing constraint involving the fitted propensity score (PS) model. The resulting estimator is doubly robust, remaining consistent if either the outcome regression (OR) or the PS model is correctly specified, and attains the semiparametric efficiency bound when both models are correctly specified. Our formulation encompasses classical inverse probability weighting (IPW) and augmented IPW (AIPW) as special cases and accommodates a broad class of entropy functions. We illustrate the versatility of the approach in three important settings: semi-supervised learning with unlabeled outcomes, regression analysis with missing covariates, and causal effect estimation in observational studies. Extensive simulation studies and real-data applications demonstrate that the proposed estimators achieve greater efficiency and numerical stability than existing methods. In particular, the proposed estimator outperforms the classical AIPW estimator under the OR model misspecification.",
      "published": "2026-02-22",
      "updated": "2026-02-22",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.19203v1",
      "arxivUrl": "http://arxiv.org/abs/2602.19203v1",
      "doi": null
    },
    {
      "id": "2602.19201",
      "title": "Panel Quantile Regression with Common Shocks",
      "authors": [
        "Harold D. Chiang",
        "Antonio F. Galvao",
        "Chia-Min Wei"
      ],
      "abstract": "This paper develops an asymptotic and inferential theory for fixed-effects panel quantile regression (FEQR) that delivers inference robust to pervasive common shocks. Such shocks induce cross-sectional dependence that is central in many economic and financial panels but largely ignored in existing FEQR theory, which typically assumes cross-sectional independence and requires $T \\gg N$. We show that the standard FEQR estimator remains asymptotically normal under the mild condition $(\\log N)^2/T \\to 0$, thereby accommodating empirically relevant regimes, including those with $T \\ll N$. We further show that common shocks fundamentally alter the asymptotic covariance structure, rendering conventional covariance estimators inconsistent, and we propose a simple covariance estimator that remains consistent both in the presence and absence of common shocks. The proposed procedure therefore provides valid robust inference without requiring prior knowledge of the dependence structure, substantially expanding the applicability of FEQR methods in realistic panel data settings.",
      "published": "2026-02-22",
      "updated": "2026-02-22",
      "categories": [
        "econ.EM",
        "stat.ME"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.19201v1",
      "arxivUrl": "http://arxiv.org/abs/2602.19201v1",
      "doi": null
    },
    {
      "id": "2602.19196",
      "title": "An Interpretable Data-Driven Model of the Flight Dynamics of Hawks",
      "authors": [
        "Lydia France",
        "Karl Lapo",
        "J. Nathan Kutz"
      ],
      "abstract": "Despite significant analysis of bird flight, generative physics models for flight dynamics do not currently exist. Yet the underlying mechanisms responsible for various flight manoeuvres are important for understanding how agile flight can be accomplished. Even in a simple flight, multiple objectives are at play, complicating analysis of the overall flight mechanism. Using the data-driven method of dynamic mode decomposition (DMD) on motion capture recordings of hawks, we show that multiple behavioral states such as flapping, turning, landing, and gliding, can be modeled by simple and interpretable modal structures (i.e. the underlying wing-tail shape) which can be linearly combined to reproduce the experimental flight observations. Moreover, the DMD model can be used to extrapolate naturalistic flapping. Flight is highly individual, with differences in style across the hawks, but we find they share a common set of dynamic modes. The DMD model is a direct fit to data, unlike traditional models constructed from physics principles which can rarely be tested on real data and whose assumptions are typically invalid in real flight. The DMD approach gives a highly accurate reconstruction of the flight dynamics with only three parameters needed to characterize flapping, and a fourth to integrate turning manoeuvres. The DMD analysis further shows that the underlying mechanism of flight, much like simplest walking models, displays a parametric coupling between dominant modes suggesting efficiency for locomotion.",
      "published": "2026-02-22",
      "updated": "2026-02-22",
      "categories": [
        "q-bio.QM",
        "cs.CE",
        "cs.LG",
        "physics.flu-dyn"
      ],
      "primaryCategory": "q-bio.QM",
      "pdfUrl": "https://arxiv.org/pdf/2602.19196v1",
      "arxivUrl": "http://arxiv.org/abs/2602.19196v1",
      "doi": null
    },
    {
      "id": "2602.19154",
      "title": "Demand estimation without outside good shares",
      "authors": [
        "Federico A. Bugni",
        "Joel L. Horowitz",
        "Linqi Zhang"
      ],
      "abstract": "The BLP model is the workhorse framework in empirical IO and enables estimation of demand models for differentiated products using aggregate product shares. In practice, however, the share of the outside good is often unobserved. This paper studies identification and inference in the BLP model when the share of the outside good is unobserved. We show that the model is partially identified, and we derive sharp identified sets for structural parameters and equilibrium objects. We also develop inference procedures based on moment inequalities that deliver valid confidence sets for these structural parameters and equilibrium objects.",
      "published": "2026-02-22",
      "updated": "2026-02-22",
      "categories": [
        "econ.EM"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.19154v1",
      "arxivUrl": "http://arxiv.org/abs/2602.19154v1",
      "doi": null
    },
    {
      "id": "2602.19129",
      "title": "Estimation and Statistical Inference for Generalized Multilayer Latent Space Model",
      "authors": [
        "Zhaozhe Liu",
        "Gongjun Xu",
        "Haoran Zhang"
      ],
      "abstract": "Multilayer networks have become increasingly ubiquitous across diverse scientific fields, ranging from social sciences and biology to economics and international relations. Despite their broad applications, the inferential theory for multilayer networks remains underdeveloped. In this paper, we propose a flexible latent space model for multilayer directed networks with various edge types, where each node is assigned with two latent positions capturing sending and receiving behaviors, and each layer has a connection matrix governing the layer-specific structure. Through nonlinear link functions, the proposed model represents the structure of a multilayer network as a tensor, which admits a Tucker low-rank decomposition. This formulation poses significant challenges on the estimation and statistical inference for the latent positions and connection matrices, where existing techniques are inapplicable. To tackle this issue, a novel unfolding and fusion method is developed to facilitate estimation. We establish both consistency and asymptotic normality for the estimated latent positions and connection matrices, which paves the way for statistical inference tasks in multilayer network applications, such as constructing confidence regions for the latent positions and testing whether two network layers share the same structure. We validate the proposed method through extensive simulation studies and demonstrate its practical utility on real-world data.",
      "published": "2026-02-22",
      "updated": "2026-02-22",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.19129v1",
      "arxivUrl": "http://arxiv.org/abs/2602.19129v1",
      "doi": null
    },
    {
      "id": "2602.19012",
      "title": "Adaptive Weighting for Time-to-Event Continual Reassessment Method: Improving Safety in Phase I Dose-Finding Through Data-Driven Delay Distribution Estimation",
      "authors": [
        "Robert Amevor",
        "Emmanuel Kubuafor",
        "Dennis Baidoo"
      ],
      "abstract": "Background: Phase I dose-finding trials increasingly encounter delayed-onset toxicities, especially with immunotherapies and targeted agents. The time-to-event continual reassessment method (TITE-CRM) handles incomplete follow-up using fixed linear weights, but this ad hoc approach doesn't reflect actual delay patterns and may expose patients to excessive risk during dose escalation. Methods: We replace TITE-CRM's fixed weights with adaptive weights, posterior predictive probabilities derived from the evolving toxicity delay distribution. Under a Weibull timing model, we get closed-form weight updates through maximum likelihood estimation, making real-time implementation straightforward. We tested our method (AW-TITE) against TITE-CRM and standard designs (3+3, mTPI, BOIN) across three dose-toxicity scenarios through simulation (N = 30 patients, 2,000 replications). We also examined robustness across varying accrual rates, sample sizes, shape parameters, observation windows, and priors. Results: Our AW-TITE reduced patient overdosing by 40.6% compared to TITE-CRM (mean fraction above MTD: 0.202 vs 0.340; 95% CI: -0.210 to -0.067, p < 0.001) while maintaining comparable MTD selection accuracy (mean difference: +0.023, p = 0.21). Against algorithm-based methods, AW-TITE achieved higher MTD identification: +32.6% vs mTPI, +19.8% vs 3+3, and +5.6% vs BOIN. Performance remained robust across all sensitivity analyses. Conclusions: Adaptive weighting offers a practical way to improve Phase I trial safety while preserving MTD selection accuracy. The method requires minimal computation and is ready for real-time use.",
      "published": "2026-02-22",
      "updated": "2026-02-22",
      "categories": [
        "stat.ME",
        "stat.AP"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.19012v1",
      "arxivUrl": "http://arxiv.org/abs/2602.19012v1",
      "doi": null
    },
    {
      "id": "2602.18988",
      "title": "Latent Moment Models for Recurrent Binary Outcomes: A Bayesian and Quasi-Distributional Approach",
      "authors": [
        "Niloofar Ramezani",
        "Lori P. Selby",
        "Pascal Nitiema",
        "Jeffrey R. Wilson"
      ],
      "abstract": "Recurrent binary outcomes within individuals, such as hospital readmissions, often reflect latent risk processes that evolve over time. Conventional methods like generalized linear mixed models and generalized estimating equations estimate average risk but fail to capture temporal changes in variability, asymmetry, and tail behavior. We introduce two statistical frameworks that model each binary event as the outcome of a thresholded value drawn from a time-varying latent distribution defined by its location, scale, skewness, and kurtosis. Rather than treating these four quantities as nonparametric moment estimators, we model them as interpretable latent moments within a flexible latent distributional family. The first, BLaS-Recurrent, is a Bayesian model using the sinh-arcsinh distribution (a parametric family that provides explicit control over asymmetry and tail weight) to estimate latent moment trajectories; the second, QuaD-Recurrent, is a quasi-distributional approach that maps simulated moment vectors to event probabilities using a flexible nonparametric surface. Both models support time-dependent covariates, serial correlation, and multiple membership structures. Simulation studies show improved calibration, interpretability, and robustness over standard models. Applied to ICU readmission data from the MIMIC-IV database, both approaches uncover clinically meaningful patterns in latent risk, such as right-skewed escalation and widening dispersion, that are missed by traditional methods. These models provide interpretable, distribution-sensitive tools for longitudinal binary outcomes in healthcare while explicitly acknowledging that latent \"moments\" summarize but do not uniquely determine the underlying distribution.",
      "published": "2026-02-22",
      "updated": "2026-02-22",
      "categories": [
        "stat.ME",
        "stat.AP"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.18988v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18988v1",
      "doi": null
    },
    {
      "id": "2602.18958",
      "title": "Optimal and Structure-Adaptive CATE Estimation with Kernel Ridge Regression",
      "authors": [
        "Seok-Jin Kim"
      ],
      "abstract": "We propose an optimal algorithm for estimating conditional average treatment effects (CATEs) when response functions lie in a reproducing kernel Hilbert space (RKHS). We study settings in which the contrast function is structurally simpler than the nuisance functions: (i) it lies in a lower-complexity RKHS with faster eigenvalue decay, (ii) it satisfies a source condition relative to the nuisance kernel, or (iii) it depends on a known low-dimensional covariate representation. We develop a unified two-stage kernel ridge regression (KRR) method that attains minimax rates governed by the complexity of the contrast function rather than the nuisance class, in terms of both sample size and overlap. We also show that a simple model-selection step over candidate contrast spaces and regularization levels yields an oracle inequality, enabling adaptation to unknown CATE regularity.",
      "published": "2026-02-21",
      "updated": "2026-02-21",
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.ML"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.18958v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18958v1",
      "doi": null
    },
    {
      "id": "2602.18915",
      "title": "AAVGen: Precision Engineering of Adeno-associated Viral Capsids for Renal Selective Targeting",
      "authors": [
        "Mohammadreza Ghaffarzadeh-Esfahani",
        "Yousof Gheisari"
      ],
      "abstract": "Adeno-associated viruses (AAVs) are promising vectors for gene therapy, but their native serotypes face limitations in tissue tropism, immune evasion, and production efficiency. Engineering capsids to overcome these hurdles is challenging due to the vast sequence space and the difficulty of simultaneously optimizing multiple functional properties. The complexity also adds when it comes to the kidney, which presents unique anatomical barriers and cellular targets that require precise and efficient vector engineering. Here, we present AAVGen, a generative artificial intelligence framework for de novo design of AAV capsids with enhanced multi-trait profiles. AAVGen integrates a protein language model (PLM) with supervised fine-tuning (SFT) and a reinforcement learning technique termed Group Sequence Policy Optimization (GSPO). The model is guided by a composite reward signal derived from three ESM-2-based regression predictors, each trained to predict a key property: production fitness, kidney tropism, and thermostability. Our results demonstrate that AAVGen produces a diverse library of novel VP1 protein sequences. In silico validations revealed that the majority of the generated variants have superior performance across all three employed indices, indicating successful multi-objective optimization. Furthermore, structural analysis via AlphaFold3 confirms that the generated sequences preserve the canonical capsid folding despite sequence diversification. AAVGen establishes a foundation for data-driven viral vector engineering, accelerating the development of next-generation AAV vectors with tailored functional characteristics.",
      "published": "2026-02-21",
      "updated": "2026-02-21",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primaryCategory": "q-bio.QM",
      "pdfUrl": "https://arxiv.org/pdf/2602.18915v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18915v1",
      "doi": null
    },
    {
      "id": "2602.18865",
      "title": "Expected Shortfall Regression via Optimization",
      "authors": [
        "Yuanzhi Li",
        "Shushu Zhang",
        "Xuming He"
      ],
      "abstract": "To provide a comprehensive summary of the tail distribution, the expected shortfall is defined as the average over the tail above (or below) a certain quantile of the distribution. The expected shortfall regression captures the heterogeneous covariate-response relationship and describes the covariate effects on the tail of the response distribution. Based on a critical observation that the superquantile regression from the operations research literature does not coincide with the expected shortfall regression, we propose and validate a novel optimization-based approach for the linear expected shortfall regression, without additional assumptions on the conditional quantile models. While the proposed loss function is implicitly defined, we provide a prototype implementation of the proposed approach with some initial expected shortfall estimators based on binning techniques. With practically feasible initial estimators, we establish the consistency and the asymptotic normality of the proposed estimator. The proposed approach achieves heterogeneity-adaptive weights and therefore often offers efficiency gain over existing linear expected shortfall regression approaches in the literature, as demonstrated through simulation studies.",
      "published": "2026-02-21",
      "updated": "2026-02-21",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.18865v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18865v1",
      "doi": null
    },
    {
      "id": "2602.18808",
      "title": "Orthogonal polynomials on path-space",
      "authors": [
        "Ilya Chevyrev",
        "Emilio Ferrucci",
        "Darrick Lee",
        "Terry Lyons",
        "Harald Oberhauser",
        "Nikolas Tapia"
      ],
      "abstract": "We consider the orthogonalisation of the signature of a stochastic process as the analogue of orthogonal polynomials on path-space. Under an infinite radius of convergence assumption, we prove density of linear functions on the signature in $L^p$ functions on grouplike elements, making it possible to represent a square-integrable function on (rough) paths as an $L^2$-convergent series. By viewing the shuffle algebra as commutative polynomials on the free Lie algebra, we revisit much of the theory of classical orthogonal polynomials in several variables, such as the recurrence relation and Favard's theorem. Finally, we restrict our attention to the case of Brownian motion with and without drift, and prove that dimension-independent orthogonal signature exists with drift but not without. We end with numerical examples of how orthogonal signature polynomials of Brownian motion can be applied for the approximation of functions on paths sampled from the Wiener measure.",
      "published": "2026-02-21",
      "updated": "2026-02-21",
      "categories": [
        "math.PR",
        "stat.ME"
      ],
      "primaryCategory": "math.PR",
      "pdfUrl": "https://arxiv.org/pdf/2602.18808v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18808v1",
      "doi": null
    },
    {
      "id": "2602.18727",
      "title": "Statistical methods for reference-free single-molecule localisation microscopy",
      "authors": [
        "Jack Peyton",
        "Benjamin Davis",
        "Emily Gribbin",
        "Daniel Rolfe",
        "Hannah Mitchell"
      ],
      "abstract": "MINFLUX (Minimal Photon Flux) is a single-molecule imaging technique capable of resolving fluorophores at a precision of <5 nm. Interpretation of the point patterns generated by this technique presents challenges due to variable emitter density, incomplete bio-labelling of target molecules and their detection, error prone measurement processes, and the presence of spurious (non-structure associated) fluorescent detections. Together, these challenges ensure structural inferences from single-molecule imaging datasets are non-trivial in the absence of strong a priori information, for all but the smallest of point patterns. In addition, current methods often require subjective parameter tuning and presuppose known structural templates, limiting reference-free discovery. We present a statistically grounded, end-to-end analysis framework. Focusing on MINFLUX derived datasets and leveraging Bayesian and spatial statistical methods, a pipeline is presented that demonstrates 1) uncertainty aware clustering of measurements into emitter groups that performs better than current gold standards, 2) rapid identification of molecular structure supergroups, and 3) reconstruction of repeating structures within the dataset without substantial prior knowledge. This pipeline is demonstrated using simulated and real MINFLUX datasets, where emitter clustering and centre detection maintain high performance (emitter subset assignment accuracy > 0.75) across all conditions evaluated, while structural inference achieves reliable discrimination (F1 approx. 0.9) at high labelling efficiency. Template-free reconstruction of Nup96 and DNA-Origami 3x3 grids are achieved.",
      "published": "2026-02-21",
      "updated": "2026-02-21",
      "categories": [
        "stat.AP",
        "q-bio.QM"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.18727v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18727v1",
      "doi": null
    },
    {
      "id": "2602.18677",
      "title": "Bayesian calendar-time survival analysis with epidemic curve priors and variant-specific infection hazards",
      "authors": [
        "Angela M Dahl",
        "Elizabeth R Brown"
      ],
      "abstract": "In this paper, we develop a Bayesian calendar-time survival model motivated by infectious disease prevention studies occurring during an epidemic, when the risk of infection can change rapidly as the epidemic curve shifts. For studies in which a biomarker is the predictor of interest, we include the option to estimate a threshold of protection for the biomarker. If the intervention is hypothesized to have different associations with several circulating viral variants, or if the infectiousness of the dominant variant(s) changes over the course of the study, we treat infection from different variants as competing risks. We also introduce a novel method for incorporating existing epidemic curve estimates into an informative prior for the baseline hazard function, enabling estimation of the intervention's association with infection risk during periods of calendar time with minimal follow-up in one or more comparator groups. We demonstrate the strengths of this method via simulations, and we apply it to data from an observational COVID-19 vaccine study.",
      "published": "2026-02-21",
      "updated": "2026-02-21",
      "categories": [
        "stat.ME",
        "stat.AP"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.18677v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18677v1",
      "doi": null
    },
    {
      "id": "2602.18660",
      "title": "Better Assumptions, Stronger Conclusions: The Case for Ordinal Regression in HCI",
      "authors": [
        "Brandon Victor Syiem",
        "Eduardo Velloso"
      ],
      "abstract": "Despite the widespread use of ordinal measures in HCI, such as Likert-items, there is little consensus among HCI researchers on the statistical methods used for analysing such data. Both parametric and non-parametric methods have been extensively used within the discipline, with limited reflection on their assumptions and appropriateness for such analyses. In this paper, we examine recent HCI works that report statistical analyses of ordinal measures. We highlight prevalent methods used, discuss their limitations and spotlight key assumptions and oversights that diminish the insights drawn from these methods. Finally, we champion and detail the use of cumulative link (mixed) models (CLM/CLMM) for analysing ordinal data. Further, we provide practical worked examples of applying CLM/CLMMs using R to published open-sourced datasets. This work contributes towards a better understanding of the statistical methods used to analyse ordinal data in HCI and helps to consolidate practices for future work.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.ME",
        "cs.HC"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.18660v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18660v1",
      "doi": "10.1145/3772318.3790821"
    },
    {
      "id": "2602.18656",
      "title": "Minimally Discrete and Minimally Randomized p-Values",
      "authors": [
        "Joshua Habiger",
        "Pratyaydipta Rudra"
      ],
      "abstract": "In meta analysis, multiple hypothesis testing and many other methods, p-values are utilized as inputs and assumed to be uniformly distributed over the unit interval under the null hypotheses. If data used to generate p-values have discrete distributions then either natural, mid- or randomized p-values are typically utilized. Natural and mid-p-values can allow for valid, albeit conservative, downstream methods since under the null hypothesis they are dominated by uniform distributions in the stochastic and convex order, respectively. Randomized p-values need not lead to conservative procedures since they permit a uniform distributions under the null hypotheses through the generation of independent auxiliary variates. However, the auxiliary variates necessarily add variation to procedures. This manuscript introduces and studies ``minimally discrete'' (MD) natural p-values, MD mid-p-values and ``minimally randomized'' (MR) p-values. It is shown that MD p-values dominate their non-MD counterparts in the stochastic and convex order, and hence lead to less conservative, yet still valid, downstream methods. Likewise, MR p-values dominate their non-MR counterparts in that they are still uniformly distributed under the null hypotheses, but the added variation attributable to the independently generated auxiliary variate is smaller. It is anticipated that results here will facilitate the construction of new meta-analysis and multiple testing methods via more efficient p-value construction, and facilitate theoretical study of existing and new methods by establishing gold standards for addressing the unavoidable detrimental ``discreteness effect''.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.18656v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18656v1",
      "doi": null
    },
    {
      "id": "2602.18651",
      "title": "Hybrid combinations of parametric and empirical likelihoods",
      "authors": [
        "Nils Lid Hjort",
        "Ian W. McKeague",
        "Ingrid Van Keilegom"
      ],
      "abstract": "This paper develops a hybrid likelihood (HL) method based on a compromise between parametric and nonparametric likelihoods. Consider the setting of a parametric model for the distribution of an observation $Y$ with parameter $θ$. Suppose there is also an estimating function $m(\\cdot,μ)$ identifying another parameter $μ$ via $E\\,m(Y,μ)=0$, at the outset defined independently of the parametric model. To borrow strength from the parametric model while obtaining a degree of robustness from the empirical likelihood method, we formulate inference about $θ$ in terms of the hybrid likelihood function $H_n(θ)=L_n(θ)^{1-a}R_n(μ(θ))^a$. Here $a\\in[0,1)$ represents the extent of the compromise, $L_n$ is the ordinary parametric likelihood for $θ$, $R_n$ is the empirical likelihood function, and $μ$ is considered through the lens of the parametric model. We establish asymptotic normality of the corresponding HL estimator and a version of the Wilks theorem. We also examine extensions of these results under misspecification of the parametric model, and propose methods for selecting the balance parameter $a$.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.18651v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18651v1",
      "doi": null
    },
    {
      "id": "2602.18643",
      "title": "Project Hermes: A Model-Agnostic Validation Layer for Wearable Health Prediction Systems",
      "authors": [
        "Richik Chakraborty"
      ],
      "abstract": "The deployment of wearable-based health prediction systems has accelerated rapidly, yet these systems face a fundamental challenge: they generate alerts under substantial uncertainty without principled mechanisms for user-specific validation. While large language models (LLMs) have been increasingly applied to healthcare tasks, existing work focuses predominantly on diagnosis generation and risk prediction rather than post-prediction validation of detected signals. We introduce Project Hermes, a model-agnostic validation layer that treats signal confirmation as a sequential decision problem. Hermes operates downstream of arbitrary upstream predictors, using LLM-generated contextual queries to elicit targeted user feedback and performing Bayesian confidence updates to distinguish true positives from false alarms. In a 60-day longitudinal case study of migraine prediction, Hermes achieved a 34% reduction in false positive rate (from 61.7% to 12.5%) while maintaining 89% sensitivity, with mean lead time of 4.2 hours before symptom onset. Critically, Hermes does not perform diagnosis or make novel predictions; it validates whether signals detected by upstream models are clinically meaningful for specific individuals at specific times. This work establishes validation as a first-class computational problem distinct from prediction, with implications for trustworthy deployment of consumer health AI systems.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "q-bio.QM"
      ],
      "primaryCategory": "q-bio.QM",
      "pdfUrl": "https://arxiv.org/pdf/2602.18643v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18643v1",
      "doi": null
    },
    {
      "id": "2602.18636",
      "title": "Statistical Imaginaries, State Legitimacy: Grappling with the Arrangements Underpinning Quantification in the U.S. Census",
      "authors": [
        "Jayshree Sarathy",
        "danah boyd"
      ],
      "abstract": "Over the last century, the adoption of novel scientific methods for conducting the U.S. census has been met with wide-ranging receptions. Some methods were quietly embraced, while others sparked decades-long controversies. What accounts for these differences? We argue that controversies emerge from $\\textit{arrangements of statistical imaginaries}$, putting into tension divergent visions of the census. To analyze these dynamics, we compare reactions to two methods designed to improve data accuracy (imputation and adjustment) and two methods designed to protect confidentiality (swapping and differential privacy), offering insight into how each method reconfigures stakeholder orientations and rhetorical claims. These cases allow us to reflect on how technocratic efforts to improve accuracy and confidentiality can strengthen -- or erode -- trust in data. Our analysis shows how the credibility of the Census Bureau and its data stem not just from empirical evaluations of quantification, but also from how statistical imaginaries are contested and stabilized.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "cs.CY",
        "stat.AP"
      ],
      "primaryCategory": "cs.CY",
      "pdfUrl": "https://arxiv.org/pdf/2602.18636v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18636v1",
      "doi": "10.1177/08969205241270898"
    },
    {
      "id": "2602.18592",
      "title": "A Roof Over Risk: A House Price-at-Risk Framework for Hungary",
      "authors": [
        "Tibor Szendrei",
        "Nikolett Vágó",
        "Katalin Varga"
      ],
      "abstract": "This paper develops a House Price-at-Risk framework to examine how housing subsidies, credit conditions, and supply factors influence the distribution of house price growth in Hungary. Using quantile regression with adaptive LASSO variable selection, we identify variables driving downside versus upside risks across multiple horizons. Financial stress dominates the lower tail at short horizons, while unemployment and affordability constraints become the primary drivers of downside risk at longer horizons. Housing subsidies exhibit pro-cyclical characteristics, concentrating significant positive effects on the upper quantiles while leaving the lower tail largely unaffected. Supply-side variables display horizon-dependent sign reversals, with construction permits exerting upward pressure on prices in the short run but moderating them as supply materialises. Uncertainty decomposition reveals persistent left-tail dominance across all horizons. These findings suggest that macroprudential frameworks should account for the distributional effects of housing subsidies, particularly their pro-cyclical influence on house price growth.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "econ.EM"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.18592v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18592v1",
      "doi": null
    },
    {
      "id": "2602.18577",
      "title": "balnet: Pathwise Estimation of Covariate Balancing Propensity Scores",
      "authors": [
        "Erik Sverdrup",
        "Trevor Hastie"
      ],
      "abstract": "We present balnet, an R package for scalable pathwise estimation of covariate balancing propensity scores via logistic covariate balancing loss functions. Regularization paths are computed with Yang and Hastie (2024)'s generic elastic net solver, supporting convex losses with non-smooth penalties, as well as group penalties and feature-specific penalty factors. For lasso penalization, balnet computes a regularized balance path from the largest observed covariate imbalance to a user-specified fraction of this maximum. We illustrate the method with an application to spatial pixel-level balancing for constructing synthetic control weights for the average treatment effect on the treated, using satellite data on wildfires.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.ME",
        "stat.CO"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.18577v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18577v1",
      "doi": null
    },
    {
      "id": "2602.18573",
      "title": "Multiclass Calibration Assessment and Recalibration of Probability Predictions via the Linear Log Odds Calibration Function",
      "authors": [
        "Amy Vennos",
        "Xin Xing",
        "Christopher T. Franck"
      ],
      "abstract": "Machine-generated probability predictions are essential in modern classification tasks such as image classification. A model is well calibrated when its predicted probabilities correspond to observed event frequencies. Despite the need for multicategory recalibration methods, existing methods are limited to (i) comparing calibration between two or more models rather than directly assessing the calibration of a single model, (ii) requiring under-the-hood model access, e.g., accessing logit-scale predictions within the layers of a neural network, and (iii) providing output which is difficult for human analysts to understand. To overcome (i)-(iii), we propose Multicategory Linear Log Odds (MCLLO) recalibration, which (i) includes a likelihood ratio hypothesis test to assess calibration, (ii) does not require under-the-hood access to models and is thus applicable on a wide range of classification problems, and (iii) can be easily interpreted. We demonstrate the effectiveness of the MCLLO method through simulations and three real-world case studies involving image classification via convolutional neural network, obesity analysis via random forest, and ecology via regression modeling. We compare MCLLO to four comparator recalibration techniques utilizing both our hypothesis test and the existing calibration metric Expected Calibration Error to show that our method works well alone and in concert with other methods.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2602.18573v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18573v1",
      "doi": null
    },
    {
      "id": "2602.18570",
      "title": "Spatiotemporal double machine learning to estimate the impact of Cambodian land concessions on deforestation",
      "authors": [
        "Anika Arifin",
        "Duncan DeProfio",
        "Layla Lammers",
        "Benjamin Shapiro",
        "Brian J Reich",
        "Henry Uddyback",
        "Joshua M Gray"
      ],
      "abstract": "Environmental policy evaluation frequently requires thoughtful consideration of space and time in causal inference. We use novel statistical methods to analyze the causal effect of land concessions on deforestation rates in Cambodia. Standard approaches, such as difference-in-differences regression, effectively address spatiotemporally-correlated treatments under some conditions, but they are limited in their ability to account for unobserved confounders affecting both treatment and outcome. Double Spatial Regression (DSR) is an approach that uses double machine learning to address these scenarios. DSR resolves the confounding variables for both treatment and outcome, comparing the residuals to estimate treatment effectiveness. We improve upon DSR by considering time in our analysis of policy interventions with spatial effects. We conduct a large-scale simulation study using Bayesian Additive Regression Trees (BART) with spatial embeddings and find that, under certain conditions, our DSR model outperforms standard approaches for addressing unobserved spatial confounding. We then apply our method to evaluate the policy impacts of land concessions on deforestation in Cambodia.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.18570v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18570v1",
      "doi": null
    },
    {
      "id": "2602.18396",
      "title": "PRISM-FCP: Byzantine-Resilient Federated Conformal Prediction via Partial Sharing",
      "authors": [
        "Ehsan Lari",
        "Reza Arablouei",
        "Stefan Werner"
      ],
      "abstract": "We propose PRISM-FCP (Partial shaRing and robust calIbration with Statistical Margins for Federated Conformal Prediction), a Byzantine-resilient federated conformal prediction framework that utilizes partial model sharing to improve robustness against Byzantine attacks during both model training and conformal calibration. Existing approaches address adversarial behavior only in the calibration stage, leaving the learned model susceptible to poisoned updates. In contrast, PRISM-FCP mitigates attacks end-to-end. During training, clients partially share updates by transmitting only $M$ of $D$ parameters per round. This attenuates the expected energy of an adversary's perturbation in the aggregated update by a factor of $M/D$, yielding lower mean-square error (MSE) and tighter prediction intervals. During calibration, clients convert nonconformity scores into characterization vectors, compute distance-based maliciousness scores, and downweight or filter suspected Byzantine contributions before estimating the conformal quantile. Extensive experiments on both synthetic data and the UCI Superconductivity dataset demonstrate that PRISM-FCP maintains nominal coverage guarantees under Byzantine attacks while avoiding the interval inflation observed in standard FCP with reduced communication, providing a robust and communication-efficient approach to federated uncertainty quantification.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "cs.LG",
        "eess.SP",
        "math.PR",
        "stat.AP",
        "stat.ML"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2602.18396v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18396v1",
      "doi": null
    },
    {
      "id": "2602.18383",
      "title": "Design-based inference for generalized causal effects in randomized experiments",
      "authors": [
        "Xinyuan Chen",
        "Fan Li"
      ],
      "abstract": "Generalized causal effect estimands, including the Mann-Whitney parameter and causal net benefit, provide flexible summaries of treatment effects in randomized experiments with non-Gaussian or multivariate outcomes. We develop a unified design-based inference framework for regression adjustment and variance estimation of a broad class of generalized causal effect estimands defined through pairwise contrast functions. Leveraging the theory of U-statistics and finite-population asymptotics, we establish the consistency and asymptotic normality of regression estimators constructed from individual pairs and per-unit pair averages, even when the working models are misspecified. Consequently, these estimators are model-assisted rather than model-based. In contrast to classical average treatment effect estimands, we show that for nonlinear contrast functions, covariate adjustment preserves consistency but does not admit a universal efficiency guarantee. For inference, we demonstrate that standard heteroskedasticity-robust and cluster-robust variance estimators are generally inconsistent in this setting. As a remedy, we prove that a complete two-way cluster-robust variance estimator, which fully accounts for pairwise dependence and reverse comparisons, is consistent.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.18383v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18383v1",
      "doi": null
    },
    {
      "id": "2602.18369",
      "title": "Hidden multistate models to study multimorbidity trajectories",
      "authors": [
        "Valentina Manzoni",
        "Francesca Ieva",
        "Amaia Calderón-Larrañaga",
        "Davide Liborio Vetrano",
        "Caterina Gregorio"
      ],
      "abstract": "Multimorbidity in older adults is common, heterogeneous, and highly dynamic, and it is strongly associated with disability and increased healthcare utilization. However, existing approaches to studying multimorbidity trajectories are largely descriptive or rely on discrete-time models, which struggle to handle irregular observation intervals and right-censoring. We developed a continuous-time hidden multistate modeling framework to capture transitions among latent multimorbidity patterns while accounting for interval censoring and misclassification. A simulation study compared alternative model specifications under varying sample sizes and follow-up schemes, and the best-performing specification was applied to longitudinal data from the Swedish National study on Aging and Care-Kungsholmen (SNAC-K), including 2,716 multimorbid participants followed for up to 18 years. Simulation results showed that hidden multistate models substantially reduced bias in transition hazard estimates compared to non-hidden models, with fully time-inhomogeneous models outperforming piecewise approximations. Application to SNAC-K confirmed the feasibility and practical utility of this framework, enabling identification of risk factors for accelerated progression toward complex multimorbidity and revealing a gradient of mortality risk across patterns. Continuous-time hidden multistate models provide a robust alternative to traditional approaches, supporting individualized predictions and informing targeted interventions and secondary prevention strategies for multimorbidity in aging populations.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.AP",
        "stat.ME"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.18369v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18369v1",
      "doi": null
    },
    {
      "id": "2602.18358",
      "title": "Forecasting the Evolving Composition of Inbound Tourism Demand: A Bayesian Compositional Time Series Approach Using Platform Booking Data",
      "authors": [
        "Harrison Katz"
      ],
      "abstract": "Understanding how the composition of guest origin markets evolves over time is critical for destination marketing organizations, hospitality businesses, and tourism planners. We develop and apply Bayesian Dirichlet autoregressive moving average (BDARMA) models to forecast the compositional dynamics of guest origin market shares using proprietary Airbnb booking data spanning 2017--2024 across four major destination regions. Our analysis reveals substantial pandemic-induced structural breaks in origin composition, with heterogeneous recovery patterns across markets. The BDARMA framework achieves the lowest average forecast error across all destination regions, outperforming standard benchmarks including naïve forecasts, exponential smoothing, and SARIMA on log-ratio transformed data. For EMEA destinations, BDARMA achieves 23% lower forecast error than naive methods, with statistically significant improvements. By modeling compositions directly on the simplex with a Dirichlet likelihood and incorporating seasonal variation in both mean and precision parameters, our approach produces coherent forecasts that respect the unit-sum constraint while capturing complex temporal dependencies. The methodology provides destination stakeholders with probabilistic forecasts of source market shares, enabling more informed strategic planning for marketing resource allocation, infrastructure investment, and crisis response.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.AP",
        "q-fin.ST"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.18358v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18358v1",
      "doi": null
    },
    {
      "id": "2602.18271",
      "title": "Two-Stage Multiple Test Procedures Controlling False Discovery Rate with auxiliary variable and their Application to Set4Delta Mutant Data",
      "authors": [
        "Seohwa Hwang",
        "Mark Louie Ramos",
        "DoHwan Park",
        "Junyong Park",
        "Johan Lim",
        "Erin Green"
      ],
      "abstract": "In this paper, we present novel methodologies that incorporate auxiliary variables for multiple hypotheses testing related to the main point of interest while effectively controlling the false discovery rate. When dealing with multiple tests concerning the primary variable of interest, researchers can use auxiliary variables to set preconditions for the significance of primary variables, thereby enhancing test efficacy. Depending on the auxiliary variable's role, we propose two approaches: one terminates testing of the primary variable if it does not meet predefined conditions, and the other adjusts the evaluation criteria based on the auxiliary variable. Employing the copula method, we elucidate the dependence between the auxiliary and primary variables by deriving their joint distribution from individual marginal distributions.Our numerical studies, compared with existing methods, demonstrate that the proposed methodologies effectively control the FDR and yield greater statistical power than previous approaches solely based on the primary variable. As an illustrative example, we apply our methods to the Set4$Δ$ mutant dataset. Our findings highlight the distinctions between our methodologies and traditional approaches, emphasising the potential advantages of our methods in introducing the auxiliary variable for selecting more genes.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.18271v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18271v1",
      "doi": null
    },
    {
      "id": "2602.18241",
      "title": "Online FDR Controlling procedures for statistical SIS Model and its application to COVID19 data",
      "authors": [
        "Seohwa Hwang",
        "Junyong Park"
      ],
      "abstract": "We propose an online false discovery rate (FDR) controlling method based on conditional local FDR (LIS), designed for infectious disease datasets that are discrete and exhibit complex dependencies. Unlike existing online FDR methods, which often assume independence or suffer from low statistical power in dependent settings, our approach effectively controls FDR while maintaining high detection power in realistic epidemic scenarios. For disease modeling, we establish a Dynamic Bayesian Network (DBN) structure within the Susceptible-Infected-Susceptible (SIS) model, a widely used epidemiological framework for infectious diseases. Our method requires no additional tuning parameters apart from the width of the sliding window, making it practical for real-time disease monitoring. From a statistical perspective, we prove that our method ensures valid FDR control under stationary and ergodic dependencies, extending online hypothesis testing to a broader range of dependent and discrete datasets. Additionally, our method achieves higher statistical power than existing approaches by leveraging LIS, which has been shown to be more powerful than traditional $p$-value-based methods. We validate our method through extensive simulations and real-world applications, including the analysis of infectious disease incidence data. Our results demonstrate that the proposed approach outperforms existing methods by achieving higher detection power while maintaining rigorous FDR control.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.18241v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18241v1",
      "doi": null
    },
    {
      "id": "2602.18210",
      "title": "Semiparametric Uncertainty Quantification via Isotonized Posterior for Deconvolutions",
      "authors": [
        "Francesco Gili",
        "Geurt Jongbloed"
      ],
      "abstract": "We address the problem of uncertainty quantification for the deconvolution model \\(Z = X + Y\\), where \\(X\\) and \\(Y\\) are nonnegative random variables and the goal is to estimate the signal's distribution of \\(X \\sim F_0\\) supported on~\\([0,\\infty)\\), from observations where the noise distribution is known. Existing frequentist methods often produce confidence intervals for $F_0(x)$ that depend on unknown nuisance parameters, such as the density of \\(X\\) and its derivative, which are difficult to estimate in practice. This paper introduces a novel and computationally efficient nonparametric Bayesian approach, based on projecting the posterior, to overcome this limitation. Our method leverages the solution \\(p\\) to a specific Volterra integral equation as in \\cite{74}, which relates the cumulative distribution function (CDF) of the signal, \\(F_0\\), to the distribution of the observables. We place a Dirichlet Process prior directly on the distribution of the observed data $Z$, yielding a simple, conjugate posterior. To ensure the resulting estimates for \\(F_0\\) are valid CDFs, we isotonize posterior draws taking the Greatest Convex Majorant of the primitive of the posterior draws and defining what we term the Isotonic Inverse Posterior. We show that this framework yields posterior credible sets for \\(F_0\\) that are not only computationally fast to generate but also possess asymptotically correct frequentist coverage after a straightforward recalibration technique for the so-called Bayes Chernoff distribution introduced in \\cite{54}. Our approach thus does not require the estimation of nuisance parameters to deliver uncertainty quantification for the parameter of interest $F_0(x)$. The practical effectiveness and robustness of the method are demonstrated through a simulation study with various noise distributions for $Y$.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.18210v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18210v1",
      "doi": null
    },
    {
      "id": "2602.18184",
      "title": "Kolmogorov-Type Maximal Inequalities for Independent and Dependent Negative Binomial Random Variables: Sharp Bounds, Sub-Exponential Refinements, and Applications to Overdispersed Count Data",
      "authors": [
        "Aristides V. Doumas",
        "S. Spektor"
      ],
      "abstract": "This paper develops Kolmogorov-type maximal inequalities for sums of Negative Binomial random variables under both independence and dependence structures. For independent heterogeneous Negative Binomial variables we derive sharp Markov-type deviation inequalities and Kolmogorov-type bounds expressed in terms of Tweedie dispersion parameters, providing explicit control limits for NB2 generalized linear model monitoring. For dependent count data arising through a shared Gamma mixing variable, we establish a \\emph{sub-exponential Bernstein-type refinement} that exploits the Poisson-Gamma hierarchical structure to yield exponentially decaying tail probabilities -- this refinement is new in the literature. Through moment-matched Monte Carlo experiments ($n=20$, 2{,}000 replications), we document a 55\\% reduction in mean maximum deviation under appropriate dependence structures, a stabilization effect we explain analytically. A concrete epidemiological application with NB2 parameters calibrated from COVID-19 surveillance data demonstrates practical utility. These results materially advance the applicability of classical maximal inequalities to overdispersed and dependent count data prevalent in public health, insurance, and ecological modeling.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "math.ST",
        "math.PR",
        "stat.ME"
      ],
      "primaryCategory": "math.ST",
      "pdfUrl": "https://arxiv.org/pdf/2602.18184v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18184v1",
      "doi": null
    },
    {
      "id": "2602.18170",
      "title": "Minimum L2 and robust Kullback-Leibler estimation",
      "authors": [
        "Nils Lid Hjort"
      ],
      "abstract": "This paper introduces two new robust methods for estimation of parameters in a given parametric family. The first method is that of `minimum weighted L2', effectively minimising an estimate of the integrated (and possibly weighted) squared error. The second is `robust Kullback-Leibler', consisting of minimising a robust version of the empirical Kullback-Leibler distance, and can be viewed as a general robust modification of the maximum likelihood procedure. This second method is also related to recent local likelihood ideas for semiparametric density estimation. The methods are described, influence functions are found, as are formulae for asymptotic variances. In particular large-sample efficiencies are computed under the home turf conditions of the underlying parametric model. The methods and formulae are illustrated for the normal model.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.18170v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18170v1",
      "doi": null
    },
    {
      "id": "2602.18161",
      "title": "Equal Marginal Power for Co-Primary Endpoints",
      "authors": [
        "Simon Bond"
      ],
      "abstract": "The choice of sample size in the context of co-primary endpoints for a randomised trial is discussed. Current guidance can leave endpoints with unequal marginal power. A method is provided to achieve equal marginal power by using the flexibility provided in multiple testing procedures. A comparison is made to several choices of rule to determine the sample size, in terms of the study design and its operating characteristics.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.18161v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18161v1",
      "doi": null
    },
    {
      "id": "2602.18150",
      "title": "Inclusive Ranking of Indian States via Bayesian Bradley-Terry Model",
      "authors": [
        "Arshi Rizvi",
        "Rahul Singh"
      ],
      "abstract": "Evaluating the performance of different administrative regions within a country is crucial for its development and policy formulation. The performance evaluators are mostly based on health, education, per capita income, awareness, family planning and so on. Not only evaluating regions, but also ranking them is a crucial step, and various methods have been proposed to date. We aim to provide a ranking system for Indian states that uses a Bayesian approach via the famous Bradley-Terry model for paired comparisons. The ranking method uses indicators from the NFHS-5 dataset with the prior information of per-capita incomes of the states/UTs, thus leading to a holistic ranking, which not only includes human development factors but also take account the economic background of the states. We also carried out various Markov chain Monte Carlo diagnostics required for the reliability of the estimates of merits for these states. These merits thus provide a ranking for the states/UTs and can further be utilised to make informed policy decisions.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.18150v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18150v1",
      "doi": null
    },
    {
      "id": "2602.18087",
      "title": "Optimal inference via confidence distributions for two-by-two tables modelled as Poisson pairs: fixed and random effects",
      "authors": [
        "Céline Cunen",
        "Nils Lid Hjort"
      ],
      "abstract": "This paper presents methods for meta-analysis of $2 \\times 2$ tables, both with and without allowing heterogeneity in the treatment effects. Meta-analysis is common in medical research, but most existing methods are unsuited for $2 \\times 2$ tables with rare events. Usually the tables are modelled as pairs of binomial variables, but we will model them as Poisson pairs. The methods presented here are based on confidence distributions, and offer optimal inference for the treatment effect parameter. We also propose an optimal method for inference on the ratio between treatment effects, and illustrate our methods on a real dataset.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.18087v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18087v1",
      "doi": null
    },
    {
      "id": "2602.18058",
      "title": "Probabilistic Methods for Initial Orbit Determination and Orbit Determination in Cislunar Space",
      "authors": [
        "Ishan Paranjape",
        "Tarun Hejmadi",
        "Suman Chakravorty"
      ],
      "abstract": "In orbital mechanics, Gauss's method for orbit determination (OD) is a popular, minimal assumption solution for obtaining the initial state estimate of a passing resident space object (RSO). Since much of the cislunar domain relies on three-body dynamics, a key assumption of Gauss's method is rendered incompatible, creating a need for a new, minimal assumption method for initial orbit determination (IOD). In this work, we present a framework for short and long term probabilistic target tracking in cislunar space which produces an initial state estimate with as few assumptions as possible. Specifically, we propose an IOD method involving the kinematic fitting of several series of noisy, consecutive ground-based observations. Once a probabilistic initial state estimate in the form of a particle cloud is formed, we apply the powerful Particle Gaussian Mixture (PGM) Filter to reduce the uncertainty of our state estimate over time. This combined IOD/OD framework is demonstrated for several classes of trajectories in cislunar space and compared to better-known filtering frameworks.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "astro-ph.EP",
        "astro-ph.IM",
        "eess.SY",
        "math.OC",
        "physics.data-an",
        "physics.space-ph",
        "stat.AP"
      ],
      "primaryCategory": "astro-ph.EP",
      "pdfUrl": "https://arxiv.org/pdf/2602.18058v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18058v1",
      "doi": null
    },
    {
      "id": "2602.18045",
      "title": "Conformal Tradeoffs: Guarantees Beyond Coverage",
      "authors": [
        "Petrus H. Zwart"
      ],
      "abstract": "Deployed conformal predictors are long-lived decision infrastructure operating over finite operational windows. The real-world question is not only ``Does the true label lie in the prediction set at the target rate?'' (marginal coverage), but ``How often does the system commit versus defer? What error exposure does it induce when it acts? How do these rates trade off?'' Marginal coverage does not determine these deployment-facing quantities: the same calibrated thresholds can yield different operational profiles depending on score geometry. We provide a framework for operational certification and planning beyond coverage with three contributions. (1) Small-Sample Beta Correction (SSBC): we invert the exact finite-sample Beta/rank law for split conformal to map a user request $(α^\\star,δ)$ to a calibrated grid point with PAC-style semantics, yielding explicit finite-window coverage guarantees. (2) Calibrate-and-Audit: since no distribution-free pivot exists for rates beyond coverage, we introduce a two-stage design in which an independent audit set produces a reusable region -- label table and certified finite-window envelopes (Binomial/Beta-Binomial) for operational quantities -- commitment frequency, deferral, decisive error exposure, and commit purity -- via linear projection. (3) Geometric characterization: we describe feasibility constraints, regime boundaries (hedging vs.\\ rejection), and cost-coherence conditions induced by a fixed conformal partition, explaining why operational rates are coupled and how calibration navigates their trade-offs. The output is an auditable operational menu: for a fixed scoring model, we trace attainable operational profiles across calibration settings and attach finite-window uncertainty envelopes. We demonstrate the approach on Tox21 toxicity prediction (12 endpoints) and aqueous solubility screening using AquaSolDB.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.ME",
        "cs.AI"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.18045v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18045v1",
      "doi": null
    },
    {
      "id": "2602.18039",
      "title": "A context-specific causal model for estimating the effect of extended length of overnight stay on traveller's total expenditure",
      "authors": [
        "Lauri Valkonen",
        "Juha Karvanen"
      ],
      "abstract": "Tourism significantly affects the economies of many countries. Understanding the causal relationship between the length of overnight stay and traveller's expenditure is crucial for stakeholders to characterize spending profiles and to design marketing strategies. Causal mechanisms differ between personal and work-related travel because the decision-making processes have different drivers and constraints. We apply context-specific independence relations to model causal mechanisms in contexts specified by trip purpose and identify the causal effect of the length of stay on expenditure. Using the international visitor survey data on foreign travellers to Finland, we fit a hierarchical Bayesian model to estimate the posterior distribution of the counterfactual expenditure due to extending the length of stay by one night. We also perform a Bayesian sensitivity analysis of the estimated causal effect with respect to omitted variable bias.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.AP"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.18039v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18039v1",
      "doi": null
    },
    {
      "id": "2602.18004",
      "title": "Preconditioned Robust Neural Posterior Estimation for Misspecified Simulators",
      "authors": [
        "Ryan P. Kelly",
        "David T. Frazier",
        "David J. Warne",
        "Christopher C. Drovandi"
      ],
      "abstract": "Simulation-based inference (SBI) enables parameter estimation for complex stochastic models with intractable likelihoods when model simulation is feasible. Neural posterior estimation (NPE) is a popular SBI approach that often achieves accurate inference with far fewer simulations than classical approaches. But in practice, neural approaches can be unreliable for two reasons: incompatible data summaries arising from model misspecification yield unreliable posteriors due to extrapolation, and prior-predictive draws can produce extreme summaries that lead to difficulties in obtaining an accurate posterior for the observed data of interest. Existing preconditioning schemes target well-specified settings, and their behaviour under misspecification remains unexplored. We study preconditioning under misspecification and propose preconditioned robust neural posterior estimation, which computes data-dependent weights that focus training near the observed summaries and fits a robust neural posterior approximation. We also introduce a forest-proximity preconditioning approach that uses tree-based proximity scores to down-weight outlying simulations and concentrate computation around the observed dataset. Across two synthetic examples and one real example with incompatible summaries and extreme prior-predictive behaviour, we demonstrate that preconditioning combined with robust NPE increases stability and improves accuracy, calibration, and posterior-predictive fit over standard baseline methods.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.ME",
        "stat.CO",
        "stat.ML"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.18004v1",
      "arxivUrl": "http://arxiv.org/abs/2602.18004v1",
      "doi": null
    },
    {
      "id": "2602.17995",
      "title": "Hybrid Non-informative and Informative Prior Model-assisted Designs for Mid-trial Dose Insertion",
      "authors": [
        "Kana Yamada",
        "Hisato Sunami",
        "Kentaro Takeda",
        "Keisuke Hanada",
        "Masahiro Kojima"
      ],
      "abstract": "In oncology phase I trials, model-assisted designs have been increasingly adopted because they enable adaptive yet operationally simple dose adjustment based on accumulating safety data, leading to a paradigm shift in dose-escalation methodology. In practice, a single mid-trial dose insertion may be considered to examine safer doses and/or to collect more informative efficacy data. In this study, we investigate methods to improve dose assignment and the selection of the maximum tolerated dose (MTD) or the optimal biological dose (OBD) when a new dose level is added during an ongoing trial under a model-assisted framework, by assigning informative prior information to the inserted dose. We propose a hybrid design that uses a non-informative model-assisted design at trial initiation and, upon dose insertion, applies an informative-prior extension only to the newly added dose. In addition, to address potential skeleton misspecification, we propose two adaptive extensions: (i) an online-weighting approach that updates the skeleton over time, and (ii) a Bayesian-mixture approach that robustly combines multiple candidate skeletons. We evaluate the proposed methods through simulation studies.",
      "published": "2026-02-20",
      "updated": "2026-02-20",
      "categories": [
        "stat.ME",
        "stat.AP"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.17995v1",
      "arxivUrl": "http://arxiv.org/abs/2602.17995v1",
      "doi": null
    }
  ],
  "metadata": {
    "lastUpdated": "2026-02-24T02:50:54.653Z",
    "totalPapers": 50,
    "categories": [
      "stat.ME",
      "stat.AP",
      "econ.EM",
      "q-bio.QM"
    ],
    "queryDate": "2026-02-24"
  }
}