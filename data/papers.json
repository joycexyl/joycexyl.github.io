{
  "papers": [
    {
      "id": "2602.23355",
      "title": "Robust model selection using likelihood as data",
      "authors": [
        "Jongwoo Choi",
        "Neil A. Spencer",
        "Jeffrey W. Miller"
      ],
      "abstract": "Model selection is a central task in statistics, but standard methods are not robust in misspecified settings where the true data-generating process (DGP) is not in the set of candidate models. The key limitation is that existing methods -- including information criteria and Bayesian posteriors -- do not quantify uncertainty about how well each candidate model approximates the true DGP. In this paper, we introduce a novel approach to model selection based on modeling the likelihood values themselves. Specifically, given $K$ candidate models and $n$ observations, we view the $n\\times K$ matrix of negative log-likelihood values as a random data matrix and observe that the expectation of each row is equal to the vector of Kullback--Leibler divergences between the $K$ models and the true DGP, up to an additive constant. We use a multivariate normal model to estimate and quantify uncertainty in this expectation, providing calibrated inferences for robust model selection under misspecification. The procedure is easy to compute, interpretable, and comes with theoretical guarantees, including consistency.",
      "published": "2026-02-26",
      "updated": "2026-02-26",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.23355v1",
      "arxivUrl": "http://arxiv.org/abs/2602.23355v1",
      "doi": null
    },
    {
      "id": "2602.23324",
      "title": "Discrete turn strategies emerge in information-limited navigation",
      "authors": [
        "Jose M. Betancourt",
        "Matthew P. Leighton",
        "Thierry Emonet",
        "Benjamin B. Machta",
        "Michael C. Abbott"
      ],
      "abstract": "Navigation up a sensory gradient is one of the simplest behaviours, and the simplest strategy is run and tumble. But some organisms use other strategies, such as reversing direction or turning by some angle. Here we ask what drives the choice of strategy, which we frame as maximising up-gradient speed using a given amount of sensory information per unit time. We find that, without directional information on which way to turn, behavioural strategies which make sudden turns perform better than gradual steering. We see various transitions where a different strategy becomes optimal, such as a switch from reversing direction to fully re-orienting tumbles as more information becomes available. And, among more complex re-orientation strategies, we show that discrete turn angles are best, and see transitions in how many such angles the optimal strategy employs.",
      "published": "2026-02-26",
      "updated": "2026-02-26",
      "categories": [
        "physics.bio-ph",
        "cond-mat.stat-mech",
        "q-bio.QM"
      ],
      "primaryCategory": "physics.bio-ph",
      "pdfUrl": "https://arxiv.org/pdf/2602.23324v1",
      "arxivUrl": "http://arxiv.org/abs/2602.23324v1",
      "doi": null
    },
    {
      "id": "2602.23311",
      "title": "Data-Efficient Generative Modeling of Non-Gaussian Global Climate Fields via Scalable Composite Transformations",
      "authors": [
        "Johannes Brachem",
        "Paul F. V. Wiemann",
        "Matthias Katzfuss"
      ],
      "abstract": "Quantifying uncertainty in future climate projections is hindered by the prohibitive computational cost of running physical climate models, which severely limits the availability of training data. We propose a data-efficient framework for emulating the internal variability of global climate fields, specifically designed to overcome these sample-size constraints. Inspired by copula modeling, our approach constructs a highly expressive joint distribution via a composite transformation to a multivariate standard normal space. We combine a nonparametric Bayesian transport map for spatial dependence modeling with flexible, spatially varying marginal models, essential for capturing non-Gaussian behavior and heavy-tailed extremes. These marginals are defined by a parametric model followed by a semi-parametric B-spline correction to capture complex distributional features. The marginal parameters are spatially smoothed using Gaussian-process priors with low-rank approximations, rendering the computational cost linear in the spatial dimension. When applied to global log-precipitation-rate fields at more than 50,000 grid locations, our stochastic surrogate achieves high fidelity, accurately quantifying the climate distribution's spatial dependence and marginal characteristics, including the tails. Using only 10 training samples, it outperforms a state-of-the-art competitor trained on 80 samples, effectively octupling the computational budget for climate research. We provide a Python implementation at https://github.com/jobrachem/ppptm .",
      "published": "2026-02-26",
      "updated": "2026-02-26",
      "categories": [
        "stat.ME",
        "stat.AP"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.23311v1",
      "arxivUrl": "http://arxiv.org/abs/2602.23311v1",
      "doi": null
    },
    {
      "id": "2602.23291",
      "title": "Identifiability of Treatment Effects with Unobserved Spatially Varying Confounders",
      "authors": [
        "Tommy Tang",
        "Xinran Li",
        "Bo Li"
      ],
      "abstract": "The study of causal effects in the presence of unmeasured spatially varying confounders has garnered increasing attention. However, a general framework for identifiability, which is critical for reliable causal inference from observational data, has yet to be advanced. In this paper, we study a linear model with various parametric model assumptions on the covariance structure between the unmeasured confounder and the exposure of interest. We establish identifiability of the treatment effect for many commonly 20 used spatial models for both discrete and continuous data, under mild conditions on the structure of observation locations and the exposure-confounder association. We also emphasize models or scenarios where identifiability may not hold, under which statistical inference should be conducted with caution.",
      "published": "2026-02-26",
      "updated": "2026-02-26",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.23291v1",
      "arxivUrl": "http://arxiv.org/abs/2602.23291v1",
      "doi": null
    },
    {
      "id": "2602.23269",
      "title": "An Active Learning Framework for Data-Efficient, Human-in-the-Loop Enzyme Function Prediction",
      "authors": [
        "Ashley Babjac",
        "Adrienne Hoarfrost"
      ],
      "abstract": "Generalizable protein function prediction is increasingly constrained by the growing mismatch between exponentially expanding sequences of environmental proteins and the comparatively slow accumulation of experimentally verified functional data. Active learning offers a promising path forward for accelerating biological function prediction, by selecting the most informative proteins to experimentally annotate for data-efficient training, yet its potential remains largely unexplored. We introduce HATTER (Human-in-the-loop Adaptive Toolkit for Transferable Enzyme Representations), a modular framework that integrates multiple active learning strategies with human-in-the-loop experimental annotation to efficiently fine tune function prediction models. We compare active learning training to standard supervised training for biological enzyme function prediction, demonstrating that active learning achieves performance comparable to standard training across diverse protein sequence evaluation datasets while requiring fewer model updates, processing less data, and substantially reducing computational cost. Interestingly, point-based uncertainty sampling methods like entropy or margin sampling perform as well or better than more complex acquisition functions such as bayesian sampling or BALD, highlighting the relative importance of sequence diversity in training datasets and model architecture design. These results demonstrate that human-in-the-loop active learning can efficiently accelerate enzyme discovery, providing a flexible platform for adaptive, scalable, and expert-guided protein function prediction.",
      "published": "2026-02-26",
      "updated": "2026-02-26",
      "categories": [
        "q-bio.QM"
      ],
      "primaryCategory": "q-bio.QM",
      "pdfUrl": "https://arxiv.org/pdf/2602.23269v1",
      "arxivUrl": "http://arxiv.org/abs/2602.23269v1",
      "doi": null
    },
    {
      "id": "2602.23257",
      "title": "Randomization Tests in Switchback Experiments",
      "authors": [
        "Jizhou Liu",
        "Liang Zhong"
      ],
      "abstract": "Switchback experiments--alternating treatment and control over time--are widely used when unit-level randomization is infeasible, outcomes are aggregated, or user interference is unavoidable. In practice, experimentation must support fast product cycles, so teams often run studies for limited durations and make decisions with modest samples. At the same time, outcomes in these time-indexed settings exhibit serial dependence, seasonality, and occasional heavy-tailed shocks, and temporal interference (carryover or anticipation) can render standard asymptotics and naive randomization tests unreliable. In this paper, we develop a randomization-test framework that delivers finite-sample valid, distribution-free p-values for several null hypotheses of interest using only the known assignment mechanism, without parametric assumptions on the outcome process. For causal effects of interests, we impose two primitive conditions--non-anticipation and a finite carryover horizon m--and construct conditional randomization tests (CRTs) based on an ex ante pooling of design blocks into \"sections,\" which yields a tractable conditional assignment law and ensures imputability of focal outcomes. We provide diagnostics for learning the carryover window and assessing non-anticipation, and we introduce studentized CRTs for a session-wise weak null that accommodates within-session seasonality with asymptotic validity. Power approximations under distributed-lag effects with AR(1) noise guide design and analysis choices, and simulations demonstrate favorable size and power relative to common alternatives. Our framework extends naturally to other time-indexed designs.",
      "published": "2026-02-26",
      "updated": "2026-02-26",
      "categories": [
        "stat.ME",
        "econ.EM"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.23257v1",
      "arxivUrl": "http://arxiv.org/abs/2602.23257v1",
      "doi": null
    },
    {
      "id": "2602.23233",
      "title": "The Counterfactual Combine: A Causal Framework for Player Evaluation",
      "authors": [
        "Herbert P. Susmann",
        "Antonio D'Alessandro"
      ],
      "abstract": "Evaluating sports players based on their performance shares core challenges with evaluating healthcare providers based on patient outcomes. Drawing on recent advances in healthcare provider profiling, we cast sports player evaluation within a rigorous causal inference framework and define a flexible class of causal player evaluation estimands. Using stochastic interventions, we compare player success rates on repeated tasks (such as field goal attempts or plate appearance) to counterfactual success rates had those same attempts been randomly reassigned to players according to prespecified reference distributions. This setup encompasses direct and indirect standardization parameters familiar from healthcare provider profiling, and we additionally propose a \"performance above random replacement\" estimand designed for interpretability in sports settings. We develop doubly robust estimators for these evaluation metrics based on modern semiparametric statistical methods, with a focus on Targeted Minimum Loss-based Estimation, and incorporate machine learning methods to capture complex relationships driving player performance. We illustrate our framework in detailed case studies of field goal kickers in the National Football League and batters in Major League Baseball, highlighting how different causal estimands yield distinct interpretations and insights about player performance.",
      "published": "2026-02-26",
      "updated": "2026-02-26",
      "categories": [
        "stat.AP"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.23233v1",
      "arxivUrl": "http://arxiv.org/abs/2602.23233v1",
      "doi": null
    },
    {
      "id": "2602.23143",
      "title": "Dimension Reduction in Multivariate Extremes via Latent Linear Factor Models",
      "authors": [
        "Alexis Boulin",
        "Axel Bücher"
      ],
      "abstract": "We propose a new and interpretable class of high-dimensional tail dependence models based on latent linear factor structures. Specifically, extremal dependence of an observable vector is assumed to be driven by a lower-dimensional latent $K$-factor model, where $K \\ll d$, thereby inducing an explicit form of dimension reduction. Geometrically, this is reflected in the support of the associated spectral dependence measure, whose intrinsic dimension is at most $K-1$. The loading structure may additionally exhibit sparsity, meaning that each component is influenced by only a small number of latent factors, which further enhances interpretability and scalability. Under mild structural assumptions, we establish identifiability of the model parameters and provide a constructive recovery procedure based on a margin-free tail pairwise dependence matrix, which also yields practical rank-based estimation methods. The framework combines naturally with marginal tail models and is particularly well suited to high-dimensional settings. We illustrate its applicability in a spatial wind energy application, where the latent factor structure enables tractable estimation of the risk that a large proportion of turbines simultaneously fall below their cut-in wind speed thresholds.",
      "published": "2026-02-26",
      "updated": "2026-02-26",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.23143v1",
      "arxivUrl": "http://arxiv.org/abs/2602.23143v1",
      "doi": null
    },
    {
      "id": "2602.23131",
      "title": "Titanic overconfidence -- dark uncertainty can sink hybrid metrology for semiconductor manufacturing",
      "authors": [
        "Ronald G. Dixson",
        "Adam L. Pintar",
        "R. Joseph. Kline",
        "Thomas A. Germer",
        "J. Alexander Liddle",
        "John S. Villarrubia",
        "Samuel M. Stavis"
      ],
      "abstract": "Hybrid metrology for semiconductor manufacturing is on a collision course with dark uncertainty. An IEEE technology roadmap for this venture has targeted a linewidth uncertainty of +/- 0.17 nm at 95 % coverage and advised the hybridization of results from different measurement methods to hit this target. Related studies have applied statistical models that require consistent results to compel a lower uncertainty, whereas inconsistent results are prevalent. We illuminate this lurking issue, studying how standard methods of uncertainty evaluation fail to account for the causes and effects of dark uncertainty. We revisit a comparison of imaging and scattering methods to measure linewidths of approximately 13 nm, applying contrasting statistical models to highlight the potential effect of dark uncertainty on hybrid metrology. A random effects model allows the combination of inconsistent results, accounting for dark uncertainty and estimating a total uncertainty of +/- 0.8 nm at 95 % coverage. In contrast, a common mean model requires consistent results for combination, ignoring dark uncertainty and underestimating the total uncertainty by as much as a factor of five. To avoid such titanic overconfidence, which can sink a venture, we outline good practices to reduce dark uncertainty and guide the combination of indeterminately consistent results.",
      "published": "2026-02-26",
      "updated": "2026-02-26",
      "categories": [
        "physics.data-an",
        "stat.AP"
      ],
      "primaryCategory": "physics.data-an",
      "pdfUrl": "https://arxiv.org/pdf/2602.23131v1",
      "arxivUrl": "http://arxiv.org/abs/2602.23131v1",
      "doi": null
    },
    {
      "id": "2602.23087",
      "title": "Forecasting on the Accuracy-Timeliness Frontier: Two Novel `Look Ahead' Predictors",
      "authors": [
        "Marc Wildi"
      ],
      "abstract": "We re-examine the traditional Mean-Squared Error (MSE) forecasting paradigm by formally integrating an accuracy-timeliness trade-off: accuracy is defined by MSE (or target correlation) and timeliness by advancement (or phase excess). While MSE-optimized predictors are accurate in tracking levels, they sacrifice dynamic lead, causing them to lag behind changing targets. To address this, we introduce two `look-ahead' frameworks--Decoupling-from-Present (DFP) and Peak-Correlation-Shifting (PCS)--and provide closed-form solutions for their optimization. Notably, the classical MSE predictor is shown to be a special case within these frameworks. Dually, our methods achieve maximum advancement for any given accuracy level, so our approach reveals the complete efficient frontier of the accuracy-timeliness trade-off, whereas MSE represents only a single point. We also derive a universal upper bound on lead over MSE for any linear predictor under a consistency constraint and prove that our methods hit this ceiling. We validate this approach through applications in forecasting and real-time signal extraction, introducing a leading-indicator criterion and tailored linear benchmarks.",
      "published": "2026-02-26",
      "updated": "2026-02-26",
      "categories": [
        "econ.EM"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.23087v1",
      "arxivUrl": "http://arxiv.org/abs/2602.23087v1",
      "doi": null
    },
    {
      "id": "2602.23045",
      "title": "Semiparametric Joint Inference for Sensitivity and Specificity at the Youden-Optimal Cut-Off",
      "authors": [
        "Siyan Liu",
        "Qinglong Tian",
        "Chunlin Wang",
        "Pengfei Li"
      ],
      "abstract": "Sensitivity and specificity evaluated at an optimal diagnostic cut-off are fundamental measures of classification accuracy when continuous biomarkers are used for disease diagnosis. Joint inference for these quantities is challenging because their estimators are evaluated at a common, data-driven threshold estimated from both diseased and healthy samples, inducing statistical dependence. Existing approaches are largely based on parametric assumptions or fully nonparametric procedures, which may be sensitive to model misspecification or lack efficiency in moderate samples. We propose a semiparametric framework for joint inference on sensitivity and specificity at the Youden-optimal cut-off under the density ratio model. Using maximum empirical likelihood, we derive estimators of the optimal threshold and the corresponding sensitivity and specificity, and establish their joint asymptotic normality. This leads to Wald-type and range-preserving logit-transformed confidence regions. Simulation studies show that the proposed method achieves accurate coverage with improved efficiency relative to existing parametric and nonparametric alternatives across a variety of distributional settings. An analysis of COVID-19 antibody data demonstrates the practical advantages of the proposed approach for diagnostic decision-making.",
      "published": "2026-02-26",
      "updated": "2026-02-26",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.23045v1",
      "arxivUrl": "http://arxiv.org/abs/2602.23045v1",
      "doi": null
    },
    {
      "id": "2602.23020",
      "title": "Testing Partially-Identifiable Causal Queries Using Ternary Tests",
      "authors": [
        "Sourbh Bhadane",
        "Joris M. Mooij",
        "Philip Boeken",
        "Onno Zoeter"
      ],
      "abstract": "We consider hypothesis testing of binary causal queries using observational data. Since the mapping of causal models to the observational distribution that they induce is not one-to-one, in general, causal queries are often only partially identifiable. When binary statistical tests are used for testing partially-identifiable causal queries, their results do not translate in a straightforward manner to the causal hypothesis testing problem. We propose using ternary (three-outcome) statistical tests to test partially-identifiable causal queries. We establish testability requirements that ternary tests must satisfy in terms of uniform consistency and present equivalent topological conditions on the hypotheses. To leverage the existing toolbox of binary tests, we prove that obtaining ternary tests by combining binary tests is complete. Finally, we demonstrate how topological conditions serve as a guide to construct ternary tests for two concrete causal hypothesis testing problems, namely testing the instrumental variable (IV) inequalities and comparing treatment efficacy.",
      "published": "2026-02-26",
      "updated": "2026-02-26",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.23020v1",
      "arxivUrl": "http://arxiv.org/abs/2602.23020v1",
      "doi": null
    },
    {
      "id": "2602.22975",
      "title": "permApprox: a general framework for accurate permutation p-value approximation",
      "authors": [
        "Stefanie Peschel",
        "Anne-Laure Boulesteix",
        "Erika von Mutius",
        "Christian L. Müller"
      ],
      "abstract": "Permutation procedures are common practice in hypothesis testing when distributional assumptions about the test statistic are not met or unknown. With only few permutations, empirical p-values lie on a coarse grid and may even be zero when the observed test statistic exceeds all permuted values. Such zero p-values are statistically invalid and hinder multiple testing correction. Parametric tail modeling with the Generalized Pareto Distribution (GPD) has been proposed to address this issue, but existing implementations can again yield zero p-values when the estimated shape parameter is negative and the fitted distribution has a finite upper bound. We introduce a method for accurate and zero-free p-value approximation in permutation testing, embedded in the permApprox workflow and R package. Building on GPD tail modeling, the method enforces a support constraint during parameter estimation to ensure valid extrapolation beyond the observed statistic, thereby strictly avoiding zero p-values. The workflow further integrates robust parameter estimation, data-driven threshold selection, and principled handling of hybrid p-values that are discrete in the bulk and continuous in the extreme tail. Extensive simulations using two-sample t-tests and Wilcoxon rank-sum tests show that permApprox produces accurate, robust, and zero-free p-value approximations across a wide range of sample and effect sizes. Applications to single-cell RNA-seq and microbiome data demonstrate its practical utility: permApprox yields smooth and interpretable p-value distributions even with few permutations. By resolving the zero-p-value problem while preserving accuracy and computational efficiency, permApprox enables reliable permutation-based inference in high-dimensional and computationally intensive settings.",
      "published": "2026-02-26",
      "updated": "2026-02-26",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.22975v1",
      "arxivUrl": "http://arxiv.org/abs/2602.22975v1",
      "doi": null
    },
    {
      "id": "2602.22965",
      "title": "A note on the area under the likelihood and the fake evidence for model selection",
      "authors": [
        "L. Martino",
        "F. Llorente"
      ],
      "abstract": "Improper priors are not allowed for the computation of the Bayesian evidence $Z=p({\\bf y})$ (a.k.a., marginal likelihood), since in this case $Z$ is not completely specified due to an arbitrary constant involved in the computation. However, in this work, we remark that they can be employed in a specific type of model selection problem: when we have several (possibly infinite) models belonging to the same parametric family (i.e., for tuning parameters of a parametric model). However, the quantities involved in this type of selection cannot be considered as Bayesian evidences: we suggest to use the name ``fake evidences'' (or ``areas under the likelihood'' in the case of uniform improper priors). We also show that, in this model selection scenario, using a diffuse prior and increasing its scale parameter asymptotically to infinity, we cannot recover the value of the area under the likelihood, obtained with a uniform improper prior. We first discuss it from a general point of view. Then we provide, as an applicative example, all the details for Bayesian regression models with nonlinear bases, considering two cases: the use of a uniform improper prior and the use of a Gaussian prior, respectively. A numerical experiment is also provided confirming and checking all the previous statements.",
      "published": "2026-02-26",
      "updated": "2026-02-26",
      "categories": [
        "stat.ME",
        "cs.CE",
        "eess.SP",
        "stat.CO",
        "stat.ML"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.22965v1",
      "arxivUrl": "http://arxiv.org/abs/2602.22965v1",
      "doi": "10.1007/s00180-025-01641-2"
    },
    {
      "id": "2602.22877",
      "title": "Projection depth for functional data: Practical issues, computation and applications",
      "authors": [
        "Filip Bočinec",
        "Stanislav Nagy",
        "Hyemin Yeon"
      ],
      "abstract": "Statistical analysis of functional data is challenging due to their complex patterns, for which functional depth provides an effective means of reflecting their ordering structure. In this work, we investigate practical aspects of the recently proposed regularized projection depth (RPD), which induces a meaningful ordering of functional data while appropriately accommodating their complex shape features. Specifically, we examine the impact and choice of its tuning parameter, which regulates the degree of effective dimension reduction applied to the data, and propose a random projection-based approach for its efficient computation, supported by theoretical justification. Through comprehensive numerical studies, we explore a wide range of statistical applications of the RPD and demonstrate its particular usefulness in uncovering shape features in functional data analysis. This ability allows the RPD to outperform competing depth-based methods, especially in tasks such as functional outlier detection, classification, and two-sample hypothesis testing.",
      "published": "2026-02-26",
      "updated": "2026-02-26",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.22877v1",
      "arxivUrl": "http://arxiv.org/abs/2602.22877v1",
      "doi": null
    },
    {
      "id": "2602.22803",
      "title": "Rejoinder to the discussants of the two JASA articles `Frequentist Model Averaging' and `The Focused Information Ctierion', by Nils Lid Hjort and Gerda Claeskens",
      "authors": [
        "Nils Lid Hjort",
        "Gerda Claeskens"
      ],
      "abstract": "We are honoured to have our work read and discussed at such a thorough level by several experts. Words of appreciation and encouragement are gratefully received, while the many supplementary comments, thoughtful reminders, new perspectives and additional themes raised are warmly welcomed and deeply appreciated. Our thanks go also to JASA Editor Francisco Samaniego and his editorial helpers for organising this discussion. Space does not allow us answering all of the many worthwhile points raised by our discussants, but in the following we make an attempt to respond to what we perceive of as being the major issues. Our responses are organised by themes rather than by discussants. We shall refer to our two articles as `the FMA paper' (Hjort and Claeskens) and `the FIC paper' (Claeskens and Hjort).",
      "published": "2026-02-26",
      "updated": "2026-02-26",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.22803v1",
      "arxivUrl": "http://arxiv.org/abs/2602.22803v1",
      "doi": null
    },
    {
      "id": "2602.22768",
      "title": "Asymptotic Theory and Sequential Test for General Multi-Armed Bandit Process",
      "authors": [
        "Li Yang",
        "Xiaodong Yan",
        "Dandan Jiang"
      ],
      "abstract": "Multi-armed bandit (MAB) processes constitute a foundational subclass of reinforcement learning problems and represent a central topic in statistical decision theory, but are limited to simultaneous adaptive allocation and sequential test, because of the absence of asymptotic theory under non-i.i.d sequence and sublinear information. To address this open challenge, we propose Urn Bandit (UNB) process to integrate the reinforcement mechanism of urn probabilistic models with MAB principles, ensuring almost sure convergence of resource allocation to optimal arms. We establish the joint functional central limit theorem (FCLT) for consistent estimators of expected rewards under non-i.i.d., non-sub-Gaussian and sublinear reward samples with pairwise correlations across arms. To overcome the limitations of existing methods that focus mainly on cumulative regret, we establish the asymptotic theory along with adaptive allocation that serves powerful sequential test, such as arms comparison, A/B testing, and policy valuation. Simulation studies and real data analysis demonstrate that UNB maintains statistical test performance of equal randomization (ER) design but obtain more average rewards like classical MAB processes.",
      "published": "2026-02-26",
      "updated": "2026-02-26",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.22768v1",
      "arxivUrl": "http://arxiv.org/abs/2602.22768v1",
      "doi": null
    },
    {
      "id": "2602.22758",
      "title": "Decomposing Physician Disagreement in HealthBench",
      "authors": [
        "Satya Borgohain",
        "Roy Mariathas"
      ],
      "abstract": "We decompose physician disagreement in the HealthBench medical AI evaluation dataset to understand where variance resides and what observable features can explain it. Rubric identity accounts for 15.8% of met/not-met label variance but only 3.6-6.9% of disagreement variance; physician identity accounts for just 2.4%. The dominant 81.8% case-level residual is not reduced by HealthBench's metadata labels (z = -0.22, p = 0.83), normative rubric language (pseudo R^2 = 1.2%), medical specialty (0/300 Tukey pairs significant), surface-feature triage (AUC = 0.58), or embeddings (AUC = 0.485). Disagreement follows an inverted-U with completion quality (AUC = 0.689), confirming physicians agree on clearly good or bad outputs but split on borderline cases. Physician-validated uncertainty categories reveal that reducible uncertainty (missing context, ambiguous phrasing) more than doubles disagreement odds (OR = 2.55, p < 10^(-24)), while irreducible uncertainty (genuine medical ambiguity) has no effect (OR = 1.01, p = 0.90), though even the former explains only ~3% of total variance. The agreement ceiling in medical AI evaluation is thus largely structural, but the reducible/irreducible dissociation suggests that closing information gaps in evaluation scenarios could lower disagreement where inherent clinical ambiguity does not, pointing toward actionable evaluation design improvements.",
      "published": "2026-02-26",
      "updated": "2026-02-26",
      "categories": [
        "cs.AI",
        "stat.AP"
      ],
      "primaryCategory": "cs.AI",
      "pdfUrl": "https://arxiv.org/pdf/2602.22758v1",
      "arxivUrl": "http://arxiv.org/abs/2602.22758v1",
      "doi": null
    },
    {
      "id": "2602.22694",
      "title": "Robust optimal reconciliation for hierarchical time series forecasting with M-estimation",
      "authors": [
        "Zhichao Wang",
        "Shanshan Wang",
        "Wei Cao",
        "Fei Yang"
      ],
      "abstract": "Aggregation constraints, arising from geographical or sectoral division, frequently emerge in a large set of time series. Coherent forecasts of these constrained series are anticipated to conform to their hierarchical structure organized by the aggregation rules. To enhance its resilience against potential irregular series, we explore the robust reconciliation process for hierarchical time series (HTS) forecasting. We incorporate M-estimation to obtain the reconciled forecasts by minimizing a robust loss function of transforming a group of base forecasts subject to the aggregation constraints. The related minimization procedure is developed and implemented through a modified Newton-Raphson algorithm via local quadratic approximation. Extensive numerical experiments are carried out to evaluate the performance of the proposed method, and the results suggest its feasibility in handling numerous abnormal cases (for instance, series with non-normal errors). The proposed robust reconciliation also demonstrates excellent efficiency when no outliers exist in HTS. Finally, we showcase the practical application of the proposed method in a real-data study on Australian domestic tourism.",
      "published": "2026-02-26",
      "updated": "2026-02-26",
      "categories": [
        "stat.AP"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.22694v1",
      "arxivUrl": "http://arxiv.org/abs/2602.22694v1",
      "doi": null
    },
    {
      "id": "2602.22687",
      "title": "Renewable estimation in linear expectile regression models with streaming data sets",
      "authors": [
        "Wei Cao",
        "Shanshan Wanga",
        "Xiaoxue Hua"
      ],
      "abstract": "Streaming data often exhibit heterogeneity due to heteroscedastic variances or inhomogeneous covariate effects. Online renewable quantile and expectile regression methods provide valuable tools for detecting such heteroscedasticity by combining current data with summary statistics from historical data. However, quantile regression can be computationally demanding because of the non-smooth check function. To address this, we propose a novel online renewable method based on expectile regression, which efficiently updates estimates using both current observations and historical summaries, thereby reducing storage requirements. By exploiting the smoothness of the expectile loss function, our approach achieves superior computational efficiency compared with existing online renewable methods for streaming data with heteroscedastic variances or inhomogeneous covariate effects. We establish the consistency and asymptotic normality of the proposed estimator under mild regularity conditions, demonstrating that it achieves the same statistical efficiency as oracle estimators based on full individual-level data. Numerical experiments and real-data applications demonstrate that our method performs comparably to the oracle estimator while maintaining high computational efficiency and minimal storage costs.",
      "published": "2026-02-26",
      "updated": "2026-02-26",
      "categories": [
        "stat.ME",
        "stat.CO"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.22687v1",
      "arxivUrl": "http://arxiv.org/abs/2602.22687v1",
      "doi": null
    },
    {
      "id": "2602.22684",
      "title": "Learning about Corner Kicks in Soccer by Analysis of Event Times Using a Frailty Model",
      "authors": [
        "Riley L Isaacs",
        "X. Joan Hu",
        "K. Ken Peng",
        "Tim Swartz"
      ],
      "abstract": "Corner kicks are an important event in soccer because they are often the result of strong attacking play and can be of keen interest to sports fans and bettors. Peng, Hu, and Swartz (2024, Computational Statistics) formulate the mixture feature of corner kick times caused by previous corner kicks, frame the commonly available corner kick data as right-censored event times, and explore patterns of corner kicks. This paper extends their modeling to accommodate the potential correlations between corner kicks by the same teams within the same games. We con- sider a frailty model for event times and apply the Monte Carlo Expec- tation Maximization (MCEM) algorithm to obtain the maximum like- lihood estimates for the model parameters. We compare the proposed model with the model in Peng, Hu, and Swartz (2024) using likelihood ratio tests. The 2019 Chinese Super League (CSL) data are employed throughout the paper for motivation and illustration.",
      "published": "2026-02-26",
      "updated": "2026-02-26",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.22684v1",
      "arxivUrl": "http://arxiv.org/abs/2602.22684v1",
      "doi": null
    },
    {
      "id": "2602.22673",
      "title": "Forecasting Antimicrobial Resistance Trends Using Machine Learning on WHO GLASS Surveillance Data: A Retrieval-Augmented Generation Approach for Policy Decision Support",
      "authors": [
        "Md Tanvir Hasan Turja"
      ],
      "abstract": "Antimicrobial resistance (AMR) is a growing global crisis projected to cause 10 million deaths per year by 2050. While the WHO Global Antimicrobial Resistance and Use Surveillance System (GLASS) provides standardized surveillance data across 44 countries, few studies have applied machine learning to forecast population-level resistance trends from this data. This paper presents a two-component framework for AMR trend forecasting and evidence-grounded policy decision support. We benchmark six models -- Naive, Linear Regression, Ridge Regression, XGBoost, LightGBM, and LSTM -- on 5,909 WHO GLASS observations across six WHO regions (2021-2023). XGBoost achieved the best performance with a test MAE of 7.07% and R-squared of 0.854, outperforming the naive baseline by 83.1%. Feature importance analysis identified the prior-year resistance rate as the dominant predictor (50.5% importance), while regional MAE ranged from 4.16% (European Region) to 10.14% (South-East Asia Region). We additionally implemented a Retrieval-Augmented Generation (RAG) pipeline combining a ChromaDB vector store of WHO policy documents with a locally deployed Phi-3 Mini language model, producing source-attributed, hallucination-constrained policy answers. Code and data are available at https://github.com/TanvirTurja",
      "published": "2026-02-26",
      "updated": "2026-02-26",
      "categories": [
        "cs.LG",
        "q-bio.QM"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2602.22673v1",
      "arxivUrl": "http://arxiv.org/abs/2602.22673v1",
      "doi": null
    },
    {
      "id": "2602.22648",
      "title": "A General (Non-Markovian) Framework for Covariate Adaptive Randomization: Achieving Balance While Eliminating the Shift",
      "authors": [
        "Hengjia Fang",
        "Wei Ma"
      ],
      "abstract": "Emerging applications increasingly demand flexible covariate adaptive randomization (CAR) methods that support unequal targeted allocation ratios. While existing procedures can achieve covariate balance, they often suffer from the shift problem. This occurs when the allocation ratios of some additional covariates deviate from the target. We show that this problem is equivalent to a mismatch between the conditional average allocation ratio and the target among units sharing specific covariate values, revealing a failure of existing procedures in the long run. To address it, we derive a new form of allocation function by requiring that balancing covariates ensures the ratio matches the target. Based on this form, we design a class of parameterized allocation functions. When the parameter roughly matches certain characteristics of the covariate distribution, the resulting procedure can balance covariates. Thus, we propose a feasible randomization procedure that updates the parameter based on collected covariate information, rendering the procedure non-Markovian. To accommodate this, we introduce a CAR framework that allows non-Markovian procedure. We then establish its key theoretical properties, including the boundedness of covariate imbalance in probability and the asymptotic distribution of the imbalance for additional covariates. Ultimately, we conclude that the feasible randomization procedure can achieve covariate balance and eliminate the shift.",
      "published": "2026-02-26",
      "updated": "2026-02-26",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.22648v1",
      "arxivUrl": "http://arxiv.org/abs/2602.22648v1",
      "doi": null
    },
    {
      "id": "2602.22612",
      "title": "Feasible Fusion: Constrained Joint Estimation under Structural Non-Overlap",
      "authors": [
        "Yuxi Du",
        "Zhiheng Zhang",
        "Haoxuan Li",
        "Cong Fang",
        "Jixing Xu",
        "Peng Zhen",
        "Jiecheng Guo"
      ],
      "abstract": "Causal inference in modern largescale systems faces growing challenges, including highdimensional covariates, multi-valued treatments, massive observational (OBS) data, and limited randomized controlled trial (RCT) samples due to cost constraints. We formalize treatment-induced structural non-overlap and show that, under this regime, commonly used weighted fusion methods provably fail to satisfy randomized identifying restrictions.To address this issue,we propose a constrained joint estimation framework that minimizes observational risk while enforcing causal validity through orthogonal experimental moment conditions. We further show that structural non-overlap creates a feasibility obstruction for moment enforcement in the original covariate space.We also derive a penalized primaldual algorithm that jointly learns representations and predictors, and establish oracle inequalities decomposing error into overlap recovery, moment violation, and statistical terms.Extensive synthetic experiments demonstrate robust performance under varying degrees of nonoverlap. A largescale ridehailing application shows that our method achieves substantial gains over existing baselines, matching the performance of models trained with significantly more RCT data.",
      "published": "2026-02-26",
      "updated": "2026-02-26",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.22612v1",
      "arxivUrl": "http://arxiv.org/abs/2602.22612v1",
      "doi": null
    },
    {
      "id": "2602.22590",
      "title": "Beyond Vintage Rotation: Bias-Free Sparse Representation Learning with Oracle Inference",
      "authors": [
        "Chengyu Cui",
        "Yunxiao Chen",
        "Jing Ouyang",
        "Gongjun Xu"
      ],
      "abstract": "Learning low-dimensional latent representations is a central topic in statistics and machine learning, and rotation methods have long been used to obtain sparse and interpretable representations. Despite nearly a century of widespread use across many fields, rigorous guarantees for valid inference for the learned representation remain lacking. In this paper, we identify a surprisingly prevalent phenomenon that suggests a reason for this gap: for a broad class of vintage rotations, the resulting estimators exhibit a non-estimable bias. Because this bias is independent of the data, it fundamentally precludes the development of valid inferential procedures, including the construction of confidence intervals and hypothesis testing. To address this challenge, we propose a novel bias-free rotation method within a general representation learning framework based on latent variables. We establish an oracle inference property for the learned sparse representations: the estimators achieve the same asymptotic variance as in the ideal setting where the latent variables are observed. To bridge the gap between theory and computation, we develop an efficient computational framework and prove that its output estimators retain the same oracle property. Our results provide a rigorous inference procedure for the rotated estimators, yielding statistically valid and interpretable representation learning.",
      "published": "2026-02-26",
      "updated": "2026-02-26",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.22590v1",
      "arxivUrl": "http://arxiv.org/abs/2602.22590v1",
      "doi": null
    },
    {
      "id": "2602.22588",
      "title": "Modeling Covariate Feedback, Reversal, and Latent Traits in Longitudinal Data: A Joint Hierarchical Framework",
      "authors": [
        "Niloofar Ramezani",
        "Pascal Nitiema",
        "Jeffrey R. Wilson"
      ],
      "abstract": "Time-varying covariates in longitudinal studies frequently evolve through reciprocal feedback, undergo role reversal, and reflect unobserved individual heterogeneity. Standard statistical frameworks often assume fixed covariate roles and exogenous predictors, limiting their utility in systems governed by dynamic behavioral or physiological processes. We develop a hierarchical joint modeling framework that unifies three key features of such systems: (i) bidirectional feedback between a binary and a continuous covariate, (ii) role reversal in which these covariates become jointly modeled outcomes at a prespecified decision phase, and (iii) a shared latent trait influencing both intermediate covariates and a final binary endpoint. The model proceeds in three phases: a feedback-driven longitudinal process, a reversal phase in which the two covariates are jointly modeled conditional on the latent trait, and an outcome model linking a binary, decision-relevant endpoint to observed and latent components. Estimation is carried out using maximum likelihood and Bayesian inference, with Hamiltonian Monte Carlo supporting robust posterior estimation for models with latent structure and mixed outcome types. Simulation studies show that the model yields well calibrated coverage, small bias, and improved predictive performance compared to standard generalized linear mixed models, marginal approaches, and models that ignore feedback or latent traits. In an analysis of nationally representative U.S. panel data, the model captures the co-evolution of physical activity and body mass index and their joint influence, moderated by a latent behavioral resilience factor, on income mobility. The framework offers a flexible, practically implementable tool for analyzing longitudinal decision systems in which feedback, covariate role transition, and unmeasured capacity are central to prediction and intervention.",
      "published": "2026-02-26",
      "updated": "2026-02-26",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.22588v1",
      "arxivUrl": "http://arxiv.org/abs/2602.22588v1",
      "doi": null
    },
    {
      "id": "2602.22582",
      "title": "Predictive variational inference for flexible regression models",
      "authors": [
        "Lucas Kock",
        "Scott A. Sisson",
        "G. S. Rodrigues",
        "David J. Nott"
      ],
      "abstract": "A conventional Bayesian approach to prediction uses the posterior distribution to integrate out parameters in a density for unobserved data conditional on the observed data and parameters. When the true posterior is intractable, it is replaced by an approximation; here we focus on variational approximations. Recent work has explored methods that learn posteriors optimized for predictive accuracy under a chosen scoring rule, while regularizing toward the prior or conventional posterior. Our work builds on an existing predictive variational inference (PVI) framework that improves prediction, but also diagnoses model deficiencies through implicit model expansion. In models where the sampling density depends on the parameters through a linear predictor, we improve the interpretability of existing PVI methods as a diagnostic tool. This is achieved by adopting PVI posteriors of Gaussian mixture form (GM-PVI) and establishing connections with plug-in prediction for mixture-of-experts models. We make three main contributions. First, we show that GM-PVI prediction is equivalent to plug-in prediction for certain mixture-of-experts models with covariate-independent weights in generalized linear models and hierarchical extensions of them. Second, we extend standard PVI by allowing GM-PVI posteriors to vary with the prediction covariate and in this case an equivalence to plug-in prediction for mixtures of experts with covariate-dependent weights is established. Third, we demonstrate the diagnostic value of this approach across several examples, including generalized linear models, linear mixed models, and latent Gaussian process models, demonstrating how the parameters of the original model must vary across the covariate space to achieve improvements in prediction.",
      "published": "2026-02-26",
      "updated": "2026-02-26",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.22582v1",
      "arxivUrl": "http://arxiv.org/abs/2602.22582v1",
      "doi": null
    },
    {
      "id": "2602.22307",
      "title": "The global structure of the time delay likelihood",
      "authors": [
        "Namu Kroupa",
        "Will Handley"
      ],
      "abstract": "We identify a fundamental pathology in the likelihood for time delay inference which challenges standard inference methods. By analysing the likelihood for time delay inference with Gaussian process light curve models, we show that it generically develops a boundary-driven \"W\"-shape with a global maximum at the true delay and gradual rises towards the edges of the observation window. This arises because time delay estimation is intrinsically extrapolative. In practice, global samplers such as nested sampling are steered towards spurious edge modes unless strict convergence criteria are adopted. We demonstrate this with simulations and show that the effect strengthens with higher data density over a fixed time span. To ensure convergence, we provide concrete guidance, notably increasing the number of live points. Further, we show that methods implicitly favouring small delays, for example optimisers and local MCMC, induce a bias towards larger $H_0$. Our results clarify failure modes and offer practical remedies for robust fully Bayesian time delay inference.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "stat.ME",
        "astro-ph.CO",
        "astro-ph.GA",
        "astro-ph.IM"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.22307v1",
      "arxivUrl": "http://arxiv.org/abs/2602.22307v1",
      "doi": null
    },
    {
      "id": "2602.22203",
      "title": "Local Bayesian Regression",
      "authors": [
        "Nils Lid Hjort"
      ],
      "abstract": "This paper develops a class of Bayesian non- and semiparametric methods for estimating regression curves and surfaces. The main idea is to model the regression as locally linear, and then place suitable local priors on the local parameters. The method requires the posterior distribution of the local parameters given local data, and this is found via a suitably defined local likelihood function. When the width of the local data window is large the methods reduce to familiar fully parametric Bayesian methods, and when the width is small the estimators are essentially nonparametric. When noninformative reference priors are used the resulting estimators coincide with recently developed well-performing local weighted least squares methods for nonparametric regression. Each local prior distribution needs in general a centre parameter and a variance parameter. Of particular interest are versions of the scheme that are more or less automatic and objective in the sense that they do not require subjective specifications of prior parameters. We therefore develop empirical Bayes methods to obtain the variance parameter and a hierarchical Bayes method to account for uncertainty in the choice of centre parameter. There are several possible versions of the general programme, and a number of its specialisations are discussed. Some of these are shown to be capable of outperforming standard nonparametric regression methods, particularly in situations with several covariates.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.22203v1",
      "arxivUrl": "http://arxiv.org/abs/2602.22203v1",
      "doi": null
    },
    {
      "id": "2602.22083",
      "title": "Coarsening Bias from Variable Discretization in Causal Functionals",
      "authors": [
        "Xiaxian Ou",
        "Razieh Nabi"
      ],
      "abstract": "A class of causal effect functionals requires integration over conditional densities of continuous variables, as in mediation effects and nonparametric identification in causal graphical models. Estimating such densities and evaluating the resulting integrals can be statistically and computationally demanding. A common workaround is to discretize the variable and replace integrals with finite sums. Although convenient, discretization alters the population-level functional and can induce non-negligible approximation bias, even under correct identification. Under smoothness conditions, we show that this coarsening bias is first order in the bin width and arises at the level of the target functional, distinct from statistical estimation error. We propose a simple bias-reduced functional that evaluates the outcome regression at within-bin conditional means, eliminating the leading term and yielding a second-order approximation error. We derive plug-in and one-step estimators for the bias-reduced functional. Simulations demonstrate substantial bias reduction and near-nominal confidence interval coverage, even under coarse binning. Our results provide a simple framework for controlling the impact of variable discretization on parameter approximation and estimation.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "stat.ME",
        "cs.LG",
        "stat.ML"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.22083v1",
      "arxivUrl": "http://arxiv.org/abs/2602.22083v1",
      "doi": null
    },
    {
      "id": "2602.22062",
      "title": "Robust Model Selection for Discovery of Latent Mechanistic Processes",
      "authors": [
        "Jiawei Li",
        "Nguyen Nguyen",
        "Meng Lai",
        "Ioannis Ch. Paschalidis",
        "Jonathan H. Huggins"
      ],
      "abstract": "When learning interpretable latent structures using model-based approaches, even small deviations from modeling assumptions can lead to inferential results that are not mechanistically meaningful. In this work, we consider latent structures that consist of $K_o$ mechanistic processes, where $K_o$ is unknown. When the model is misspecified, likelihood-based model selection methods can substantially overestimate $K_o$ while more robust nonparametric methods can be overly conservative. Hence, there is a need for approaches that combine the sensitivity of likelihood-based methods with the robustness of nonparametric ones. We formalize this objective in terms of a robust model selection consistency property, which is based on a component-level discrepancy measure that captures the mechanistic structure of the model. We then propose the accumulated cutoff discrepancy criterion (ACDC), which leverages plug-in estimates of component-level discrepancies. To apply ACDC, we develop mechanistically meaningful component-level discrepancies for a general class of latent variable models that includes unsupervised and supervised variants of probabilistic matrix factorization and mixture modeling. We show that ACDC is robustly consistent when applied to unsupervised matrix factorization and mixture models. Numerical results demonstrate that in practice our approach reliably identifies a mechanistically meaningful number of latent processes in numerous illustrative applications, outperforming existing methods.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.22062v1",
      "arxivUrl": "http://arxiv.org/abs/2602.22062v1",
      "doi": null
    },
    {
      "id": "2602.22021",
      "title": "Budgeted Active Experimentation for Treatment Effect Estimation from Observational and Randomized Data",
      "authors": [
        "Jiacan Gao",
        "Xinyan Su",
        "Mingyuan Ma",
        "Yiyan Huang",
        "Xiao Xu",
        "Xinrui Wan",
        "Tianqi Gu",
        "Enyun Yu",
        "Jiecheng Guo",
        "Zhiheng Zhang"
      ],
      "abstract": "Estimating heterogeneous treatment effects is central to data-driven decision-making, yet industrial applications often face a fundamental tension between limited randomized controlled trial (RCT) budgets and abundant but biased observational data collected under historical targeting policies. Although observational logs offer the advantage of scale, they inherently suffer from severe policyinduced imbalance and overlap violations, rendering standalone estimation unreliable. We propose a budgeted active experimentation framework that iteratively enhances model training for causal effect estimation via active sampling. By leveraging observational priors, we develop an acquisition function targeting uplift estimation uncertainty, overlap deficits, and domain discrepancy to select the most informative units for randomized experiments. We establish finite-sample deviation bounds, asymptotic normality via martingale Central Limit Theorems (CLTs), and minimax lower bounds to prove information-theoretic optimality. Extensive experiments on industrial datasets demonstrate that our approach significantly outperforms standard randomized baselines in cost-constrained settings.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.22021v1",
      "arxivUrl": "http://arxiv.org/abs/2602.22021v1",
      "doi": null
    },
    {
      "id": "2602.21998",
      "title": "Design-based theory for causal inference from adaptive experiments",
      "authors": [
        "Xinran Li",
        "Anqi Zhao"
      ],
      "abstract": "Adaptive designs dynamically update treatment probabilities using information accumulated during the experiment. Existing theory for causal inference from adaptive experiments primarily assumes the superpopulation framework with independent and identically distributed units, and may not apply when the distribution of units evolves over time. This paper makes two contributions. First, we extend the literature to the finite-population framework, which allows for possibly nonexchangeable units, and establish the design-based theory for causal inference under general adaptive designs using inverse-propensity-weighted (IPW) and augmented IPW (AIPW) estimators. Our theory accommodates nonexchangeable units, both nonconverging and vanishing treatment probabilities, and nonconverging outcome estimators, thereby justifying inference using AIPW estimators with black-box outcome models that integrate advances from machine learning methods. To alleviate the conservativeness inherent in variance estimation under finite-population inference, we also introduce a covariance estimator for the AIPW estimator that becomes sharp when the residuals from the adaptive regression of potential outcomes on covariates are additive across units. Our framework encompasses widely used adaptive designs, such as multi-armed bandits, covariate-adaptive randomization, and sequential rerandomization, advancing the design-based theory for causal inference in these specific settings. Second, as a methodological contribution, we propose an adaptive covariate adjustment approach for analyzing even nonadaptive designs. The martingale structure induced by adaptive adjustment enables valid inference with black-box outcome estimators that would otherwise require strong assumptions under standard nonadaptive analysis.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.21998v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21998v1",
      "doi": null
    },
    {
      "id": "2602.21993",
      "title": "Prediction of source nutrients for microorganisms using metabolic networks",
      "authors": [
        "Olivia Bulka",
        "Chabname Ghassemi Nedjad",
        "Loïc Paulevé",
        "Sylvain Prigent",
        "Clémence Frioux"
      ],
      "abstract": "Metagenomics has lowered the barrier to microbial discovery--enabling the identification of novel microbes without isolation--but cultures remain imperative for the deep study of microbes. Cultivation and isolation of non-model microbes remains a major challenge, despite advances in high-throughput culturomic methods. The quantity of simultaneous experimental variables is constrained by time and resources, but the list can be reduced using computational biology. Given an annotated genome, metabolic modelling can be used to predict source nutrients required for the growth of a microbe, which acts as an initial screen to inform culture and isolation experiments. This chapter provides an overview of metabolic networks and modelling and how they can be used to predict the nutrient requirements of a microorganism, followed by a sample protocol using a toy metabolic network, which is then expanded to a genome-scale metabolic network application. These methods can be applied to any metabolic network of interest--which in turn can be created from any genome of interest--and are a starting point for experimental validation of source nutrients required for microorganisms that remain uncultivated to date.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "q-bio.MN",
        "q-bio.QM"
      ],
      "primaryCategory": "q-bio.MN",
      "pdfUrl": "https://arxiv.org/pdf/2602.21993v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21993v1",
      "doi": null
    },
    {
      "id": "2602.21969",
      "title": "Estimation of the complexity of a network under a Gaussian graphical model",
      "authors": [
        "Nabaneet Das",
        "Thorsten Dickhaus"
      ],
      "abstract": "The proportion of edges in a Gaussian graphical model (GGM) characterizes the complexity of its conditional dependence structure. Since edge presence corresponds to a nonzero entry of the precision matrix, estimation of this proportion can be formulated as a large-scale multiple testing problem. We propose an estimator that combines p-values from simultaneous edge-wise tests, conducted under false discovery rate control, with Storey's estimator of the proportion of true null hypotheses. We establish weak dependence conditions on the precision matrix under which the empirical cumulative distribution function of the p-values converges to its population counterpart. These conditions cover high-dimensional regimes, including those arising in genetic association studies. Under such dependence, we characterize the asymptotic bias of the Schweder--Spjøtvoll estimator, showing that it is upward biased and thus slightly underestimates the true edge proportion. Simulation studies across a variety of models confirm accurate recovery of graph complexity.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.21969v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21969v1",
      "doi": null
    },
    {
      "id": "2602.22289",
      "title": "What Topological and Geometric Structure Do Biological Foundation Models Learn? Evidence from 141 Hypotheses",
      "authors": [
        "Ihor Kendiukhov"
      ],
      "abstract": "When biological foundation models such as scGPT and Geneformer process single-cell gene expression, what geometric and topological structure forms in their internal representations? Is that structure biologically meaningful or a training artifact, and how confident should we be in such claims? We address these questions through autonomous large-scale hypothesis screening: an AI-driven executor-brainstormer loop that proposed, tested, and refined 141 geometric and topological hypotheses across 52 iterations, covering persistent homology, manifold distances, cross-model alignment, community structure, and directed topology, all with explicit null controls and disjoint gene-pool splits. Three principal findings emerge. First, the models learn genuine geometric structure. Gene embedding neighborhoods exhibit non-trivial topology, with persistent homology significant in 11 of 12 transformer layers at p < 0.05 in the weakest domain and 12 of 12 in the other two. A multi-level distance hierarchy shows that manifold-aware metrics outperform Euclidean distance for identifying regulatory gene pairs, and graph community partitions track known transcription factor target relationships. Second, this structure is shared across independently trained models. CCA alignment between scGPT and Geneformer yields canonical correlation of 0.80 and gene retrieval accuracy of 72 percent, yet none of 19 tested methods reliably recover gene-level correspondences. The models agree on the global shape of gene space but not on precise gene placement. Third, the structure is more localized than it first appears. Under stringent null controls applied across all null families, robust signal concentrates in immune tissue, while lung and external lung signals weaken substantially.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "q-bio.QM",
        "cs.LG",
        "q-bio.GN"
      ],
      "primaryCategory": "q-bio.QM",
      "pdfUrl": "https://arxiv.org/pdf/2602.22289v1",
      "arxivUrl": "http://arxiv.org/abs/2602.22289v1",
      "doi": null
    },
    {
      "id": "2602.21903",
      "title": "Jackknife Inference for Fixed Effects Models",
      "authors": [
        "Ayden Higgins"
      ],
      "abstract": "This paper develops a general method of inference for fixed effects models which is (i) automatic, (ii) computationally inexpensive, and (iii) highly model agnostic. Specifically, we show how to combine a collection of subsample estimators into a self-normalised jackknife $t$-statistic, from which hypothesis tests, confidence intervals, and $p$-values are readily obtained.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "econ.EM"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.21903v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21903v1",
      "doi": null
    },
    {
      "id": "2602.21876",
      "title": "Comparative Evaluation of Machine Learning Models for Predicting Donor Kidney Discard",
      "authors": [
        "Peer Schliephacke",
        "Hannah Schult",
        "Leon Mizera",
        "Judith Würfel",
        "Gunter Grieser",
        "Axel Rahmel",
        "Carl-Ludwig Fischer-Fröhlich",
        "Antje Jahn-Eimermacher"
      ],
      "abstract": "A kidney transplant can improve the life expectancy and quality of life of patients with end-stage renal failure. Even more patients could be helped with a transplant if the rate of kidneys that are discarded and not transplanted could be reduced. Machine learning (ML) can support decision-making in this context by early identification of donor organs at high risk of discard, for instance to enable timely interventions to improve organ utilization such as rescue allocation. Although various ML models have been applied, their results are difficult to compare due to heterogenous datasets and differences in feature engineering and evaluation strategies. This study aims to provide a systematic and reproducible comparison of ML models for donor kidney discard prediction. We trained five commonly used ML models: Logistic Regression, Decision Tree, Random Forest, Gradient Boosting, and Deep Learning along with an ensemble model on data from 4,080 deceased donors (death determined by neurologic criteria) in Germany. A unified benchmarking framework was implemented, including standardized feature engineering and selection, and Bayesian hyperparameter optimization. Model performance was assessed for discrimination (MCC, AUC, F1), calibration (Brier score), and explainability (SHAP). The ensemble achieved the highest discrimination performance (MCC=0.76, AUC=0.87, F1=0.90), while individual models such as Logistic Regression, Random Forest, and Deep Learning performed comparably and better than Decision Trees. Platt scaling improved calibration for tree-and neural network-based models. SHAP consistently identified donor age and renal markers as dominant predictors across models, reflecting clinical plausibility. This study demonstrates that consistent data preprocessing, feature selection, and evaluation can be more decisive for predictive success than the choice of the ML algorithm.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "stat.AP"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.21876v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21876v1",
      "doi": null
    },
    {
      "id": "2602.21846",
      "title": "Scalable Kernel-Based Distances for Statistical Inference and Integration",
      "authors": [
        "Masha Naslidnyk"
      ],
      "abstract": "Representing, comparing, and measuring the distance between probability distributions is a key task in computational statistics and machine learning. The choice of representation and the associated distance determine properties of the methods in which they are used: for example, certain distances can allow one to encode robustness or smoothness of the problem. Kernel methods offer flexible and rich Hilbert space representations of distributions that allow the modeller to enforce properties through the choice of kernel, and estimate associated distances at efficient nonparametric rates. In particular, the maximum mean discrepancy (MMD), a kernel-based distance constructed by comparing Hilbert space mean functions, has received significant attention due to its computational tractability and is favoured by practitioners. In this thesis, we conduct a thorough study of kernel-based distances with a focus on efficient computation, with core contributions in Chapters 3 to 6. Part I of the thesis is focused on the MMD, specifically on improved MMD estimation. In Chapter 3 we propose a theoretically sound, improved estimator for MMD in simulation-based inference. Then, in Chapter 4, we propose an MMD-based estimator for conditional expectations, a ubiquitous task in statistical computation. Closing Part I, in Chapter 5 we study the problem of calibration when MMD is applied to the task of integration. In Part II, motivated by the recent developments in kernel embeddings beyond the mean, we introduce a family of novel kernel-based discrepancies: kernel quantile discrepancies. These address some of the pitfalls of MMD, and are shown through both theoretical results and an empirical study to offer a competitive alternative to MMD and its fast approximations. We conclude with a discussion on broader lessons and future work emerging from the thesis.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.ST",
        "stat.ME"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2602.21846v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21846v1",
      "doi": null
    },
    {
      "id": "2602.22282",
      "title": "Differentially Private Truncation of Unbounded Data via Public Second Moments",
      "authors": [
        "Zilong Cao",
        "Xuan Bi",
        "Hai Zhang"
      ],
      "abstract": "Data privacy is important in the AI era, and differential privacy (DP) is one of the golden solutions. However, DP is typically applicable only if data have a bounded underlying distribution. We address this limitation by leveraging second-moment information from a small amount of public data. We propose Public-moment-guided Truncation (PMT), which transforms private data using the public second-moment matrix and applies a principled truncation whose radius depends only on non-private quantities: data dimension and sample size. This transformation yields a well-conditioned second-moment matrix, enabling its inversion with a significantly strengthened ability to resist the DP noise. Furthermore, we demonstrate the applicability of PMT by using penalized and generalized linear regressions. Specifically, we design new loss functions and algorithms, ensuring that solutions in the transformed space can be mapped back to the original domain. We have established improvements in the models' DP estimation through theoretical error bounds, robustness guarantees, and convergence results, attributing the gains to the conditioning effect of PMT. Experiments on synthetic and real datasets confirm that PMT substantially improves the accuracy and stability of DP models.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "cs.CR",
        "cs.LG",
        "stat.AP",
        "stat.ME",
        "stat.ML"
      ],
      "primaryCategory": "cs.CR",
      "pdfUrl": "https://arxiv.org/pdf/2602.22282v1",
      "arxivUrl": "http://arxiv.org/abs/2602.22282v1",
      "doi": null
    },
    {
      "id": "2602.21713",
      "title": "Multi-Parameter Estimation of Prevalence (MPEP): A Bayesian modelling approach to estimate the prevalence of opioid dependence",
      "authors": [
        "Andreas Markoulidakis",
        "Matthew Hickman",
        "Nicky J Welton",
        "Loukia Meligkotsidou",
        "Hayley E Jones"
      ],
      "abstract": "Estimating the number of the number of people from hidden and/or marginalised populations - such as people dependent on opioids or cocaine - is important to guide policy decisions and provision of harm reduction services. Methods such as capture-recapture are widely used, but rely on assumptions that are often violated and not feasible in specific applications. We describe a Bayesian modelling approach called Multi-Parameter Estimation of Prevalence (MPEP). The MPEP approach leverages routinely collected administrative data, starting from a large baseline cohort of individuals from the population of interest and linked events, to estimate the full size of the target population. When multiple event types are included, the approach enables checking of the consistency of evidence about prevalence from different event types. Additional evidence can be incorporated where inconsistencies are identified. In this article, we summarize the general framework of MPEP, with focus on the most recent version, with improved computational efficiency (implemented in STAN). We also explore several extensions to the model that help us understand the sensitivity of the results to modelling assumptions or identify potential sources of bias. We demonstrate the MPEP approach through a case study estimating the prevalence of opioid dependence in Scotland each year from 2014 to 2022.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.21713v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21713v1",
      "doi": null
    },
    {
      "id": "2602.21711",
      "title": "Adaptive Penalized Doubly Robust Regression for Longitudinal Data",
      "authors": [
        "Yuyao Wang",
        "Yu Lu",
        "Tianni Zhang",
        "Mengfei Ran"
      ],
      "abstract": "Longitudinal data often involve heterogeneity, sparse signals, and contamination from response outliers or high-leverage observations especially in biomedical science. Existing methods usually address only part of this problem, either emphasizing penalized mixed effects modeling without robustness or robust mixed effects estimation without high-dimensional variable selection. We propose a doubly adaptive robust regression (DAR-R) framework for longitudinal linear mixed effects models. It combines a robust pilot fit, doubly adaptive observation weights for residual outliers and leverage points, and folded concave penalization for fixed effect selection, together with weighted updates of random effects and variance components. We develop an iterative reweighting algorithm and establish estimation and prediction error bounds, support recovery consistency, and oracle-type asymptotic normality. Simulations show that DAR-R improves estimation accuracy, false-positive control, and covariance estimation under both vertical outliers and bad leverage contamination. In the TADPOLE/ADNI Alzheimer's disease application, DAR-R achieves accurate and stable prediction of ADAS13 while selecting clinically meaningful predictors with strong resampling stability.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.AP",
        "stat.CO"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.21711v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21711v1",
      "doi": null
    },
    {
      "id": "2602.21663",
      "title": "Estimation, inference and model selection for jump regression models",
      "authors": [
        "Steffen Grønneberg",
        "Gudmund Hermansen",
        "Nils Lid Hjort"
      ],
      "abstract": "We consider regression models with data of the type $y_i=m(x_i)+\\varepsilon_i$, where the $m(x)$ curve is taken locally constant, with unknown levels and jump points. We investigate the large-sample properties of the minimum least squares estimators, finding in particular that jump point parameters and level parameters are estimated with respectively $n$-rate precision and $\\sqrt{n}$-rate precision, where $n$ is sample size. Bayes solutions are investigated as well and found to be superior. We then construct jump information criteria, respectively AJIC and BJIC, for selecting the right number of jump points from data. This is done by following the line of arguments that lead to the Akaike and Bayesian information criteria AIC and BIC, but which here lead to different formulae due to the different type of large-sample approximations involved.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.21663v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21663v1",
      "doi": null
    },
    {
      "id": "2602.21648",
      "title": "Multimodal Survival Modeling and Fairness-Aware Clinical Machine Learning for 5-Year Breast Cancer Risk Prediction",
      "authors": [
        "Toktam Khatibi"
      ],
      "abstract": "Clinical risk prediction models often underperform in real-world settings due to poor calibration, limited transportability, and subgroup disparities. These challenges are amplified in high-dimensional multimodal cancer datasets characterized by complex feature interactions and a p >> n structure. We present a fully reproducible multimodal machine learning framework for 5-year overall survival prediction in breast cancer, integrating clinical variables with high-dimensional transcriptomic and copy-number alteration (CNA) features from the METABRIC cohort. After variance- and sparsity-based filtering and dimensionality reduction, models were trained using stratified train/validation/test splits with validation-based hyperparameter tuning. Two survival approaches were compared: an elastic-net regularized Cox model (CoxNet) and a gradient-boosted survival tree model implemented using XGBoost. CoxNet provides embedded feature selection and stable estimation, whereas XGBoost captures nonlinear effects and higher-order interactions. Performance was assessed using time-dependent area under the ROC curve (AUC), average precision (AP), calibration curves, Brier score, and bootstrapped 95 percent confidence intervals. CoxNet achieved validation and test AUCs of 98.3 and 96.6, with AP values of 90.1 and 80.4. XGBoost achieved validation and test AUCs of 98.6 and 92.5, with AP values of 92.5 and 79.9. Fairness diagnostics showed stable discrimination across age groups, estrogen receptor status, molecular subtypes, and menopausal state. This work introduces a governance-oriented multimodal survival framework emphasizing calibration, fairness auditing, robustness, and reproducibility for high-dimensional clinical machine learning.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "cs.LG",
        "q-bio.QM"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2602.21648v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21648v1",
      "doi": null
    },
    {
      "id": "2602.21579",
      "title": "Asymptotically Optimal Sequential Confidence Interval for the Gini Index Under Complex Household Survey Design with Sub-Stratification",
      "authors": [
        " Shivam",
        "Bhargab Chattopadhyay",
        "Nil Kamal Hazra"
      ],
      "abstract": "We examine the optimality properties of the Gini index estimator under complex survey design involving stratification, clustering, and sub-stratification. While Darku et al. (Econometrics, 26, 2020) considered only stratification and clustering and did not provide theoretical guarantees, this study addresses these limitations by proposing two procedures - a purely sequential method and a two-stage method. Under suitable regularity conditions, we establish uniform continuity in probability for the proposed estimator, thereby contributing to the development of random central limit theorems under sequential sampling frameworks. Furthermore, we show that the resulting procedures satisfy both asymptotic first-order efficiency and asymptotic consistency. Simulation results demonstrate that the proposed procedures achieve the desired optimality properties across diverse settings. The practical utility of the methodology is further illustrated through an empirical application using data collected by the National Sample Survey agency of India",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.21579v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21579v1",
      "doi": null
    },
    {
      "id": "2602.21572",
      "title": "Goodness-of-Fit Tests for Latent Class Models with Ordinal Categorical Data",
      "authors": [
        "Huan Qing"
      ],
      "abstract": "Ordinal categorical data are widely collected in psychology, education, and other social sciences, appearing commonly in questionnaires, assessments, and surveys. Latent class models provide a flexible framework for uncovering unobserved heterogeneity by grouping individuals into homogeneous classes based on their response patterns. A fundamental challenge in applying these models is determining the number of latent classes, which is unknown and must be inferred from data. In this paper, we propose one test statistic for this problem. The test statistic centers the largest singular value of a normalized residual matrix by a simple sample-size adjustment. Under the null hypothesis that the candidate number of latent classes is correct, its upper bound converges to zero in probability. Under an under-fitted alternative, the statistic itself exceeds a fixed positive constant with probability approaching one. This sharp dichotomous behavior of the test statistic yields two sequential testing algorithms that consistently estimate the true number of latent classes. Extensive experimental studies confirm the theoretical findings and demonstrate their accuracy and reliability in determining the number of latent classes.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2602.21572v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21572v1",
      "doi": null
    },
    {
      "id": "2602.21569",
      "title": "How many asymmetric communities are there in multi-layer directed networks?",
      "authors": [
        "Huan Qing"
      ],
      "abstract": "Estimating the asymmetric numbers of communities in multi-layer directed networks is a challenging problem due to the multi-layer structures and inherent directional asymmetry, leading to possibly different numbers of sender and receiver communities. This work addresses this issue under the multi-layer stochastic co-block model, a model for multi-layer directed networks with distinct community structures in sending and receiving sides, by proposing a novel goodness-of-fit test. The test statistic relies on the deviation of the largest singular value of an aggregated normalized residual matrix from the constant 2. The test statistic exhibits a sharp dichotomy: Under the null hypothesis of correct model specification, its upper bound converges to zero with high probability; under underfitting, the test statistic itself diverges to infinity. With this property, we develop a sequential testing procedure that searches through candidate pairs of sender and receiver community numbers in a lexicographic order. The process stops at the smallest such pair where the test statistic drops below a decaying threshold. For robustness, we also propose a ratio-based variant algorithm, which detects sharp changes in the sequence of test statistics by comparing consecutive candidates. Both methods are proven to consistently determine the true numbers of sender and receiver communities under the multi-layer stochastic co-block model.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "math.ST",
        "cs.LG",
        "stat.ME",
        "stat.ML"
      ],
      "primaryCategory": "math.ST",
      "pdfUrl": "https://arxiv.org/pdf/2602.21569v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21569v1",
      "doi": null
    },
    {
      "id": "2602.22263",
      "title": "CryoNet.Refine: A One-step Diffusion Model for Rapid Refinement of Structural Models with Cryo-EM Density Map Restraints",
      "authors": [
        "Fuyao Huang",
        "Xiaozhu Yu",
        "Kui Xu",
        "Qiangfeng Cliff Zhang"
      ],
      "abstract": "High-resolution structure determination by cryo-electron microscopy (cryo-EM) requires the accurate fitting of an atomic model into an experimental density map. Traditional refinement pipelines such as Phenix.real_space_refine and Rosetta are computationally expensive, demand extensive manual tuning, and present a significant bottleneck for researchers. We present CryoNet.Refine, an end-to-end deep learning framework that automates and accelerates molecular structure refinement. Our approach utilizes a one-step diffusion model that integrates a density-aware loss function with robust stereochemical restraints, enabling rapid optimization of a structure against experimental data. CryoNet.Refine provides a unified and versatile solution capable of refining protein complexes as well as DNA/RNA-protein complexes. In benchmarks against Phenix.real_space_refine, CryoNet.Refine consistently achieves substantial improvements in both model-map correlation and overall geometric quality metrics. By offering a scalable, automated, and powerful alternative, CryoNet.Refine aims to serve as an essential tool for next-generation cryo-EM structure refinement. Web server: https://cryonet.ai/refine; Source code: https://github.com/kuixu/cryonet.refine.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "eess.IV",
        "q-bio.QM"
      ],
      "primaryCategory": "q-bio.BM",
      "pdfUrl": "https://arxiv.org/pdf/2602.22263v1",
      "arxivUrl": "http://arxiv.org/abs/2602.22263v1",
      "doi": null
    },
    {
      "id": "2602.21509",
      "title": "Fair Model-based Clustering",
      "authors": [
        "Jinwon Park",
        "Kunwoong Kim",
        "Jihu Lee",
        "Yongdai Kim"
      ],
      "abstract": "The goal of fair clustering is to find clusters such that the proportion of sensitive attributes (e.g., gender, race, etc.) in each cluster is similar to that of the entire dataset. Various fair clustering algorithms have been proposed that modify standard K-means clustering to satisfy a given fairness constraint. A critical limitation of several existing fair clustering algorithms is that the number of parameters to be learned is proportional to the sample size because the cluster assignment of each datum should be optimized simultaneously with the cluster center, and thus scaling up the algorithms is difficult. In this paper, we propose a new fair clustering algorithm based on a finite mixture model, called Fair Model-based Clustering (FMC). A main advantage of FMC is that the number of learnable parameters is independent of the sample size and thus can be scaled up easily. In particular, mini-batch learning is possible to obtain clusters that are approximately fair. Moreover, FMC can be applied to non-metric data (e.g., categorical data) as long as the likelihood is well-defined. Theoretical and empirical justifications for the superiority of the proposed algorithm are provided.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.AP"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2602.21509v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21509v1",
      "doi": null
    },
    {
      "id": "2602.21490",
      "title": "Connection Probabilities Estimation in Multi-layer Networks via Iterative Neighborhood Smoothing",
      "authors": [
        "Dingzi Guo",
        "Diqing Li",
        "Jingyi Wang",
        "Wen-Xin Zhou"
      ],
      "abstract": "Understanding the structural mechanisms of multi-layer networks is essential for analyzing complex systems characterized by multiple interacting layers. This work studies the problem of estimating connection probabilities in multi-layer networks and introduces a new Multi-layer Iterative Connection Probability Estimation (MICE) method. The proposed approach employs an iterative framework that jointly refines inter-layer and intra-layer similarity sets by dynamically updating distance metrics derived from current probability estimates. By leveraging both layer-level and node-level neighborhood information, MICE improves estimation accuracy while preserving computational efficiency. Theoretical analysis establishes the consistency of the estimator and shows that, under mild regularity conditions, the proposed method achieves an optimal convergence rate comparable to that of an oracle estimator. Extensive simulation studies across diverse graphon structures demonstrate the superior performance of MICE relative to existing methods. Empirical evaluations using brain network data from patients with Attention-Deficit/Hyperactivity Disorder (ADHD) and global food and agricultural trade network data further illustrate the robustness and effectiveness of the method in link prediction tasks. Overall, this work provides a theoretically grounded and practically scalable framework for probabilistic modeling and inference in multi-layer network systems.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.21490v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21490v1",
      "doi": null
    }
  ],
  "metadata": {
    "lastUpdated": "2026-02-28T02:33:12.218Z",
    "totalPapers": 50,
    "categories": [
      "stat.ME",
      "stat.AP",
      "econ.EM",
      "q-bio.QM"
    ],
    "queryDate": "2026-02-28"
  }
}