{
  "papers": [
    {
      "id": "2602.22021",
      "title": "Budgeted Active Experimentation for Treatment Effect Estimation from Observational and Randomized Data",
      "authors": [
        "Jiacan Gao",
        "Xinyan Su",
        "Mingyuan Ma",
        "Yiyan Huang",
        "Xiao Xu",
        "Xinrui Wan",
        "Tianqi Gu",
        "Enyun Yu",
        "Jiecheng Guo",
        "Zhiheng Zhang"
      ],
      "abstract": "Estimating heterogeneous treatment effects is central to data-driven decision-making, yet industrial applications often face a fundamental tension between limited randomized controlled trial (RCT) budgets and abundant but biased observational data collected under historical targeting policies. Although observational logs offer the advantage of scale, they inherently suffer from severe policyinduced imbalance and overlap violations, rendering standalone estimation unreliable. We propose a budgeted active experimentation framework that iteratively enhances model training for causal effect estimation via active sampling. By leveraging observational priors, we develop an acquisition function targeting uplift estimation uncertainty, overlap deficits, and domain discrepancy to select the most informative units for randomized experiments. We establish finite-sample deviation bounds, asymptotic normality via martingale Central Limit Theorems (CLTs), and minimax lower bounds to prove information-theoretic optimality. Extensive experiments on industrial datasets demonstrate that our approach significantly outperforms standard randomized baselines in cost-constrained settings.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.22021v1",
      "arxivUrl": "http://arxiv.org/abs/2602.22021v1",
      "doi": null
    },
    {
      "id": "2602.21998",
      "title": "Design-based theory for causal inference from adaptive experiments",
      "authors": [
        "Xinran Li",
        "Anqi Zhao"
      ],
      "abstract": "Adaptive designs dynamically update treatment probabilities using information accumulated during the experiment. Existing theory for causal inference from adaptive experiments primarily assumes the superpopulation framework with independent and identically distributed units, and may not apply when the distribution of units evolves over time. This paper makes two contributions. First, we extend the literature to the finite-population framework, which allows for possibly nonexchangeable units, and establish the design-based theory for causal inference under general adaptive designs using inverse-propensity-weighted (IPW) and augmented IPW (AIPW) estimators. Our theory accommodates nonexchangeable units, both nonconverging and vanishing treatment probabilities, and nonconverging outcome estimators, thereby justifying inference using AIPW estimators with black-box outcome models that integrate advances from machine learning methods. To alleviate the conservativeness inherent in variance estimation under finite-population inference, we also introduce a covariance estimator for the AIPW estimator that becomes sharp when the residuals from the adaptive regression of potential outcomes on covariates are additive across units. Our framework encompasses widely used adaptive designs, such as multi-armed bandits, covariate-adaptive randomization, and sequential rerandomization, advancing the design-based theory for causal inference in these specific settings. Second, as a methodological contribution, we propose an adaptive covariate adjustment approach for analyzing even nonadaptive designs. The martingale structure induced by adaptive adjustment enables valid inference with black-box outcome estimators that would otherwise require strong assumptions under standard nonadaptive analysis.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.21998v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21998v1",
      "doi": null
    },
    {
      "id": "2602.21993",
      "title": "Prediction of source nutrients for microorganisms using metabolic networks",
      "authors": [
        "Olivia Bulka",
        "Chabname Ghassemi Nedjad",
        "Loïc Paulevé",
        "Sylvain Prigent",
        "Clémence Frioux"
      ],
      "abstract": "Metagenomics has lowered the barrier to microbial discovery--enabling the identification of novel microbes without isolation--but cultures remain imperative for the deep study of microbes. Cultivation and isolation of non-model microbes remains a major challenge, despite advances in high-throughput culturomic methods. The quantity of simultaneous experimental variables is constrained by time and resources, but the list can be reduced using computational biology. Given an annotated genome, metabolic modelling can be used to predict source nutrients required for the growth of a microbe, which acts as an initial screen to inform culture and isolation experiments. This chapter provides an overview of metabolic networks and modelling and how they can be used to predict the nutrient requirements of a microorganism, followed by a sample protocol using a toy metabolic network, which is then expanded to a genome-scale metabolic network application. These methods can be applied to any metabolic network of interest--which in turn can be created from any genome of interest--and are a starting point for experimental validation of source nutrients required for microorganisms that remain uncultivated to date.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "q-bio.MN",
        "q-bio.QM"
      ],
      "primaryCategory": "q-bio.MN",
      "pdfUrl": "https://arxiv.org/pdf/2602.21993v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21993v1",
      "doi": null
    },
    {
      "id": "2602.21969",
      "title": "Estimation of the complexity of a network under a Gaussian graphical model",
      "authors": [
        "Nabaneet Das",
        "Thorsten Dickhaus"
      ],
      "abstract": "The proportion of edges in a Gaussian graphical model (GGM) characterizes the complexity of its conditional dependence structure. Since edge presence corresponds to a nonzero entry of the precision matrix, estimation of this proportion can be formulated as a large-scale multiple testing problem. We propose an estimator that combines p-values from simultaneous edge-wise tests, conducted under false discovery rate control, with Storey's estimator of the proportion of true null hypotheses. We establish weak dependence conditions on the precision matrix under which the empirical cumulative distribution function of the p-values converges to its population counterpart. These conditions cover high-dimensional regimes, including those arising in genetic association studies. Under such dependence, we characterize the asymptotic bias of the Schweder--Spjøtvoll estimator, showing that it is upward biased and thus slightly underestimates the true edge proportion. Simulation studies across a variety of models confirm accurate recovery of graph complexity.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.21969v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21969v1",
      "doi": null
    },
    {
      "id": "2602.21903",
      "title": "Jackknife Inference for Fixed Effects Models",
      "authors": [
        "Ayden Higgins"
      ],
      "abstract": "This paper develops a general method of inference for fixed effects models which is (i) automatic, (ii) computationally inexpensive, and (iii) highly model agnostic. Specifically, we show how to combine a collection of subsample estimators into a self-normalised jackknife $t$-statistic, from which hypothesis tests, confidence intervals, and $p$-values are readily obtained.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "econ.EM"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.21903v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21903v1",
      "doi": null
    },
    {
      "id": "2602.21876",
      "title": "Comparative Evaluation of Machine Learning Models for Predicting Donor Kidney Discard",
      "authors": [
        "Peer Schliephacke",
        "Hannah Schult",
        "Leon Mizera",
        "Judith Würfel",
        "Gunter Grieser",
        "Axel Rahmel",
        "Carl-Ludwig Fischer-Fröhlich",
        "Antje Jahn-Eimermacher"
      ],
      "abstract": "A kidney transplant can improve the life expectancy and quality of life of patients with end-stage renal failure. Even more patients could be helped with a transplant if the rate of kidneys that are discarded and not transplanted could be reduced. Machine learning (ML) can support decision-making in this context by early identification of donor organs at high risk of discard, for instance to enable timely interventions to improve organ utilization such as rescue allocation. Although various ML models have been applied, their results are difficult to compare due to heterogenous datasets and differences in feature engineering and evaluation strategies. This study aims to provide a systematic and reproducible comparison of ML models for donor kidney discard prediction. We trained five commonly used ML models: Logistic Regression, Decision Tree, Random Forest, Gradient Boosting, and Deep Learning along with an ensemble model on data from 4,080 deceased donors (death determined by neurologic criteria) in Germany. A unified benchmarking framework was implemented, including standardized feature engineering and selection, and Bayesian hyperparameter optimization. Model performance was assessed for discrimination (MCC, AUC, F1), calibration (Brier score), and explainability (SHAP). The ensemble achieved the highest discrimination performance (MCC=0.76, AUC=0.87, F1=0.90), while individual models such as Logistic Regression, Random Forest, and Deep Learning performed comparably and better than Decision Trees. Platt scaling improved calibration for tree-and neural network-based models. SHAP consistently identified donor age and renal markers as dominant predictors across models, reflecting clinical plausibility. This study demonstrates that consistent data preprocessing, feature selection, and evaluation can be more decisive for predictive success than the choice of the ML algorithm.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "stat.AP"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.21876v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21876v1",
      "doi": null
    },
    {
      "id": "2602.21846",
      "title": "Scalable Kernel-Based Distances for Statistical Inference and Integration",
      "authors": [
        "Masha Naslidnyk"
      ],
      "abstract": "Representing, comparing, and measuring the distance between probability distributions is a key task in computational statistics and machine learning. The choice of representation and the associated distance determine properties of the methods in which they are used: for example, certain distances can allow one to encode robustness or smoothness of the problem. Kernel methods offer flexible and rich Hilbert space representations of distributions that allow the modeller to enforce properties through the choice of kernel, and estimate associated distances at efficient nonparametric rates. In particular, the maximum mean discrepancy (MMD), a kernel-based distance constructed by comparing Hilbert space mean functions, has received significant attention due to its computational tractability and is favoured by practitioners. In this thesis, we conduct a thorough study of kernel-based distances with a focus on efficient computation, with core contributions in Chapters 3 to 6. Part I of the thesis is focused on the MMD, specifically on improved MMD estimation. In Chapter 3 we propose a theoretically sound, improved estimator for MMD in simulation-based inference. Then, in Chapter 4, we propose an MMD-based estimator for conditional expectations, a ubiquitous task in statistical computation. Closing Part I, in Chapter 5 we study the problem of calibration when MMD is applied to the task of integration. In Part II, motivated by the recent developments in kernel embeddings beyond the mean, we introduce a family of novel kernel-based discrepancies: kernel quantile discrepancies. These address some of the pitfalls of MMD, and are shown through both theoretical results and an empirical study to offer a competitive alternative to MMD and its fast approximations. We conclude with a discussion on broader lessons and future work emerging from the thesis.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.ST",
        "stat.ME"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2602.21846v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21846v1",
      "doi": null
    },
    {
      "id": "2602.21713",
      "title": "Multi-Parameter Estimation of Prevalence (MPEP): A Bayesian modelling approach to estimate the prevalence of opioid dependence",
      "authors": [
        "Andreas Markoulidakis",
        "Matthew Hickman",
        "Nicky J Welton",
        "Loukia Meligkotsidou",
        "Hayley E Jones"
      ],
      "abstract": "Estimating the number of the number of people from hidden and/or marginalised populations - such as people dependent on opioids or cocaine - is important to guide policy decisions and provision of harm reduction services. Methods such as capture-recapture are widely used, but rely on assumptions that are often violated and not feasible in specific applications. We describe a Bayesian modelling approach called Multi-Parameter Estimation of Prevalence (MPEP). The MPEP approach leverages routinely collected administrative data, starting from a large baseline cohort of individuals from the population of interest and linked events, to estimate the full size of the target population. When multiple event types are included, the approach enables checking of the consistency of evidence about prevalence from different event types. Additional evidence can be incorporated where inconsistencies are identified. In this article, we summarize the general framework of MPEP, with focus on the most recent version, with improved computational efficiency (implemented in STAN). We also explore several extensions to the model that help us understand the sensitivity of the results to modelling assumptions or identify potential sources of bias. We demonstrate the MPEP approach through a case study estimating the prevalence of opioid dependence in Scotland each year from 2014 to 2022.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.21713v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21713v1",
      "doi": null
    },
    {
      "id": "2602.21711",
      "title": "Adaptive Penalized Doubly Robust Regression for Longitudinal Data",
      "authors": [
        "Yuyao Wang",
        "Yu Lu",
        "Tianni Zhang",
        "Mengfei Ran"
      ],
      "abstract": "Longitudinal data often involve heterogeneity, sparse signals, and contamination from response outliers or high-leverage observations especially in biomedical science. Existing methods usually address only part of this problem, either emphasizing penalized mixed effects modeling without robustness or robust mixed effects estimation without high-dimensional variable selection. We propose a doubly adaptive robust regression (DAR-R) framework for longitudinal linear mixed effects models. It combines a robust pilot fit, doubly adaptive observation weights for residual outliers and leverage points, and folded concave penalization for fixed effect selection, together with weighted updates of random effects and variance components. We develop an iterative reweighting algorithm and establish estimation and prediction error bounds, support recovery consistency, and oracle-type asymptotic normality. Simulations show that DAR-R improves estimation accuracy, false-positive control, and covariance estimation under both vertical outliers and bad leverage contamination. In the TADPOLE/ADNI Alzheimer's disease application, DAR-R achieves accurate and stable prediction of ADAS13 while selecting clinically meaningful predictors with strong resampling stability.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.AP",
        "stat.CO"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.21711v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21711v1",
      "doi": null
    },
    {
      "id": "2602.21663",
      "title": "Estimation, inference and model selection for jump regression models",
      "authors": [
        "Steffen Grønneberg",
        "Gudmund Hermansen",
        "Nils Lid Hjort"
      ],
      "abstract": "We consider regression models with data of the type $y_i=m(x_i)+\\varepsilon_i$, where the $m(x)$ curve is taken locally constant, with unknown levels and jump points. We investigate the large-sample properties of the minimum least squares estimators, finding in particular that jump point parameters and level parameters are estimated with respectively $n$-rate precision and $\\sqrt{n}$-rate precision, where $n$ is sample size. Bayes solutions are investigated as well and found to be superior. We then construct jump information criteria, respectively AJIC and BJIC, for selecting the right number of jump points from data. This is done by following the line of arguments that lead to the Akaike and Bayesian information criteria AIC and BIC, but which here lead to different formulae due to the different type of large-sample approximations involved.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.21663v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21663v1",
      "doi": null
    },
    {
      "id": "2602.21648",
      "title": "Multimodal Survival Modeling and Fairness-Aware Clinical Machine Learning for 5-Year Breast Cancer Risk Prediction",
      "authors": [
        "Toktam Khatibi"
      ],
      "abstract": "Clinical risk prediction models often underperform in real-world settings due to poor calibration, limited transportability, and subgroup disparities. These challenges are amplified in high-dimensional multimodal cancer datasets characterized by complex feature interactions and a p >> n structure. We present a fully reproducible multimodal machine learning framework for 5-year overall survival prediction in breast cancer, integrating clinical variables with high-dimensional transcriptomic and copy-number alteration (CNA) features from the METABRIC cohort. After variance- and sparsity-based filtering and dimensionality reduction, models were trained using stratified train/validation/test splits with validation-based hyperparameter tuning. Two survival approaches were compared: an elastic-net regularized Cox model (CoxNet) and a gradient-boosted survival tree model implemented using XGBoost. CoxNet provides embedded feature selection and stable estimation, whereas XGBoost captures nonlinear effects and higher-order interactions. Performance was assessed using time-dependent area under the ROC curve (AUC), average precision (AP), calibration curves, Brier score, and bootstrapped 95 percent confidence intervals. CoxNet achieved validation and test AUCs of 98.3 and 96.6, with AP values of 90.1 and 80.4. XGBoost achieved validation and test AUCs of 98.6 and 92.5, with AP values of 92.5 and 79.9. Fairness diagnostics showed stable discrimination across age groups, estrogen receptor status, molecular subtypes, and menopausal state. This work introduces a governance-oriented multimodal survival framework emphasizing calibration, fairness auditing, robustness, and reproducibility for high-dimensional clinical machine learning.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "cs.LG",
        "q-bio.QM"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2602.21648v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21648v1",
      "doi": null
    },
    {
      "id": "2602.21579",
      "title": "Asymptotically Optimal Sequential Confidence Interval for the Gini Index Under Complex Household Survey Design with Sub-Stratification",
      "authors": [
        " Shivam",
        "Bhargab Chattopadhyay",
        "Nil Kamal Hazra"
      ],
      "abstract": "We examine the optimality properties of the Gini index estimator under complex survey design involving stratification, clustering, and sub-stratification. While Darku et al. (Econometrics, 26, 2020) considered only stratification and clustering and did not provide theoretical guarantees, this study addresses these limitations by proposing two procedures - a purely sequential method and a two-stage method. Under suitable regularity conditions, we establish uniform continuity in probability for the proposed estimator, thereby contributing to the development of random central limit theorems under sequential sampling frameworks. Furthermore, we show that the resulting procedures satisfy both asymptotic first-order efficiency and asymptotic consistency. Simulation results demonstrate that the proposed procedures achieve the desired optimality properties across diverse settings. The practical utility of the methodology is further illustrated through an empirical application using data collected by the National Sample Survey agency of India",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.21579v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21579v1",
      "doi": null
    },
    {
      "id": "2602.21572",
      "title": "Goodness-of-Fit Tests for Latent Class Models with Ordinal Categorical Data",
      "authors": [
        "Huan Qing"
      ],
      "abstract": "Ordinal categorical data are widely collected in psychology, education, and other social sciences, appearing commonly in questionnaires, assessments, and surveys. Latent class models provide a flexible framework for uncovering unobserved heterogeneity by grouping individuals into homogeneous classes based on their response patterns. A fundamental challenge in applying these models is determining the number of latent classes, which is unknown and must be inferred from data. In this paper, we propose one test statistic for this problem. The test statistic centers the largest singular value of a normalized residual matrix by a simple sample-size adjustment. Under the null hypothesis that the candidate number of latent classes is correct, its upper bound converges to zero in probability. Under an under-fitted alternative, the statistic itself exceeds a fixed positive constant with probability approaching one. This sharp dichotomous behavior of the test statistic yields two sequential testing algorithms that consistently estimate the true number of latent classes. Extensive experimental studies confirm the theoretical findings and demonstrate their accuracy and reliability in determining the number of latent classes.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2602.21572v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21572v1",
      "doi": null
    },
    {
      "id": "2602.21569",
      "title": "How many asymmetric communities are there in multi-layer directed networks?",
      "authors": [
        "Huan Qing"
      ],
      "abstract": "Estimating the asymmetric numbers of communities in multi-layer directed networks is a challenging problem due to the multi-layer structures and inherent directional asymmetry, leading to possibly different numbers of sender and receiver communities. This work addresses this issue under the multi-layer stochastic co-block model, a model for multi-layer directed networks with distinct community structures in sending and receiving sides, by proposing a novel goodness-of-fit test. The test statistic relies on the deviation of the largest singular value of an aggregated normalized residual matrix from the constant 2. The test statistic exhibits a sharp dichotomy: Under the null hypothesis of correct model specification, its upper bound converges to zero with high probability; under underfitting, the test statistic itself diverges to infinity. With this property, we develop a sequential testing procedure that searches through candidate pairs of sender and receiver community numbers in a lexicographic order. The process stops at the smallest such pair where the test statistic drops below a decaying threshold. For robustness, we also propose a ratio-based variant algorithm, which detects sharp changes in the sequence of test statistics by comparing consecutive candidates. Both methods are proven to consistently determine the true numbers of sender and receiver communities under the multi-layer stochastic co-block model.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "math.ST",
        "cs.LG",
        "stat.ME",
        "stat.ML"
      ],
      "primaryCategory": "math.ST",
      "pdfUrl": "https://arxiv.org/pdf/2602.21569v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21569v1",
      "doi": null
    },
    {
      "id": "2602.21509",
      "title": "Fair Model-based Clustering",
      "authors": [
        "Jinwon Park",
        "Kunwoong Kim",
        "Jihu Lee",
        "Yongdai Kim"
      ],
      "abstract": "The goal of fair clustering is to find clusters such that the proportion of sensitive attributes (e.g., gender, race, etc.) in each cluster is similar to that of the entire dataset. Various fair clustering algorithms have been proposed that modify standard K-means clustering to satisfy a given fairness constraint. A critical limitation of several existing fair clustering algorithms is that the number of parameters to be learned is proportional to the sample size because the cluster assignment of each datum should be optimized simultaneously with the cluster center, and thus scaling up the algorithms is difficult. In this paper, we propose a new fair clustering algorithm based on a finite mixture model, called Fair Model-based Clustering (FMC). A main advantage of FMC is that the number of learnable parameters is independent of the sample size and thus can be scaled up easily. In particular, mini-batch learning is possible to obtain clusters that are approximately fair. Moreover, FMC can be applied to non-metric data (e.g., categorical data) as long as the likelihood is well-defined. Theoretical and empirical justifications for the superiority of the proposed algorithm are provided.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.AP"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2602.21509v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21509v1",
      "doi": null
    },
    {
      "id": "2602.21490",
      "title": "Connection Probabilities Estimation in Multi-layer Networks via Iterative Neighborhood Smoothing",
      "authors": [
        "Dingzi Guo",
        "Diqing Li",
        "Jingyi Wang",
        "Wen-Xin Zhou"
      ],
      "abstract": "Understanding the structural mechanisms of multi-layer networks is essential for analyzing complex systems characterized by multiple interacting layers. This work studies the problem of estimating connection probabilities in multi-layer networks and introduces a new Multi-layer Iterative Connection Probability Estimation (MICE) method. The proposed approach employs an iterative framework that jointly refines inter-layer and intra-layer similarity sets by dynamically updating distance metrics derived from current probability estimates. By leveraging both layer-level and node-level neighborhood information, MICE improves estimation accuracy while preserving computational efficiency. Theoretical analysis establishes the consistency of the estimator and shows that, under mild regularity conditions, the proposed method achieves an optimal convergence rate comparable to that of an oracle estimator. Extensive simulation studies across diverse graphon structures demonstrate the superior performance of MICE relative to existing methods. Empirical evaluations using brain network data from patients with Attention-Deficit/Hyperactivity Disorder (ADHD) and global food and agricultural trade network data further illustrate the robustness and effectiveness of the method in link prediction tasks. Overall, this work provides a theoretically grounded and practically scalable framework for probabilistic modeling and inference in multi-layer network systems.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.21490v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21490v1",
      "doi": null
    },
    {
      "id": "2602.21478",
      "title": "Efficient Inference after Directionally Stable Adaptive Experiments",
      "authors": [
        "Zikai Shen",
        "Houssam Zenati",
        "Nathan Kallus",
        "Arthur Gretton",
        "Koulik Khamaru",
        "Aurélien Bibaut"
      ],
      "abstract": "We study inference on scalar-valued pathwise differentiable targets after adaptive data collection, such as a bandit algorithm. We introduce a novel target-specific condition, directional stability, which is strictly weaker than previously imposed target-agnostic stability conditions. Under directional stability, we show that estimators that would have been efficient under i.i.d. data remain asymptotically normal and semiparametrically efficient when computed from adaptively collected trajectories. The canonical gradient has a martingale form, and directional stability guarantees stabilization of its predictable quadratic variation, enabling high-dimensional asymptotic normality. We characterize efficiency using a convolution theorem for the adaptive-data setting, and give a condition under which the one-step estimator attains the efficiency bound. We verify directional stability for LinUCB, yielding the first semiparametric efficiency guarantee for a regular scalar target under LinUCB sampling.",
      "published": "2026-02-25",
      "updated": "2026-02-25",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.ST",
        "stat.ME"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2602.21478v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21478v1",
      "doi": null
    },
    {
      "id": "2602.21423",
      "title": "Causal Inference with High-Dimensional Treatments",
      "authors": [
        "Patrick Kramer",
        "Edward H. Kennedy",
        "Isaac M. Opper"
      ],
      "abstract": "In this work, we consider causal inference in various high-dimensional treatment settings, including for single multi-valued treatments and vector treatments with binary or continuous components, when the number of treatments can be comparable to or even larger than the number of observations. These settings bring unique challenges: first, the treatment effects of interest are a high-dimensional vector rather than a low-dimensional scalar; second, positivity violations are often unavoidable; and third, estimation can be based on a smaller effective sample size. We first discuss fundamental limits of estimating effects here, showing that consistent estimation is impossible without further assumptions. We go on to propose a novel sparse pseudo-outcome regression framework for arbitrary high-dimensional statistical functionals, which includes generic constrained regression estimators and error guarantees. We use the framework to derive new doubly robust estimators for mean potential outcomes of high-dimensional treatments, though it can also be applied to other scenarios. We analyze the proposed estimators under exact and approximate sparsity assumptions, giving finite-sample risk bounds. Finally, we derive minimax lower bounds to characterize optimal rates of convergence and show our risk bounds are unimprovable.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "math.ST",
        "stat.ME"
      ],
      "primaryCategory": "math.ST",
      "pdfUrl": "https://arxiv.org/pdf/2602.21423v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21423v1",
      "doi": null
    },
    {
      "id": "2602.21410",
      "title": "Identifying the potential of sample overlap in evidence synthesis of observational studies",
      "authors": [
        "Zhentian Zhang",
        "Tim Friede",
        "Tim Mathes"
      ],
      "abstract": "Sample overlap is a common issue in evidence synthesis in the field of medical research, particularly when integrating findings from observational studies utilizing existing databases such as registries. Due to the general inaccessibility of unique identifiers for each observation, addressing sample overlap has been a complex problem, potentially biasing evidence synthesis outcomes and undermining their credibility. We developed a method to construct indicators for the degree of sample overlap in evidence synthesis of studies based on existing data. Our method is rooted in set theory and is based on the coding of the ranges of several well selected sample characteristics, offers a practical solution by focusing on making inference based on sample characteristics rather than on individual participant data. Useful information, such as the overlap-free sample set with the largest sample size in an evidence synthesis, can be derived from this method. We applied our model to several real-world evidence syntheses, demonstrating its effectiveness and flexibility. Our findings highlight the growing importance of addressing sample overlap in evidence synthesis, especially with the increasing relevance of secondary use of data, an area currently under-explored in research.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.21410v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21410v1",
      "doi": null
    },
    {
      "id": "2602.21408",
      "title": "Generative Bayesian Computation as a Scalable Alternative to Gaussian Process Surrogates",
      "authors": [
        "Nick Polson",
        "Vadim Sokolov"
      ],
      "abstract": "Gaussian process (GP) surrogates are the default tool for emulating expensive computer experiments, but cubic cost, stationarity assumptions, and Gaussian predictive distributions limit their reach. We propose Generative Bayesian Computation (GBC) via Implicit Quantile Networks (IQNs) as a surrogate framework that targets all three limitations. GBC learns the full conditional quantile function from input--output pairs; at test time, a single forward pass per quantile level produces draws from the predictive distribution. Across fourteen benchmarks we compare GBC to four GP-based methods. GBC improves CRPS by 11--26\\% on piecewise jump-process benchmarks, by 14\\% on a ten-dimensional Friedman function, and scales linearly to 90,000 training points where dense-covariance GPs are infeasible. A boundary-augmented variant matches or outperforms Modular Jump GPs on two-dimensional jump datasets (up to 46\\% CRPS improvement). In active learning, a randomized-prior IQN ensemble achieves nearly three times lower RMSE than deep GP active learning on Rocket LGBB. Overall, GBC records a favorable point estimate in 12 of 14 comparisons. GPs retain an edge on smooth surfaces where their smoothness prior provides effective regularization.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "cs.LG",
        "stat.AP",
        "stat.CO",
        "stat.ME",
        "stat.ML"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2602.21408v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21408v1",
      "doi": null
    },
    {
      "id": "2602.21403",
      "title": "An index of effective number of variables for uncertainty and reliability analysis in model selection problems",
      "authors": [
        "Luca Martino",
        "Eduardo Morgado",
        "Roberto San Millán-Castillo"
      ],
      "abstract": "An index of an effective number of variables (ENV) is introduced for model selection in nested models. This is the case, for instance, when we have to decide the order of a polynomial function or the number of bases in a nonlinear regression, choose the number of clusters in a clustering problem, or the number of features in a variable selection application (to name few examples). It is inspired by the idea of the maximum area under the curve (AUC). The interpretation of the ENV index is identical to the effective sample size (ESS) indices concerning a set of samples. The ENV index improves {drawbacks of} the elbow detectors described in the literature and introduces different confidence measures of the proposed solution. These novel measures can be also employed jointly with the use of different information criteria, such as the well-known AIC and BIC, or any other model selection procedures. Comparisons with classical and recent schemes are provided in different experiments involving real datasets. Related Matlab code is given.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.ME",
        "cs.CE",
        "eess.SP",
        "stat.CO"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.21403v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21403v1",
      "doi": "10.1016/j.sigpro.2024.109735"
    },
    {
      "id": "2602.21393",
      "title": "An information-based model selection criterion for data-driven model discovery",
      "authors": [
        "Michael C Chung",
        "Alen Zacharia",
        "Juan Guan"
      ],
      "abstract": "Data-driven model discovery (DDMD) algorithms are powerful tools for extracting interpretable symbolic models from data. However, identifying the model that best balances goodness-of-fit and sparsity is often a laborious process requiring user fine-tuning, is prone to overfitting, and results may significantly vary depending on model initialization and specific training procedure. Here, we present a sparse regression algorithm that automatically and adaptively generates candidate models, and uses a novel sample-length-scaling logarithmic information criterion (SLIC) to identify the best model from these candidates. We demonstrate that SLIC greatly outperforms other popular information criteria in extracting the correct model from the data of several nonlinear ordinary and partial differential equations. Then, we demonstrate SLIC's ability to discover interpretable models from experimental datasets in fluid dynamics and nanotechnology that generate new testable predictions.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "q-bio.QM"
      ],
      "primaryCategory": "q-bio.QM",
      "pdfUrl": "https://arxiv.org/pdf/2602.21393v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21393v1",
      "doi": null
    },
    {
      "id": "2602.21383",
      "title": "Evaluating time-varying treatment effects in hybrid SMART-MRT designs",
      "authors": [
        "Mengbing Li",
        "Inbal Nahum-Shani",
        "Walter Dempsey"
      ],
      "abstract": "Recently a new experimental approach, the hybrid experimental design (HED), was introduced to enable investigators to answer scientific questions about building behavioral interventions in which human-delivered and digital components are integrated and adapted on multiple timescales: slow (e.g., every few weeks) and fast (e.g., every few hours), respectively. An increasingly common HED involves the integration of the sequential, multiple assignment, randomized trial (SMART) with the micro-randomized trial (MRT), allowing investigators to answer scientific questions about potential synergistic effects of digital and human-delivered interventions. Approaches to formalize these questions in terms of causal estimands and associated data analytic methods are limited. In this paper, we formally define and assess these synergistic effects in hybrid SMART-MRTs on both proximal and distal outcomes. Practical utility is shown through the analysis of M-Bridge, a hybrid SMART-MRT aimed at reducing binge drinking among first-year college students.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.21383v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21383v1",
      "doi": null
    },
    {
      "id": "2602.21376",
      "title": "Fenchel-Young Estimators of Perturbed Utility Models",
      "authors": [
        "Xi Lin",
        "Yafeng Yin",
        "Tianming Liu"
      ],
      "abstract": "The Perturbed Utility Model framework offers a powerful generalization of discrete choice analysis, unifying models like Multinomial Logit and Sparsemax through convex optimization. However, standard Maximum Likelihood Estimation (MLE) faces severe theoretical and numerical challenges when applied to this broader class, particularly regarding non-convexity and instability in sparse regimes. To resolve these issues, this paper introduces a unified estimation framework based on the Fenchel-Young loss. By leveraging the intrinsic convex conjugate structure of PUMs, we demonstrate that the Fenchel-Young estimator guarantees global convexity and bounded gradients, providing a mathematically natural alternative to MLE. Addressing the critical challenge of data scarcity, we further extend this framework via Wasserstein Distributionally Robust Optimization. We first derive an exact finite-dimensional reformulation of the infinite-dimensional primal problem, establishing its theoretical convexity. However, recognizing that the resulting worst-case constraints involve computationally intractable inner maximizations, we subsequently construct a tractable safe approximation by exploiting the global Lipschitz continuity of the Fenchel-Young loss. Through this tractable formulation, we uncover a rigorous geometric unification: two canonical regularization techniques, standard L2-regularization and the margin-enforcing Hinge loss, emerge mathematically as specific limiting cases of our distributionally robust estimator. Extensive experiments on synthetic data and the Swissmetro benchmark validate that the proposed framework significantly outperforms traditional methods, recovering stable preferences even under severe data limitations.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "math.OC",
        "stat.ME"
      ],
      "primaryCategory": "math.OC",
      "pdfUrl": "https://arxiv.org/pdf/2602.21376v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21376v1",
      "doi": null
    },
    {
      "id": "2602.21370",
      "title": "Evaluation of Minimal Residual Disease as a Surrogate for Progression-Free Survival in Hematology Oncology Trials: A Meta-Analytic Review",
      "authors": [
        "Jane She",
        "Xiaofei Chen",
        "Malini Iyengar",
        "Judy Li"
      ],
      "abstract": "Traditional health authority approval for oncology drugs is based on a clinical benefit endpoint, or a valid surrogate. In 1992 the FDA created the Accelerated Approval pathway to allow for earlier approval of therapies in serious conditions with an unmet medical need. This is accomplished typically by granting accelerated approval based on a surrogate endpoint that can be measured earlier than a traditional approval endpoint. Minimal residual disease (MRD) is a sensitive measure of residual cancer cells in hematology oncology after treatment, and is increasingly considered as a secondary or exploratory endpoint due to its prognostic potential for traditional clinical trial endpoints such as progression-free survival (PFS) and overall survival (OS). This work aims to evaluate MRD's surrogacy potential across several hematologic cancer indications while keeping the focus on follicular lymphoma (FL), using data from published studies. We examine individual-level and trial-level correlations extracted from previously published studies to elucidate the potential role of MRD in accelerating the drug approval process in hematology oncology trials.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.AP"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.21370v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21370v1",
      "doi": null
    },
    {
      "id": "2602.21359",
      "title": "Some Asymptotic Results on Multiple Testing under Weak Dependence",
      "authors": [
        "Swarnadeep Datta",
        "Monitirtha Dey"
      ],
      "abstract": "This paper studies the means-testing problem under weakly correlated Normal setups. Although quite common in genomic applications, test procedures having exact FWER control under such dependence structures are nonexistent. We explore the asymptotic behaviors of the classical Bonferroni (when adjusted suitably) and the Sidak procedure; and show that both of these control FWER at the desired level exactly as the number of hypotheses approaches infinity. We derive analogous limiting results on the generalized family-wise error rate and power. Simulation studies depict the asymptotic exactness of the procedures empirically.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "math.ST",
        "stat.ME"
      ],
      "primaryCategory": "math.ST",
      "pdfUrl": "https://arxiv.org/pdf/2602.21359v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21359v1",
      "doi": null
    },
    {
      "id": "2602.21314",
      "title": "Discussion of \"Matrix Completion When Missing Is Not at Random and Its Applications in Causal Panel Data Models\"",
      "authors": [
        "Eli Ben-Michael",
        "Avi Feller"
      ],
      "abstract": "Choi and Yuan (2025) propose a novel approach to applying matrix completion to the problem of estimating causal effects in panel data. The key insight is that even in the presence of structured patterns of missing data -- i.e. selection into treatment -- matrix completion can be effective if the number of treated observations is small relative to the number of control observations. We applaud the authors for their insightful and interesting paper. We discuss this proposal from two complementary perspectives. First, we situate their proposal as an example of a \"split-apply-combine\" strategy that underlies many modern panel data estimators, including difference-in-differences and synthetic control approaches. Second, we discuss the issue of the statistical \"last mile problem\" -- the gap between theory and practice -- and offer suggestions on how to partially address it. We conclude by considering the challenges of estimating the impacts of public policies using panel data and apply the approach to a study on the effect of right to carry laws on violent crime.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.21314v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21314v1",
      "doi": null
    },
    {
      "id": "2602.21200",
      "title": "A Time-Varying and Covariate-Dependent Correlation Model for Multivariate Longitudinal Studies",
      "authors": [
        "Qingzhi Liu",
        "Gen Li",
        "Anastasia K. Yocum",
        "Melvin McInnis",
        "Brian D. Athey",
        "Veerabhadran Baladandayuthapani"
      ],
      "abstract": "In multivariate longitudinal studies, associations between outcomes often exhibit time-varying and individual level heterogeneity, motivating the modeling of correlations as an explicit function of time and covariates. However, most existing methods for correlation analysis fail to simultaneously capture the time-varying and covariate-dependent effects. We propose a Time-Varying and Covariate-Dependent (TiVAC) correlation model that jointly allows covariate effects on correlation to change flexibly and smoothly across time. TiVAC employs a bivariate Gaussian model where the covariate-dependent correlations are modeled semiparametrically using penalized splines. We develop a penalized maximum likelihood-based Newton-Raphson algorithm, and inference on time-varying effects is provided through simultaneous confidence bands. Simulation studies show that TiVAC consistently outperforms existing methods in accurately estimating correlations across a wide range of settings, including binary and continuous covariates, sparse to dense observation schedules, and across diverse correlation trajectory patterns. We apply TiVAC to a psychiatric case study of 291 bipolar I patients, modeling the time-varying correlation between depression and anxiety scores as a function of their clinical variables. Our analyses reveal significant heterogeneity associated with gender and nervous-system medication use, which varies with age, revealing the complex dynamic relationship between depression and anxiety in bipolar disorders.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.21200v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21200v1",
      "doi": null
    },
    {
      "id": "2602.21160",
      "title": "Not Just How Much, But Where: Decomposing Epistemic Uncertainty into Per-Class Contributions",
      "authors": [
        "Mame Diarra Toure",
        "David A. Stephens"
      ],
      "abstract": "In safety-critical classification, the cost of failure is often asymmetric, yet Bayesian deep learning summarises epistemic uncertainty with a single scalar, mutual information (MI), that cannot distinguish whether a model's ignorance involves a benign or safety-critical class. We decompose MI into a per-class vector $C_k(x)=σ_k^{2}/(2μ_k)$, with $μ_k{=}\\mathbb{E}[p_k]$ and $σ_k^2{=}\\mathrm{Var}[p_k]$ across posterior samples. The decomposition follows from a second-order Taylor expansion of the entropy; the $1/μ_k$ weighting corrects boundary suppression and makes $C_k$ comparable across rare and common classes. By construction $\\sum_k C_k \\approx \\mathrm{MI}$, and a companion skewness diagnostic flags inputs where the approximation degrades. After characterising the axiomatic properties of $C_k$, we validate it on three tasks: (i) selective prediction for diabetic retinopathy, where critical-class $C_k$ reduces selective risk by 34.7\\% over MI and 56.2\\% over variance baselines; (ii) out-of-distribution detection on clinical and image benchmarks, where $\\sum_k C_k$ achieves the highest AUROC and the per-class view exposes asymmetric shifts invisible to MI; and (iii) a controlled label-noise study in which $\\sum_k C_k$ shows less sensitivity to injected aleatoric noise than MI under end-to-end Bayesian training, while both metrics degrade under transfer learning. Across all tasks, the quality of the posterior approximation shapes uncertainty at least as strongly as the choice of metric, suggesting that how uncertainty is propagated through the network matters as much as how it is measured.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.AP",
        "stat.ME"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2602.21160v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21160v1",
      "doi": null
    },
    {
      "id": "2602.21132",
      "title": "Robust and Sparse Generalized Linear Models for High-Dimensional Data via Maximum Mean Discrepancy",
      "authors": [
        "Xiaoning Kang",
        "Lulu Kang"
      ],
      "abstract": "High-dimensional datasets are frequently subject to contamination by outliers and heavy-tailed noise, which can severely bias standard regularized estimators like the Lasso. While Maximum Mean Discrepancy (MMD) has recently been introduced as a \"universal\" framework for robust regression, its application to high-dimensional Generalized Linear Models (GLMs) remains largely unexplored, particularly regarding variable selection. In this paper, we propose a penalized MMD framework for robust estimation and feature selection in GLMs. We introduce an $\\ell_1$-penalized MMD objective and develop two versions of the estimator: a full $O(n^2)$ version and a computationally efficient $O(n)$ approximation. To solve the resulting non-convex optimization problem, we employ an algorithm based on the Alternating Direction Method of Multipliers (ADMM) combined with AdaGrad. Through extensive simulation studies involving Gaussian linear regression and binary logistic regression, we demonstrate that our proposed methods significantly outperform classical penalized GLMs and existing robust benchmarks. Our approach shows particular strength in handling high-leverage points and heavy-tailed error distributions, where traditional methods often fail.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.21132v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21132v1",
      "doi": null
    },
    {
      "id": "2602.21068",
      "title": "Detecting Where Effects Occur by Testing Hypotheses in Order",
      "authors": [
        "Jake Bowers",
        "David Kim",
        "Nuole Chen"
      ],
      "abstract": "Experimental evaluations of public policies often randomize a new intervention within many sites or blocks. After a report of an overall result -- statistically significant or not -- the natural question from a policy maker is: \\emph{where} did any effects occur? Standard adjustments for multiple testing provide little power to answer this question. In simulations modeled after a 44-block education trial, the Hommel adjustment -- among the most powerful procedures controlling the family-wise error rate (FWER) -- detects effects in only 11\\% of truly non-null blocks. We develop a procedure that tests hypotheses top-down through a tree: test the overall null at the root, then groups of blocks, then individual blocks, stopping any branch where the null is not rejected. In the same 44-block design, this approach detects effects in 44\\% of non-null blocks -- roughly four times the detection rate. A stopping rule and valid tests at each node suffice for weak FWER control. We show that the strong-sense FWER depends on how rejection probabilities accumulate along paths through the tree. This yields a diagnostic: when power decays fast enough relative to branching, no adjustment is needed; otherwise, an adaptive $α$-adjustment restores control. We apply the method to 25 MDRC education trials and provide an R package, \\texttt{manytestsr}.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.AP"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.21068v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21068v1",
      "doi": null
    },
    {
      "id": "2602.21036",
      "title": "Empirically Calibrated Conditional Independence Tests",
      "authors": [
        "Milleno Pan",
        "Antoine de Mathelin",
        "Wesley Tansey"
      ],
      "abstract": "Conditional independence tests (CIT) are widely used for causal discovery and feature selection. Even with false discovery rate (FDR) control procedures, they often fail to provide frequentist guarantees in practice. We highlight two common failure modes: (i) in small samples, asymptotic guarantees for many CITs can be inaccurate and even correctly specified models fail to estimate the noise levels and control the error, and (ii) when sample sizes are large but models are misspecified, unaccounted dependencies skew the test's behavior and fail to return uniform p-values under the null. We propose Empirically Calibrated Conditional Independence Tests (ECCIT), a method that measures and corrects for miscalibration. For a chosen base CIT (e.g., GCM, HRT), ECCIT optimizes an adversary that selects features and response functions to maximize a miscalibration metric. ECCIT then fits a monotone calibration map that adjusts the base-test p-values in proportion to the observed miscalibration. Across empirical benchmarks on synthetic and real data, ECCIT achieves valid FDR with higher power than existing calibration strategies while remaining test agnostic.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.ME",
        "cs.LG",
        "stat.ML"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.21036v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21036v1",
      "doi": null
    },
    {
      "id": "2602.21031",
      "title": "Exchangeable Gaussian Processes for Staggered-Adoption Policy Evaluation",
      "authors": [
        "Hayk Gevorgyan",
        "Konstantinos Kalogeropoulos",
        "Angelos Alexopoulos"
      ],
      "abstract": "We study the use of exchangeable multi-task Gaussian processes (GPs) for causal inference in panel data, applying the framework to two settings: one with a single treated unit subject to a once-and-for-all treatment and another with multiple treated units and staggered treatment adoption. Our approach models the joint evolution of outcomes for treated and control units through a GP prior that ensures exchangeability across units while allowing for flexible nonlinear trends over time. The resulting posterior predictive distribution for the untreated potential outcomes of the treated unit provides a counterfactual path, from which we derive pointwise and cumulative treatment effects, along with credible intervals to quantify uncertainty. We implement several variations of the exchangeable GP model using different kernel functions. To assess prediction accuracy, we conduct a placebo-style validation within the pre-intervention window by selecting a ``fake'' intervention date. Ultimately, this study illustrates how exchangeable GPs serve as a flexible tool for policy evaluation in panel data settings and proposes a novel approach to staggered-adoption designs with a large number of treated and control units.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.ME",
        "stat.ML"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.21031v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21031v1",
      "doi": null
    },
    {
      "id": "2602.21029",
      "title": "On the non-uniformity of the 2026 FIFA World Cup draw",
      "authors": [
        "László Csató",
        "Martin Becker",
        "Karel Devriesere",
        "Dries Goossens"
      ],
      "abstract": "The group stage of a sports tournament is often made more appealing by introducing additional constraints in the group draw that promote an attractive and balanced group composition. For example, the number of intra-regional group matches is minimised in several World Cups. However, under such constraints, the traditional draw procedure may become non-uniform, meaning that the feasible allocations of the teams into groups are not equally likely to occur. Our paper quantifies this non-uniformity of the 2026 FIFA World Cup draw for the official draw procedure, as well as for 47 reasonable alternatives implied by all permutations of the four pots and two group labelling policies. We show why simulating with a recursive backtracking algorithm is intractable, and propose a workable implementation using integer programming. The official draw mechanism is found to be optimal based on four measures of non-uniformity. Nonetheless, non-uniformity can be more than halved if the organiser aims to treat the best teams drawn from the first pot equally.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.AP",
        "math.OC",
        "physics.soc-ph"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.21029v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21029v1",
      "doi": null
    },
    {
      "id": "2602.20965",
      "title": "Estimating the Partially Linear Zero-Inflated Poisson Regression Model: a Robust Approach Using a EM-like Algorithm",
      "authors": [
        "María José Llop",
        "Andrea Bergesio",
        "Anne-Françoise Yao"
      ],
      "abstract": "Count data with an excessive number of zeros frequently arise in fields such as economics, medicine, and public health. Traditional count models often fail to adequately handle such data, especially when the relationship between the response and some predictors is nonlinear. To overcome these limitations, the partially linear zero-inflated Poisson (PLZIP) model has been proposed as a flexible alternative. However, all existing estimation approaches for this model are based on likelihood, which is known to be highly sensitive to outliers and slight deviations from the model assumptions. This article presents the first robust estimation method specifically developed for the PLZIP model. An Expectation-Maximization-like algorithm is used to take advantage of the mixture nature of the model and to address extreme observations in both the response and the covariates. Results of the algorithm convergence and the consistency of the estimators are proved. A simulation study under various contamination schemes showed the robustness and efficiency of the proposed estimators in finite samples, compared to classical estimators. Finally, the application of the methodology is illustrated through an example using real data.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.20965v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20965v1",
      "doi": null
    },
    {
      "id": "2602.20939",
      "title": "A Statistical Framework for Detecting Emergent Narratives in Longitudinal Text Corpora",
      "authors": [
        "Cynthia Medeiros",
        "John Quigley",
        "Matthew Revie"
      ],
      "abstract": "Narratives about economic events and policies are widely recognised as influential drivers of economic and business behaviour. Yet the statistical identification of narrative emergence remains underdeveloped. Narratives evolve gradually, exhibit subtle shifts in content, and may exert influence disproportionate to their observable frequency, making it difficult to determine when observed changes reflect genuine structural shifts rather than routine variation in language use. We propose a statistical framework for detecting narrative emergence in longitudinal text corpora using Latent Dirichlet Allocation (LDA). We define emergence as a sustained increase in a topic's relative prominence over time and articulate a statistical framework for interpreting such trajectories, recognising that topic proportions are latent, model-estimated quantities. We illustrate the approach using a corpus of academic publications in economics spanning 1970-2018, where Nobel Prize-recognised contributions serve as externally observable signals of influential narratives. Topics associated with these contributions display sustained increases in estimated prevalence that coincide with periods of heightened citation activity and broader disciplinary recognition. These findings indicate that model-based topic trajectories can reflect identifiable shifts in economic discourse and provide a statistically grounded basis for analysing thematic change in longitudinal textual data.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.20939v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20939v1",
      "doi": null
    },
    {
      "id": "2602.20912",
      "title": "A Corrected Welch Satterthwaite Equation. And: What You Always Wanted to Know About Kish's Effective Sample but Were Afraid to Ask",
      "authors": [
        "Matthias von Davier"
      ],
      "abstract": "This article presents a corrected version of the Satterthwaite (1941, 1946) approximation for the degrees of freedom of a weighted sum of independent variance components. The original formula is known to yield biased estimates when component degrees of freedom are small. The correction, derived from exact moment matching, adjusts for the bias by incorporating a factor that accounts for the estimation of fourth moments. We show that Kish's (1965) effective sample size formula emerges as a special case when all variance components are equal, and component degrees of freedom are ignored. Simulation studies demonstrate that the corrected estimator closely matches the expected degrees of freedom even for small component sizes, while the original Satterthwaite estimator exhibits substantial downward bias. Additional applications are discussed, including jackknife variance estimation, multiple imputation total variance, and the Welch test for unequal variances.",
      "published": "2026-02-24",
      "updated": "2026-02-25",
      "categories": [
        "stat.AP"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.20912v2",
      "arxivUrl": "http://arxiv.org/abs/2602.20912v2",
      "doi": null
    },
    {
      "id": "2602.20896",
      "title": "On Stein's test of uniformity on the hypersphere",
      "authors": [
        "Paul Axmann",
        "Bruno Ebner",
        "Eduardo García-Portugués"
      ],
      "abstract": "We propose a new test of uniformity on the hypersphere based on a Stein characterization associated with the Laplace--Beltrami operator. We identify a sufficient class of test functions for this characterization, linked to the moment generating function. Exploiting the operator's eigenfunctions to obtain a harmonic decomposition in terms of Gegenbauer polynomials, we show that the proposed procedure belongs to the class of Sobolev tests. We derive closed-form expressions for the distribution of the test statistic under the null hypothesis and under fixed alternatives. To enhance power against a range of alternatives, we introduce a tuning parameter into the characterization and study its impact on rejection probabilities. We discuss data-driven strategies for selecting this parameter to maximize rejection rates for a given alternative and compare the resulting performance with that of related parametric tests. Additional numerical experiments compare the proposed test with competing Sobolev-class procedures, highlighting settings in which it offers clear advantages.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "math.ST",
        "stat.ME"
      ],
      "primaryCategory": "math.ST",
      "pdfUrl": "https://arxiv.org/pdf/2602.20896v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20896v1",
      "doi": null
    },
    {
      "id": "2602.20885",
      "title": "Combining Information Across Diverse Sources: The II-CC-FF Paradigm",
      "authors": [
        "Céline Cunen",
        "Nils Lid Hjort"
      ],
      "abstract": "We introduce and develop a general paradigm for combining information across diverse data sources. In broad terms, suppose $φ$ is a parameter of interest, built up via components $ψ_1,\\ldots,ψ_k$ from data sources $1,\\ldots,k$. The proposed scheme has three steps. First, the Independent Inspection (II) step amounts to investigating each separate data source, translating statistical information to a confidence distribution $C_j(ψ_j)$ for the relevant focus parameter $ψ_j$ associated with data source $j$. Second, Confidence Conversion (CC) techniques are used to translate the confidence distributions to confidence log-likelihood functions, say $\\ell_{{\\rm con},j}(ψ_j)$. Finally, the Focused Fusion (FF) step uses relevant and context-driven techniques to construct a confidence distribution for the primary focus parameter $φ=φ(ψ_1,\\ldots,ψ_k)$, acting on the combined confidence log-likelihood. In traditional setups, the II-CC-FF strategy amounts to versions of meta-analysis, and turns out to be competitive against state-of-the-art methods. Its potential lies in applications to harder problems, however. Illustrations are presented, related to actual applications.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.20885v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20885v1",
      "doi": null
    },
    {
      "id": "2602.20875",
      "title": "Efficient Online Learning in Interacting Particle Systems",
      "authors": [
        "Louis Sharrock",
        "Nikolas Kantas",
        "Grigorios A. Pavliotis"
      ],
      "abstract": "We introduce a new method for online parameter estimation in stochastic interacting particle systems, based on continuous observation of a small number of particles from the system. Our method recursively updates the model parameters using a stochastic approximation of the gradient of the asymptotic log likelihood, which is computed using the continuous stream of observations. Under suitable assumptions, we rigorously establish convergence of our method to the stationary points of the asymptotic log-likelihood of the interacting particle system. We consider asymptotics both in the limit as the time horizon $t\\rightarrow\\infty$, for a fixed and finite number of particles, and in the joint limit as the number of particles $N\\rightarrow\\infty$ and the time horizon $t\\rightarrow\\infty$. Under additional assumptions on the asymptotic log-likelihood, we also establish an $\\mathrm{L}^2$ convergence rate and a central limit theorem. Finally, we present several numerical examples of practical interest, including a model for systemic risk, a model of interacting FitzHugh--Nagumo neurons, and a Cucker--Smale flocking model. Our numerical results corroborate our theoretical results, and also suggest that our estimator is effective even in cases where the assumptions required for our theoretical analysis do not hold.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "math.ST",
        "math.OC",
        "math.PR",
        "stat.ME",
        "stat.ML"
      ],
      "primaryCategory": "math.ST",
      "pdfUrl": "https://arxiv.org/pdf/2602.20875v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20875v1",
      "doi": null
    },
    {
      "id": "2602.20856",
      "title": "Stochastic Discount Factors with Cross-Asset Spillovers",
      "authors": [
        "Doron Avramov",
        "Xin He"
      ],
      "abstract": "This paper develops a unified framework that links firm-level predictive signals, cross-asset spillovers, and the stochastic discount factor (SDF). Signals and spillovers are jointly estimated by maximizing the Sharpe ratio, yielding an interpretable SDF that both ranks characteristic relevance and uncovers the direction of predictive influence across assets. Out-of-sample, the SDF consistently outperforms self-predictive and expected-return benchmarks across investment universes and market states. The inferred information network highlights large, low-turnover firms as net transmitters. The framework offers a clear, economically grounded view of the informational architecture underlying cross-sectional return dynamics.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "q-fin.CP",
        "econ.EM",
        "q-fin.PM",
        "stat.ML"
      ],
      "primaryCategory": "q-fin.CP",
      "pdfUrl": "https://arxiv.org/pdf/2602.20856v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20856v1",
      "doi": null
    },
    {
      "id": "2602.20844",
      "title": "Maximum entropy based testing in network models: ERGMs and constrained optimization",
      "authors": [
        "Subhrosekhar Ghosh",
        "Rathindra Nath Karmakar",
        "Samriddha Lahiry"
      ],
      "abstract": "Stochastic network models play a central role across a wide range of scientific disciplines, and questions of statistical inference arise naturally in this context. In this paper we investigate goodness-of-fit and two-sample testing procedures for statistical networks based on the principle of maximum entropy (MaxEnt). Our approach formulates a constrained entropy-maximization problem on the space of networks, subject to prescribed structural constraints. The resulting test statistics are defined through the Lagrange multipliers associated with the constrained optimization problem, which, to our knowledge, is novel in the statistical networks literature. We establish consistency in the classical regime where the number of vertices is fixed. We then consider asymptotic regimes in which the graph size grows with the sample size, developing tests for both dense and sparse settings. In the dense case, we analyze exponential random graph models (ERGM) (including the Erdös-Rènyi models), while in the sparse regime our theory applies to Erd{ö}s-R{è}nyi graphs. Our analysis leverages recent advances in nonlinear large deviation theory for random graphs. We further show that the proposed Lagrange-multiplier framework connects naturally to classical score tests for constrained maximum likelihood estimation. The results provide a unified entropy-based framework for network model assessment across diverse growth regimes.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "math.ST",
        "cs.IT",
        "math.PR",
        "stat.ME",
        "stat.ML"
      ],
      "primaryCategory": "math.ST",
      "pdfUrl": "https://arxiv.org/pdf/2602.20844v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20844v1",
      "doi": null
    },
    {
      "id": "2602.20834",
      "title": "Confidence Distributions and Related Themes",
      "authors": [
        "Nils Lid Hjort",
        "Tore Schweder"
      ],
      "abstract": "This is the guest editors' general introduction to a Special Issue of the Journal of Statistical Planning and Inference, dedicated to confidence distributions and related themes. Confidence distributions (CDs) are distributions for parameters of interest, constructed via a statistical model after analysing the data. As such they serve the same purpose for the frequentist statisticians as the posterior distributions for the Bayesians. There have been several attempts in the literature to put up a clear theory for such confidence distributions, from Fisher's fiducial inference and onwards. There are certain obstacles and difficulties involved in these attempts, both conceptually and operationally, which have contributed to the CDs being slow in entering statistical mainstream. Recently there is a renewed surge of interest in CDs and various related themes, however, reflected in both series of new methodological research, advanced applications to substantive sciences, and dissemination and communication via workshops and conferences. The present special issue of the JSPI is a collection of papers emanating from the {\\it Inference With Confidence} workshop in Oslo, May 2015. Several of the papers appearing here were first presented at that workshop. The present collection includes however also new research papers from other scholars in the field.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.20834v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20834v1",
      "doi": null
    },
    {
      "id": "2602.20795",
      "title": "Hawkes Identification with a Prescribed Causal Basis: Closed-Form Estimators and Asymptotics",
      "authors": [
        "Xinhui Rong",
        "Girish N. Nair"
      ],
      "abstract": "Driven by the recent surge in neural-inspired modeling, point processes have gained significant traction in systems and control. While the Hawkes process is the standard model for characterizing random event sequences with memory, identifying its unknown kernels is often hindered by nonlinearity. Approaches using prescribed basis kernels have emerged to enable linear parameterization, yet they typically rely on iterative likelihood methods and lack rigorous analysis under model misspecification. This paper justifies a closed-form Least Squares identification framework for Hawkes processes with prescribed kernels. We guarantee estimator existence via the almost-sure positive definiteness of the empirical Gram matrix and prove convergence to the true parameters under correct specification, or to well-defined pseudo-true parameters under misspecification. Furthermore, we derive explicit Central Limit Theorems for both regimes, providing a complete and interpretable asymptotic theory. We demonstrate these theoretical findings through comparative numerical simulations.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.ME",
        "eess.SY"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.20795v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20795v1",
      "doi": null
    },
    {
      "id": "2602.20651",
      "title": "Sparse Bayesian Deep Functional Learning with Structured Region Selection",
      "authors": [
        "Xiaoxian Zhu",
        "Yingmeng Li",
        "Shuangge Ma",
        "Mengyun Wu"
      ],
      "abstract": "In modern applications such as ECG monitoring, neuroimaging, wearable sensing, and industrial equipment diagnostics, complex and continuously structured data are ubiquitous, presenting both challenges and opportunities for functional data analysis. However, existing methods face a critical trade-off: conventional functional models are limited by linearity, whereas deep learning approaches lack interpretable region selection for sparse effects. To bridge these gaps, we propose a sparse Bayesian functional deep neural network (sBayFDNN). It learns adaptive functional embeddings through a deep Bayesian architecture to capture complex nonlinear relationships, while a structured prior enables interpretable, region-wise selection of influential domains with quantified uncertainty. Theoretically, we establish rigorous approximation error bounds, posterior consistency, and region selection consistency. These results provide the first theoretical guarantees for a Bayesian deep functional model, ensuring its reliability and statistical rigor. Empirically, comprehensive simulations and real-world studies confirm the effectiveness and superiority of sBayFDNN. Crucially, sBayFDNN excels in recognizing intricate dependencies for accurate predictions and more precisely identifies functionally meaningful regions, capabilities fundamentally beyond existing approaches.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "cs.LG",
        "stat.AP",
        "stat.ML"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2602.20651v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20651v1",
      "doi": null
    },
    {
      "id": "2602.20611",
      "title": "Amortized Bayesian inference for actigraph time sheet data from mobile devices",
      "authors": [
        "Daniel Zhou",
        "Sudipto Banerjee"
      ],
      "abstract": "Mobile data technologies use ``actigraphs'' to furnish information on health variables as a function of a subject's movement. The advent of wearable devices and related technologies has propelled the creation of health databases consisting of human movement data to conduct research on mobility patterns and health outcomes. Statistical methods for analyzing high-resolution actigraph data depend on the specific inferential context, but the advent of Artificial Intelligence (AI) frameworks require that the methods be congruent to transfer learning and amortization. This article devises amortized Bayesian inference for actigraph time sheets. We pursue a Bayesian approach to ensure full propagation of uncertainty and its quantification using a hierarchical dynamic linear model. We build our analysis around actigraph data from the Physical Activity through Sustainable Transport Approaches in Los Angeles (PASTA-LA) study conducted by the Fielding School of Public Health in the University of California, Los Angeles. Apart from achieving probabilistic imputation of actigraph time sheets, we are also able to statistically learn about the time-varying impact of explanatory variables on the magnitude of acceleration (MAG) for a cohort of subjects.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2602.20611v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20611v1",
      "doi": null
    },
    {
      "id": "2602.20581",
      "title": "Using Prior Studies to Design Experiments: An Empirical Bayes Approach",
      "authors": [
        "Zhiheng You"
      ],
      "abstract": "We develop an empirical Bayes framework for experimental design that leverages information from prior related studies. When a researcher has access to estimates from previous studies on similar parameters, they can use empirical Bayes to estimate an informative prior over the parameter of interest in the new study. We show how this prior can be incorporated into a decision-theoretic experimental design framework to choose optimal design. The approach is illustrated via propensity score designs in stratified randomized experiments. Our theoretical results show that the empirical Bayes design achieves oracle-optimal performance as the number of prior studies grows, and characterize the rate at which regret vanishes. To illustrate the approach, we present two empirical applications--oncology drug trials and the Tennessee Project STAR experiment. Our framework connects the Bayesian meta-analysis literature to experimental design and provides practical guidance for researchers seeking to design more efficient experiments.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "econ.EM"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.20581v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20581v1",
      "doi": null
    },
    {
      "id": "2602.20572",
      "title": "Local Fréchet regression with toroidal predictors",
      "authors": [
        "Chang Jun Im",
        "Jeong Min Jeon"
      ],
      "abstract": "We provide the first regression framework that simultaneously accommodates responses taking values in a general metric space and predictors lying on a general torus. We propose intrinsic local constant and local linear estimators that respect the underlying geometries of both the response and predictor spaces. Our local linear estimator is novel even in the case of scalar responses. We further establish their asymptotic properties, including consistency and convergence rates. Simulation studies, together with an application to real data, illustrate the superior performance of the proposed methodology.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.20572v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20572v1",
      "doi": null
    },
    {
      "id": "2602.20549",
      "title": "Sample-efficient evidence estimation of score based priors for model selection",
      "authors": [
        "Frederic Wang",
        "Katherine L. Bouman"
      ],
      "abstract": "The choice of prior is central to solving ill-posed imaging inverse problems, making it essential to select one consistent with the measurements $y$ to avoid severe bias. In Bayesian inverse problems, this could be achieved by evaluating the model evidence $p(y \\mid M)$ under different models $M$ that specify the prior and then selecting the one with the highest value. Diffusion models are the state-of-the-art approach to solving inverse problems with a data-driven prior; however, directly computing the model evidence with respect to a diffusion prior is intractable. Furthermore, most existing model evidence estimators require either many pointwise evaluations of the unnormalized prior density or an accurate clean prior score. We propose \\method, an estimator of the model evidence of a diffusion prior by integrating over the time-marginals of posterior sampling methods. Our method leverages the large amount of intermediate samples naturally obtained during the reverse diffusion sampling process to obtain an accurate estimation of the model evidence using only a handful of posterior samples (e.g., 20). We also demonstrate how to implement our estimator in tandem with recent diffusion posterior sampling methods. Empirically, our estimator matches the model evidence when it can be computed analytically, and it is able to both select the correct diffusion model prior and diagnose prior misfit under different highly ill-conditioned, non-linear inverse problems, including a real-world black hole imaging problem.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "cs.LG",
        "cs.CV",
        "stat.ME"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2602.20549v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20549v1",
      "doi": null
    },
    {
      "id": "2602.20503",
      "title": "Error-Controlled Borrowing from External Data Using Wasserstein Ambiguity Sets",
      "authors": [
        "Yui Kimura",
        "Shu Tamano"
      ],
      "abstract": "Incorporating external data can improve the efficiency of clinical trials, but distributional mismatches between current and external populations threaten the validity of inference. While numerous dynamic borrowing methods exist, the calibration of their borrowing parameters relies mainly on ad hoc, simulation-based tuning. To overcome this, we propose BOND (Borrowing under Optimal Nonparametric Distributional robustness), a framework that formalizes data noncommensurability through Wasserstein ambiguity sets centered at the current-trial distribution. By deriving sharp, closed-form bounds on the worst-case mean drift for both continuous and binary outcomes, we construct a distributionally robust, bias-corrected Wald statistic that ensures asymptotic type I error control uniformly over the ambiguity set. Importantly, BOND determines the optimal borrowing strength by maximizing a worst-case power proxy, converting heuristic parameter tuning into a transparent, analytically tractable optimization problem. Furthermore, we demonstrate that many prominent borrowing methods can be reparameterized via an effective borrowing weight, rendering our calibration framework broadly applicable. Simulation studies and a real-world clinical trial application confirm that BOND preserves the nominal size under unmeasured heterogeneity while achieving efficiency gains over standard borrowing methods.",
      "published": "2026-02-24",
      "updated": "2026-02-25",
      "categories": [
        "stat.ME",
        "stat.AP"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.20503v2",
      "arxivUrl": "http://arxiv.org/abs/2602.20503v2",
      "doi": null
    }
  ],
  "metadata": {
    "lastUpdated": "2026-02-26T02:46:27.161Z",
    "totalPapers": 50,
    "categories": [
      "stat.ME",
      "stat.AP",
      "econ.EM",
      "q-bio.QM"
    ],
    "queryDate": "2026-02-26"
  }
}