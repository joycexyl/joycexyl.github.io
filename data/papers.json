{
  "papers": [
    {
      "id": "2602.15809",
      "title": "Decision Quality Evaluation Framework at Pinterest",
      "authors": [
        "Yuqi Tian",
        "Robert Paine",
        "Attila Dobi",
        "Kevin O'Sullivan",
        "Aravindh Manickavasagam",
        "Faisal Farooq"
      ],
      "abstract": "Online platforms require robust systems to enforce content safety policies at scale. A critical component of these systems is the ability to evaluate the quality of moderation decisions made by both human agents and Large Language Models (LLMs). However, this evaluation is challenging due to the inherent trade-offs between cost, scale, and trustworthiness, along with the complexity of evolving policies. To address this, we present a comprehensive Decision Quality Evaluation Framework developed and deployed at Pinterest. The framework is centered on a high-trust Golden Set (GDS) curated by subject matter experts (SMEs), which serves as a ground truth benchmark. We introduce an automated intelligent sampling pipeline that uses propensity scores to efficiently expand dataset coverage. We demonstrate the framework's practical application in several key areas: benchmarking the cost-performance trade-offs of various LLM agents, establishing a rigorous methodology for data-driven prompt optimization, managing complex policy evolution, and ensuring the integrity of policy content prevalence metrics via continuous validation. The framework enables a shift from subjective assessments to a data-driven and quantitative practice for managing content safety systems.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "categories": [
        "stat.AP",
        "cs.AI"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.15809v1",
      "arxivUrl": "http://arxiv.org/abs/2602.15809v1",
      "doi": null
    },
    {
      "id": "2602.15740",
      "title": "MRC-GAT: A Meta-Relational Copula-Based Graph Attention Network for Interpretable Multimodal Alzheimer's Disease Diagnosis",
      "authors": [
        "Fatemeh Khalvandi",
        "Saadat Izadi",
        "Abdolah Chalechale"
      ],
      "abstract": "Alzheimer's disease (AD) is a progressive neurodegenerative condition necessitating early and precise diagnosis to provide prompt clinical management. Given the paramount importance of early diagnosis, recent studies have increasingly focused on computer-aided diagnostic models to enhance precision and reliability. However, most graph-based approaches still rely on fixed structural designs, which restrict their flexibility and limit generalization across heterogeneous patient data. To overcome these limitations, the Meta-Relational Copula-Based Graph Attention Network (MRC-GAT) is proposed as an efficient multimodal model for AD classification tasks. The proposed architecture, copula-based similarity alignment, relational attention, and node fusion are integrated as the core components of episodic meta-learning, such that the multimodal features, including risk factors (RF), Cognitive test scores, and MRI attributes, are first aligned via a copula-based transformation in a common statistical space and then combined by a multi-relational attention mechanism. According to evaluations performed on the TADPOLE and NACC datasets, the MRC-GAT model achieved accuracies of 96.87% and 92.31%, respectively, demonstrating state-of-the-art performance compared to existing diagnostic models. Finally, the proposed model confirms the robustness and applicability of the proposed method by providing interpretability at various stages of disease diagnosis.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2602.15740v1",
      "arxivUrl": "http://arxiv.org/abs/2602.15740v1",
      "doi": null
    },
    {
      "id": "2602.15731",
      "title": "Generalised Exponential Kernels for Nonparametric Density Estimation",
      "authors": [
        "Laura M. Craig",
        "Wagner Barreto-Souza"
      ],
      "abstract": "This paper introduces a novel kernel density estimator (KDE) based on the generalised exponential (GE) distribution, designed specifically for positive continuous data. The proposed GE KDE offers a mathematically tractable form that avoids the use of special functions, for instance, distinguishing it from the widely used gamma KDE, which relies on the gamma function. Despite its simpler form, the GE KDE maintains similar flexibility and shape characteristics, aligning with distributions such as the gamma, which are known for their effectiveness in modelling positive data. We derive the asymptotic bias and variance of the proposed kernel density estimator, and formally demonstrate the order of magnitude of the remaining terms in these expressions. We also propose a second GE KDE, for which we are able to show that it achieves the optimal mean integrated squared error, something that is difficult to establish for the former. Through numerical experiments involving simulated and real data sets, we show that GE KDEs can be an important alternative and competitive to existing KDEs.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.15731v1",
      "arxivUrl": "http://arxiv.org/abs/2602.15731v1",
      "doi": null
    },
    {
      "id": "2602.15730",
      "title": "Causal Effect Estimation with Latent Textual Treatments",
      "authors": [
        "Omri Feldman",
        "Amar Venugopal",
        "Jann Spiess",
        "Amir Feder"
      ],
      "abstract": "Understanding the causal effects of text on downstream outcomes is a central task in many applications. Estimating such effects requires researchers to run controlled experiments that systematically vary textual features. While large language models (LLMs) hold promise for generating text, producing and evaluating controlled variation requires more careful attention. In this paper, we present an end-to-end pipeline for the generation and causal estimation of latent textual interventions. Our work first performs hypothesis generation and steering via sparse autoencoders (SAEs), followed by robust causal estimation. Our pipeline addresses both computational and statistical challenges in text-as-treatment experiments. We demonstrate that naive estimation of causal effects suffers from significant bias as text inherently conflates treatment and covariate information. We describe the estimation bias induced in this setting and propose a solution based on covariate residualization. Our empirical results show that our pipeline effectively induces variation in target features and mitigates estimation error, providing a robust foundation for causal effect estimation in text-as-treatment settings.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "categories": [
        "cs.CL",
        "econ.EM"
      ],
      "primaryCategory": "cs.CL",
      "pdfUrl": "https://arxiv.org/pdf/2602.15730v1",
      "arxivUrl": "http://arxiv.org/abs/2602.15730v1",
      "doi": null
    },
    {
      "id": "2602.15697",
      "title": "Reproducibility and Statistical Methodology",
      "authors": [
        "Anthony Almudevar",
        "Jacob Almudevar"
      ],
      "abstract": "In 2015 the Open Science Collaboration (OSC) (Nosek et al 2015) published a highly influential paper which claimed that a large fraction of published results in the psychological sciences were not reproducible. In this article we review this claim from several points of view. We first offer an extended analysis of the methods used in that study. We show that the OSC methodology induces a bias that is able by itself to explain the discrepancy between the OSC estimates of reproducibility and other more optimistic estimates made by similar studies. The article also offers a more general literature review and discussion of reproducibility in experimental science. We argue, for both scientific and ethical reasons, that a considered balance of false positive and false negative rates is preferable to a single-minded concentration on false positive rates alone.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "categories": [
        "stat.AP"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.15697v1",
      "arxivUrl": "http://arxiv.org/abs/2602.15697v1",
      "doi": null
    },
    {
      "id": "2602.15690",
      "title": "Income Inequality and Economic Growth: A Meta-Analytic Approach",
      "authors": [
        "Lisa Cpretti",
        "Lorenzo Tonni"
      ],
      "abstract": "The empirical literature on the relationship between income inequality and economic growth has produced highly heterogeneous and often conflicting results. This paper investigates the sources of this heterogeneity using a meta-analytic approach that systematically combines and analyzes evidence from relevant studies published between 1994 and 2025. We find an economically small but statistically significant negative average effect of income inequality on subsequent economic growth, together with strong evidence of substantial heterogeneity and selective publication based on statistical significance, but no evidence of systematic directional bias. To explain the observed heterogeneity, we estimate a meta-regression. The results indicate that both real-world characteristics and research design choices shape reported effect sizes. In particular, inequality measured net of taxes and transfers is associated with more negative growth effects, and the adverse impact of inequality is weaker - or even reversed - in high-income economies relative to developing countries. Methodological choices also matter: cross-sectional studies tend to report more negative estimates, while fixed-effects, instrumental-variable, and GMM estimators are associated with more positive estimates in panel settings.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "categories": [
        "econ.EM"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.15690v1",
      "arxivUrl": "http://arxiv.org/abs/2602.15690v1",
      "doi": null
    },
    {
      "id": "2602.15679",
      "title": "Safe hypotheses testing with application to order restricted inference",
      "authors": [
        "Ori Davidov"
      ],
      "abstract": "Hypothesis tests under order restrictions arise in a wide range of scientific applications. By exploiting inequality constraints, such tests can achieve substantial gains in power and interpretability. However, these gains come at a cost: when the imposed constraints are misspecified, the resulting inferences may be misleading or even invalid, and Type III errors may occur, i.e., the null hypothesis may be rejected when neither the null nor the alternative is true. To address this problem, this paper introduces safe tests. Heuristically, a safe test is a testing procedure that is asymptotically free of Type III errors. The proposed test is accompanied by a certificate of validity, a pre--test that assesses whether the original hypotheses are consistent with the data, thereby ensuring that the null hypothesis is rejected only when warranted, enabling principled inference without risk of systematic error. Although the development in this paper focus on testing problems in order--restricted inference, the underlying ideas are more broadly applicable. The proposed methodology is evaluated through simulation studies and the analysis of well--known illustrative data examples, demonstrating strong protection against Type III errors while maintaining power comparable to standard procedures.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.15679v1",
      "arxivUrl": "http://arxiv.org/abs/2602.15679v1",
      "doi": null
    },
    {
      "id": "2602.15677",
      "title": "CAMEL: An ECG Language Model for Forecasting Cardiac Events",
      "authors": [
        "Neelay Velingker",
        "Alaia Solko-Breslin",
        "Mayank Keoliya",
        "Seewon Choi",
        "Jiayi Xin",
        "Anika Marathe",
        "Alireza Oraii",
        "Rajat Deo",
        "Sameed Khatana",
        "Rajeev Alur",
        "Mayur Naik",
        "Eric Wong"
      ],
      "abstract": "Electrocardiograms (ECG) are electrical recordings of the heart that are critical for diagnosing cardiovascular conditions. ECG language models (ELMs) have recently emerged as a promising framework for ECG classification accompanied by report generation. However, current models cannot forecast future cardiac events despite the immense clinical value for planning earlier intervention. To address this gap, we propose CAMEL, the first ELM that is capable of inference over longer signal durations which enables its forecasting capability. Our key insight is a specialized ECG encoder which enables cross-understanding of ECG signals with text. We train CAMEL using established LLM training procedures, combining LoRA adaptation with a curriculum learning pipeline. Our curriculum includes ECG classification, metrics calculations, and multi-turn conversations to elicit reasoning. CAMEL demonstrates strong zero-shot performance across 6 tasks and 9 datasets, including ECGForecastBench, a new benchmark that we introduce for forecasting arrhythmias. CAMEL is on par with or surpasses ELMs and fully supervised baselines both in- and out-of-distribution, achieving SOTA results on ECGBench (+7.0% absolute average gain) as well as ECGForecastBench (+12.4% over fully supervised models and +21.1% over zero-shot ELMs).",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "categories": [
        "cs.LG",
        "q-bio.QM"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2602.15677v1",
      "arxivUrl": "http://arxiv.org/abs/2602.15677v1",
      "doi": null
    },
    {
      "id": "2602.15673",
      "title": "Leicester's Tale: Another Perspective on the EPL 2015/16 Through Expected Goals (xG) Modelling",
      "authors": [
        "Sheikh Badar Ud Din Tahir",
        "Leonardo Egidi",
        "Nicola Torelli"
      ],
      "abstract": "Probabilistic modeling is an effective tool for evaluating team performance and predicting outcomes in sports. However, an important question that hasn't been fully explored is whether these models can reliably reflect actual performance while assigning meaningful probabilities to rare results that differ greatly from expectations. In this study, we create an inference-based probabilistic framework built on expected goals (xG). This framework converts shot-level event data into season-level simulations of points, rankings, and outcome probabilities. Using the English Premier League 2015/16 season as a data, we demonstrate that the framework captures the overall structure of the league table. It correctly identifies the top-four contenders and relegation candidates while explaining a significant portion of the variance in final points and ranks. In a full-season evaluation, the model assigns a low probability to extreme outcomes, particularly Leicester City's historic title win, which stands out as a statistical anomaly. We then look at the ex ante inferential and early-diagnostic role of xG by only using mid-season information. With first-half data, we simulate the rest of the season and show that teams with stronger mid-season xG profiles tend to earn more points in the second half, even after considering their current league position. In this mid-season assessment, Leicester City ranks among the top teams by xG and is given a small but noteworthy chance of winning the league. This suggests that their ultimate success was unlikely but not entirely detached from their actual performance. Our analysis indicates that expected goals models work best as probabilistic baselines for analysis and early-warning diagnostics, rather than as certain predictors of rare season outcomes.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.15673v1",
      "arxivUrl": "http://arxiv.org/abs/2602.15673v1",
      "doi": null
    },
    {
      "id": "2602.15600",
      "title": "The geometry of online conversations and the causal antecedents of conflictual discourse",
      "authors": [
        "Carlo Santagiustina",
        "Caterina Cruciani"
      ],
      "abstract": "This article investigates the causal antecedents of conflictual language and the geometry of interaction in online threaded conversations related to climate change. We employ three annotation dimensions, inferred through LLM prompting and averaging, to capture complementary aspects of discursive conflict (such as stance: agreement vs disagreement; tone: attacking vs respectful; and emotional versus factual framing) and use data from a threaded online forum to examine how these dimensions respond to temporal, conversational, and arborescent structural features of discussions. We show that, as suggested by the literature, longer delays between successive posts in a thread are associated with replies that are, on average, more respectful, whereas longer delays relative to the parent post are associated with slightly less disagreement but more emotional (less factual) language. Second, we characterize alignment with the local conversational environment and find strong convergence both toward the average stance, tone and emotional framing of older sibling posts replying to the same parent and toward those of the parent post itself, with parent post effects generally stronger than sibling effects. We further show that early branch-level responses condition these alignment dynamics, such that parent-child stance alignment is amplified or attenuated depending on whether a branch is initiated in agreement or disagreement with the discussion's root message. These influences are largely additive for civility-related dimensions (attacking vs respectful, disagree vs agree), whereas for emotional versus factual framing there is a significant interaction: alignment with the parent's emotionality is amplified when older siblings are similarly aligned.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "categories": [
        "cs.SI",
        "cs.AI",
        "econ.EM",
        "stat.AP"
      ],
      "primaryCategory": "cs.SI",
      "pdfUrl": "https://arxiv.org/pdf/2602.15600v1",
      "arxivUrl": "http://arxiv.org/abs/2602.15600v1",
      "doi": "10.13140/RG.2.2.34796.22409"
    },
    {
      "id": "2602.15568",
      "title": "Scenario Approach with Post-Design Certification of User-Specified Properties",
      "authors": [
        "Algo Carè",
        "Marco C. Campi",
        "Simone Garatti"
      ],
      "abstract": "The scenario approach is an established data-driven design framework that comes equipped with a powerful theory linking design complexity to generalization properties. In this approach, data are simultaneously used both for design and for certifying the design's reliability, without resorting to a separate test dataset. This paper takes a step further by guaranteeing additional properties, useful in post-design usage but not considered during the design phase. To this end, we introduce a two-level framework of appropriateness: baseline appropriateness, which guides the design process, and post-design appropriateness, which serves as a criterion for a posteriori evaluation. We provide distribution-free upper bounds on the risk of failing to meet the post-design appropriateness; these bounds are computable without using any additional test data. Under additional assumptions, lower bounds are also derived. As part of an effort to demonstrate the usefulness of the proposed methodology, the paper presents two practical examples in H2 and pole-placement problems. Moreover, a method is provided to infer comprehensive distributional knowledge of relevant performance indexes from the available dataset.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "categories": [
        "stat.ME",
        "cs.LG",
        "eess.SY",
        "stat.ML"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.15568v1",
      "arxivUrl": "http://arxiv.org/abs/2602.15568v1",
      "doi": null
    },
    {
      "id": "2602.15559",
      "title": "Fixed-Horizon Self-Normalized Inference for Adaptive Experiments via Martingale AIPW/DML with Logged Propensities",
      "authors": [
        "Gabriel Saco"
      ],
      "abstract": "Adaptive randomized experiments update treatment probabilities as data accrue, but still require an end-of-study interval for the average treatment effect (ATE) at a prespecified horizon. Under adaptive assignment, propensities can keep changing, so the predictable quadratic variation of AIPW/DML score increments may remain random. When no deterministic variance limit exists, Wald statistics normalized by a single long-run variance target can be conditionally miscalibrated given the realized variance regime. We assume no interference, sequential randomization, i.i.d. arrivals, and executed overlap on a prespecified scored set, and we require two auditable pipeline conditions: the platform logs the executed randomization probability for each unit, and the nuisance regressions used to score unit $t$ are constructed predictably from past data only. These conditions make the centered AIPW/DML scores an exact martingale difference sequence. Using self-normalized martingale limit theory, we show that the Studentized statistic, with variance estimated by realized quadratic variation, is asymptotically N(0,1) at the prespecified horizon, even without variance stabilization. Simulations validate the theory and highlight when standard fixed-variance Wald reporting fails.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "categories": [
        "stat.ME",
        "econ.EM",
        "math.ST",
        "stat.ML"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.15559v1",
      "arxivUrl": "http://arxiv.org/abs/2602.15559v1",
      "doi": null
    },
    {
      "id": "2602.15496",
      "title": "Confidence Distributions for FIC scores",
      "authors": [
        "Céline Cunen",
        "Nils Lid Hjort"
      ],
      "abstract": "When using the Focused Information Criterion (FIC) for assessing and ranking candidate models with respect to how well they do for a given estimation task, it is customary to produce a so-called FIC plot. This plot has the different point estimates along the y-axis and the root-FIC scores on the x-axis, these being the estimated root-mean-square scores. In this paper we address the estimation uncertainty involved in each of the points of such a FIC plot. This needs careful assessment of each of the estimators from the candidate models, taking also modelling bias into account, along with the relative precision of the associated estimated mean squared error quantities. We use confidence distributions for these endeavours. This leads to fruitful CD-FIC plots, helping the statistician to judge to what extent the seemingly best models really are better than other models, etc. These efforts also lead to two further developments. The first is a new tool for model selection, which we call the quantile FIC, which helps overcome certain difficulties associated with the usual FIC procedures, related to somewhat arbitrary schemes for handling estimated squared biases. A particular case is the median-FIC. The second development is to form model averaged estimators with fruitful weights determined by the relative sizes of the median- and quantile-FIC scores. And Mrs. Jones is pregnant.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.15496v1",
      "arxivUrl": "http://arxiv.org/abs/2602.15496v1",
      "doi": null
    },
    {
      "id": "2602.15451",
      "title": "Molecular Design beyond Training Data with Novel Extended Objective Functionals of Generative AI Models Driven by Quantum Annealing Computer",
      "authors": [
        "Hayato Kunugi",
        "Mohsen Rahmani",
        "Yosuke Iyama",
        "Yutaro Hirono",
        "Akira Suma",
        "Matthew Woolway",
        "Vladimir Vargas-Calderón",
        "William Kim",
        "Kevin Chern",
        "Mohammad Amin",
        "Masaru Tateno"
      ],
      "abstract": "Deep generative modeling to stochastically design small molecules is an emerging technology for accelerating drug discovery and development. However, one major issue in molecular generative models is their lower frequency of drug-like compounds. To resolve this problem, we developed a novel framework for optimization of deep generative models integrated with a D-Wave quantum annealing computer, where our Neural Hash Function (NHF) presented herein is used both as the regularization and binarization schemes simultaneously, of which the latter is for transformation between continuous and discrete signals of the classical and quantum neural networks, respectively, in the error evaluation (i.e., objective) function. The compounds generated via the quantum-annealing generative models exhibited higher quality in both validity and drug-likeness than those generated via the fully-classical models, and was further indicated to exceed even the training data in terms of drug-likeness features, without any restraints and conditions to deliberately induce such an optimization. These results indicated an advantage of quantum annealing to aim at a stochastic generator integrated with our novel neural network architectures, for the extended performance of feature space sampling and extraction of characteristic features in drug design.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG",
        "quant-ph"
      ],
      "primaryCategory": "q-bio.QM",
      "pdfUrl": "https://arxiv.org/pdf/2602.15451v1",
      "arxivUrl": "http://arxiv.org/abs/2602.15451v1",
      "doi": null
    },
    {
      "id": "2602.15429",
      "title": "Deep description of static and dynamic network ties in Honduran villages",
      "authors": [
        "Marios Papamichalis",
        "Nikolaos Nakis",
        "Nicholas A. Christakis"
      ],
      "abstract": "We examine static and dynamic social network structure in 176 villages within the Copan Department of Honduras across two data waves (2016, 2019), using detailed data on multiplex networks for 20,232 individuals enrolled in a longitudinal survey. These networks capture friendship, health advice, financial help, and adversarial relationships, allowing us to show how cooperation and conflict jointly shape social structure. Using node-level network measures derived from near-census sociocentric village networks, we leverage mixed-effects zero-inflated negative binomial models to assess the influence of individual attributes, such as gender, marital status, education, religion, and indigenous status, and of village characteristics, on the dynamics of social networks over time. We complement these node-level models with dyadic assortativity (odds-ratio-based homophily) and community-level measures to describe how sorting by key attributes differs across network types and between waves. Our results demonstrate significant assortativity based on gender and religion, particularly within health and financial networks. Across networks, gender and religion exhibit the most consistent assortative mixing. Additionally, community-level assortativity metrics indicate that educational and financial factors increasingly influence social ties over time. Our findings provide insights into how personal attributes and community dynamics interact to shape network formation and socio-economic relationships in rural settings over time.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "categories": [
        "stat.AP"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.15429v1",
      "arxivUrl": "http://arxiv.org/abs/2602.15429v1",
      "doi": null
    },
    {
      "id": "2602.15390",
      "title": "Space-filling lattice designs for computer experiments",
      "authors": [
        "Naoki Sakai",
        "Takashi Goda"
      ],
      "abstract": "This paper investigates the construction of space-filling designs for computer experiments. The space-filling property is characterized by the covering and separation radii of a design, which are integrated through the unified criterion of quasi-uniformity. We focus on a special class of designs, known as quasi-Monte Carlo (QMC) lattice point sets, and propose two construction algorithms. The first algorithm generates rank-1 lattice point sets as an approximation of quasi-uniform Kronecker sequences, where the generating vector is determined explicitly. As a byproduct of our analysis, we prove that this explicit point set achieves an isotropic discrepancy of $O(N^{-1/d})$. The second algorithm utilizes Korobov lattice point sets, employing the Lenstra--Lenstra--Lovász (LLL) basis reduction algorithm to identify the generating vector that ensures quasi-uniformity. Numerical experiments are provided to validate our theoretical claims regarding quasi-uniformity. Furthermore, we conduct empirical comparisons between various QMC point sets in the context of Gaussian process regression, showcasing the efficacy of the proposed designs for computer experiments.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "categories": [
        "stat.ME",
        "math.NA",
        "math.NT"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.15390v1",
      "arxivUrl": "http://arxiv.org/abs/2602.15390v1",
      "doi": null
    },
    {
      "id": "2602.15387",
      "title": "Bayesian Nonparametrics for Gene-Gene and Gene-Environment Interactions in Case-Control Studies: A Synthesis and Extension",
      "authors": [
        "Durba Bhattacharya",
        "Sourabh Bhattacharya"
      ],
      "abstract": "Gene-gene and gene-environment interactions are widely believed to play significant roles in explaining the variability of complex traits. While substantial research exists in this area, a comprehensive statistical framework that addresses multiple sources of uncertainty simultaneously remains lacking. In this article, we synthesize and propose extension of a novel class of Bayesian nonparametric approaches that account for interactions among genes, loci, and environmental factors while accommodating uncertainty about population substructure. Our contribution is threefold: (1) We provide a unified exposition of hierarchical Bayesian models driven by Dirichlet processes for genetic interactions, clarifying their conceptual advantages over traditional regression approaches; (2) We shed light on new computational strategies that combine transformation-based MCMC with parallel processing for scalable inference; and (3) We present enhanced hypothesis testing procedures for identifying disease-predisposing loci.Through applications to myocardial infarction data, we demonstrate how these methods offer biological insights not readily obtainable from standard approaches. Our synthesis highlights the advantages of Bayesian nonparametric thinking in genetic epidemiology while providing practical guidance for implementation.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "categories": [
        "stat.ME",
        "stat.AP"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.15387v1",
      "arxivUrl": "http://arxiv.org/abs/2602.15387v1",
      "doi": null
    },
    {
      "id": "2602.15385",
      "title": "From Chain-Ladder to Individual Claims Reserving",
      "authors": [
        "Ronald Richman",
        "Mario V. Wüthrich"
      ],
      "abstract": "The chain-ladder (CL) method is the most widely used claims reserving technique in non-life insurance. This manuscript introduces a novel approach to computing the CL reserves based on a fundamental restructuring of the data utilization for the CL prediction procedure. Instead of rolling forward the cumulative claims with estimated CL factors, we estimate multi-period factors that project the latest observations directly to the ultimate claims. This alternative perspective on CL reserving creates a natural pathway for the application of machine learning techniques to individual claims reserving. As a proof of concept, we present a small-scale real data application employing neural networks for individual claims reserving.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "categories": [
        "stat.AP",
        "q-fin.RM",
        "stat.ML"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.15385v1",
      "arxivUrl": "http://arxiv.org/abs/2602.15385v1",
      "doi": null
    },
    {
      "id": "2602.15374",
      "title": "Joint Modeling of Longitudinal EHR Data with Shared Random Effects for Informative Visiting and Observation Processes",
      "authors": [
        "Cheng-Han Yang",
        "Xu Shi",
        "Bhramar Mukherjee"
      ],
      "abstract": "Longitudinal electronic health record (EHR) data offer opportunities to study biomarker trajectories; however, association estimates-the primary inferential target-from standard models designed for regular observation times may be biased by a two-stage hierarchical missingness mechanism. The first stage is the visiting process (informative presence), where encounters occur at irregular times driven by patient health status; the second is the observation process (informative observation), where biomarkers are selectively measured during visits. To address these mechanisms, we propose a unified semiparametric joint modeling framework that simultaneously characterizes the visiting, biomarker observation, and longitudinal outcome processes. Central to this framework is a shared subject-specific Gaussian latent variable that captures unmeasured frailty and induces dependence across all components. We develop a three-stage estimation procedure and establish the consistency and asymptotic normality of our estimators. We also introduce a sequential procedure that imputes missing biomarkers prior to adjusting for irregular visiting and examine its performance. Simulation results demonstrate that our method yields unbiased estimates under this mechanism, whereas existing approaches can be substantially biased; notably, methods adjusting only for irregular visiting may exhibit even greater bias than those ignoring both mechanisms. We apply our framework to data from the All of Us Research Program to investigate associations between neighborhood-level socioeconomic status indicators and six blood-based biomarker trajectories, providing a robust tool for outpatient settings where irregular monitoring and selective measurement are prevalent.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "categories": [
        "stat.ME",
        "stat.AP"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.15374v1",
      "arxivUrl": "http://arxiv.org/abs/2602.15374v1",
      "doi": null
    },
    {
      "id": "2602.15319",
      "title": "Bayesian Inference for Joint Tail Risk in Paired Biomarkers via Archimedean Copulas with Restricted Jeffreys Priors",
      "authors": [
        "Agnideep Aich",
        "Md. Monzur Murshed",
        "Sameera Hewage",
        "Ashit Baran Aich"
      ],
      "abstract": "We propose a Bayesian copula-based framework to quantify clinically interpretable joint tail risks from paired continuous biomarkers. After converting each biomarker margin to rank-based pseudo-observations, we model dependence using one-parameter Archimedean copulas and focus on three probability-scale summaries at tail level $α$: the lower-tail joint risk $R_L(θ)=C_θ(α,α)$, the upper-tail joint risk $R_U(θ)=2α-1+C_θ(1-α,1-α)$, and the conditional lower-tail risk $R_C(θ)=R_L(θ)/α$. Uncertainty is quantified via a restricted Jeffreys prior on the copula parameter and grid-based posterior approximation, which induces an exact posterior for each tail-risk functional. In simulations from Clayton and Gumbel copulas across multiple dependence strengths, posterior credible intervals achieve near-nominal coverage for $R_L$, $R_U$, and $R_C$. We then analyze NHANES 2017--2018 fasting glucose (GLU) and HbA1c (GHB) ($n=2887$) at $α=0.05$, obtaining tight posterior credible intervals for both the dependence parameter and induced tail risks. The results reveal markedly elevated extremal co-movement relative to independence; under the Gumbel model, the posterior mean joint upper-tail risk is $R_U(α)=0.0286$, approximately $11.46\\times$ the independence benchmark $α^2=0.0025$. Overall, the proposed approach provides a principled, dependence-aware method for reporting joint and conditional extremal-risk summaries with Bayesian uncertainty quantification in biomedical applications.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "categories": [
        "stat.ME",
        "stat.AP"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.15319v1",
      "arxivUrl": "http://arxiv.org/abs/2602.15319v1",
      "doi": null
    },
    {
      "id": "2602.15312",
      "title": "Extracting Consumer Insight from Text: A Large Language Model Approach to Emotion and Evaluation Measurement",
      "authors": [
        "Stephan Ludwig",
        "Peter J. Danaher",
        "Xiaohao Yang",
        "Yu-Ting Lin",
        "Ehsan Abedin",
        "Dhruv Grewal",
        "Lan Du"
      ],
      "abstract": "Accurately measuring consumer emotions and evaluations from unstructured text remains a core challenge for marketing research and practice. This study introduces the Linguistic eXtractor (LX), a fine-tuned, large language model trained on consumer-authored text that also has been labeled with consumers' self-reported ratings of 16 consumption-related emotions and four evaluation constructs: trust, commitment, recommendation, and sentiment. LX consistently outperforms leading models, including GPT-4 Turbo, RoBERTa, and DeepSeek, achieving 81% macro-F1 accuracy on open-ended survey responses and greater than 95% accuracy on third-party-annotated Amazon and Yelp reviews. An application of LX to online retail data, using seemingly unrelated regression, affirms that review-expressed emotions predict product ratings, which in turn predict purchase behavior. Most emotional effects are mediated by product ratings, though some emotions, such as discontent and peacefulness, influence purchase directly, indicating that emotional tone provides meaningful signals beyond star ratings. To support its use, a no-code, cost-free, LX web application is available, enabling scalable analyses of consumer-authored text. In establishing a new methodological foundation for consumer perception measurement, this research demonstrates new methods for leveraging large language models to advance marketing research and practice, thereby achieving validated detection of marketing constructs from consumer data.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "categories": [
        "cs.CL",
        "econ.EM"
      ],
      "primaryCategory": "cs.CL",
      "pdfUrl": "https://arxiv.org/pdf/2602.15312v1",
      "arxivUrl": "http://arxiv.org/abs/2602.15312v1",
      "doi": null
    },
    {
      "id": "2602.15291",
      "title": "Structural grouping of extreme value models via graph fused lasso",
      "authors": [
        "Takuma Yoshida",
        "Koki Momoki",
        "Shuichi Kawano"
      ],
      "abstract": "The generalized Pareto distribution (GPD) is a fundamental model for analyzing the tail behavior of a distribution. In particular, the shape parameter of the GPD characterizes the extremal properties of the distribution. As described in this paper, we propose a method for grouping shape parameters in the GPD for clustered data via graph fused lasso. The proposed method simultaneously estimates the model parameters and identifies which clusters can be grouped together. We establish the asymptotic theory of the proposed estimator and demonstrate that its variance is lower than that of the cluster-wise estimator. This variance reduction not only enhances estimation stability but also provides a principled basis for identifying homogeneity and heterogeneity among clusters in terms of their tail behavior. We assess the performance of the proposed estimator through Monte Carlo simulations. As an illustrative example, our method is applied to rainfall data from 996 clustered sites across Japan.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.15291v1",
      "arxivUrl": "http://arxiv.org/abs/2602.15291v1",
      "doi": null
    },
    {
      "id": "2602.15289",
      "title": "A Projection Approach to Nonparametric Significance and Conditional Independence Testing",
      "authors": [
        "Xiaojun Song",
        "Jichao Yuan"
      ],
      "abstract": "This paper develops a novel nonparametric significance test based on a tailored nonparametric-type projected weighting function that exhibits appealing theoretical and numerical properties. We derive the asymptotic properties of the proposed test and show that it can detect local alternatives at the parametric rate. Using the nonparametric orthogonal projection, we construct a computationally convenient multiplier bootstrap to obtain critical values from the case-dependent asymptotic null distribution. Compared with the existing literature, our approach overcomes the need for a stronger compact support assumption on the density of covariates arising from random denominators. We also extend the tailor-made projection procedure to test the conditional independence assumption. The simulation experiments further illustrate the advantages of our proposed method in testing significance and conditional independence in finite samples.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "categories": [
        "econ.EM"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.15289v1",
      "arxivUrl": "http://arxiv.org/abs/2602.15289v1",
      "doi": null
    },
    {
      "id": "2602.15247",
      "title": "Sample size and power determination for assessing overall SNP effects in joint modeling of longitudinal and time-to-event data",
      "authors": [
        "Yuan Bian",
        "Shelley B. Bull"
      ],
      "abstract": "Longitudinal biomarkers are frequently collected in clinical studies due to their strong association with time-to-event outcomes. While considerable progress has been made in methods for jointly modeling longitudinal and survival data, comparatively little attention has been paid to statistical design considerations, particularly sample size and power calculations, in genetic studies. Yet, appropriate sample size estimation is essential for ensuring adequate power and valid inference. Genetic variants may influence event risk through both direct effects and indirect effects mediated by longitudinal biomarkers. In this paper, we derive a closed-form sample size formula for testing the overall effect of a single nucleotide polymorphism within a joint modeling framework. Simulation studies demonstrate that the proposed formula yields accurate and robust performance in finite samples. We illustrate the practical utility of our method using data from the Diabetes Control and Complications Trial.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "categories": [
        "stat.ME",
        "stat.AP"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.15247v1",
      "arxivUrl": "http://arxiv.org/abs/2602.15247v1",
      "doi": null
    },
    {
      "id": "2602.15167",
      "title": "Distributional Deep Learning for Super-Resolution of 4D Flow MRI under Domain Shift",
      "authors": [
        "Xiaoyi Wen",
        "Fei Jiang"
      ],
      "abstract": "Super-resolution is widely used in medical imaging to enhance low-quality data, reducing scan time and improving abnormality detection. Conventional super-resolution approaches typically rely on paired datasets of downsampled and original high resolution images, training models to reconstruct high resolution images from their artificially degraded counterparts. However, in real-world clinical settings, low resolution data often arise from acquisition mechanisms that differ significantly from simple downsampling. As a result, these inputs may lie outside the domain of the training data, leading to poor model generalization due to domain shift. To address this limitation, we propose a distributional deep learning framework that improves model robustness and domain generalization. We develop this approch for enhancing the resolution of 4D Flow MRI (4DF). This is a novel imaging modality that captures hemodynamic flow velocity and clinically relevant metrics such as vessel wall stress. These metrics are critical for assessing aneurysm rupture risk. Our model is initially trained on high resolution computational fluid dynamics (CFD) simulations and their downsampled counterparts. It is then fine-tuned on a small, harmonized dataset of paired 4D Flow MRI and CFD samples. We derive the theoretical properties of our distributional estimators and demonstrate that our framework significantly outperforms traditional deep learning approaches through real data applications. This highlights the effectiveness of distributional learning in addressing domain shift and improving super-resolution performance in clinically realistic scenarios.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "categories": [
        "cs.CV",
        "stat.AP",
        "stat.ML"
      ],
      "primaryCategory": "cs.CV",
      "pdfUrl": "https://arxiv.org/pdf/2602.15167v1",
      "arxivUrl": "http://arxiv.org/abs/2602.15167v1",
      "doi": null
    },
    {
      "id": "2602.15150",
      "title": "bayesics: Core Statistical Methods via Bayesian Inference in R",
      "authors": [
        "Daniel K. Sewell",
        "Alan T. Arakkal"
      ],
      "abstract": "Bayesian statistics is an integral part of contemporary applied science. bayesics provides a single framework, unified in syntax and output, for performing the most commonly used statistical procedures, ranging from one- and two-sample inference to general mediation analysis. bayesics leans hard away from the requirement that users be familiar with sampling algorithms by using closed-form solutions whenever possible, and automatically selecting the number of posterior samples required for accurate inference when such solutions are not possible. bayesics} focuses on providing key inferential quantities: point estimates, credible intervals, probability of direction, region of practical equivalance (ROPE), and, when applicable, Bayes factors. While algorithmic assessment is not required in bayesics, model assessment is still critical; towards that, bayesics provides diagnostic plots for parametric inference, including Bayesian p-values. Finally, bayesics provides extensions to models implemented in alternative R packages and, in the case of mediation analysis, correction to existing implementations.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "categories": [
        "stat.ME",
        "stat.CO"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.15150v1",
      "arxivUrl": "http://arxiv.org/abs/2602.15150v1",
      "doi": null
    },
    {
      "id": "2602.15007",
      "title": "Hidden Markov Individual-level Models of Infectious Disease Transmission",
      "authors": [
        "Dirk Douwes-Schultz",
        "Rob Deardon",
        "Alexandra M. Schmidt"
      ],
      "abstract": "Individual-level epidemic models are increasingly being used to help understand the transmission dynamics of various infectious diseases. However, fitting such models to individual-level epidemic data is challenging, as we often only know when an individual's disease status was detected (e.g., when they showed symptoms) and not when they were infected or removed. We propose an autoregressive coupled hidden Markov model to infer unknown infection and removal times, as well as other model parameters, from a single observed detection time for each detected individual. Unlike more traditional data augmentation methods used in epidemic modelling, we do not assume that this detection time corresponds to infection or removal or that infected individuals must at some point be detected. Bayesian coupled hidden Markov models have been used previously for individual-level epidemic data. However, these approaches assumed each individual was continuously tested and that the tests were independent. In practice, individuals are often only tested until their first positive test, and even if they are continuously tested, only the initial detection times may be reported. In addition, multiple tests on the same individual may not be independent. We accommodate these scenarios by assuming that the probability of detecting the disease can depend on past observations, which allows us to fit a much wider range of practical applications. We illustrate the flexibility of our approach by fitting two examples: an experiment on the spread of tomato spot wilt virus in pepper plants and an outbreak of norovirus among nurses in a hospital.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "categories": [
        "stat.AP",
        "stat.ME"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.15007v1",
      "arxivUrl": "http://arxiv.org/abs/2602.15007v1",
      "doi": null
    },
    {
      "id": "2602.15095",
      "title": "Natural direct effects of vaccines and post-vaccination behaviour",
      "authors": [
        "Bronner P. Gonçalves",
        "Piero L. Olliaro",
        "Sheena G. Sullivan",
        "Benjamin J. Cowling"
      ],
      "abstract": "Knowledge of the protection afforded by vaccines might, in some circumstances, modify a vaccinated individual's behaviour, potentially increasing exposure to pathogens and hindering effectiveness. Although vaccine studies typically do not explicitly account for this possibility in their analyses, we argue that natural direct effects might represent appropriate causal estimands when an objective is to quantify the effect of vaccination on disease while blocking its influence on behaviour. There are, however, complications of a practical nature for the estimation of natural direct effects in this context. Here, we discuss some of these issues, including exposure-outcome and mediator-outcome confounding by healthcare seeking behaviour, and possible approaches to facilitate estimates of these effects. This work highlights the importance of data collection on behaviour, of assessing whether vaccination induces riskier behaviour, and of understanding the potential effects of interventions on vaccination that could turn off vaccine's influence on behaviour.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "categories": [
        "stat.ME",
        "stat.AP"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.15095v1",
      "arxivUrl": "http://arxiv.org/abs/2602.15095v1",
      "doi": null
    },
    {
      "id": "2602.14991",
      "title": "Joint analysis for multivariate longitudinal and event time data with a change point anchored at interval-censored event time",
      "authors": [
        "Yue Zhan",
        "Cheng Zheng",
        "Ying Zhang"
      ],
      "abstract": "Huntington's disease (HD) is an autosomal dominant neurodegenerative disorder characterized by motor dysfunction, psychiatric disturbances, and cognitive decline. The onset of HD is marked by severe motor impairment, which may be predicted by prior cognitive decline and, in turn, exacerbate cognitive deficits. Clinical data, however, are often collected at discrete time points, so the timing of disease onset is subject to interval censoring. To address the challenges posed by such data, we develop a joint model for multivariate longitudinal biomarkers with a change point anchored at an interval-censored event time. The model simultaneously assesses the effects of longitudinal biomarkers on the event time and the changes in biomarker trajectories following the event. We conduct a comprehensive simulation study to demonstrate the finite-sample performance of the proposed method for causal inference. Finally, we apply the method to PREDICT-HD, a multisite observational cohort study of prodromal HD individuals, to ascertain how cognitive impairment and motor dysfunction interact during disease progression.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.14991v1",
      "arxivUrl": "http://arxiv.org/abs/2602.14991v1",
      "doi": null
    },
    {
      "id": "2602.14981",
      "title": "Block Empirical Likelihood Inference for Longitudinal Generalized Partially Linear Single-Index Models",
      "authors": [
        "Tianni Zhang",
        "Yuyao Wang",
        "Yu Lu",
        "and Mengfei Ran"
      ],
      "abstract": "Generalized partially linear single-index models (GPLSIMs) provide a flexible and interpretable semiparametric framework for longitudinal outcomes by combining a low-dimensional parametric component with a nonparametric index component. For repeated measurements, valid inference is challenging because within-subject correlation induces nuisance parameters and variance estimation can be unstable in semiparametric settings. We propose a profile estimating-equation approach based on spline approximation of the unknown link function and construct a subject-level block empirical likelihood (BEL) for joint inference on the parametric coefficients and the single-index direction. The resulting BEL ratio statistic enjoys a Wilks-type chi-square limit, yielding likelihood-free confidence regions without explicit sandwich variance estimation. We also discuss practical implementation, including constrained optimization for the index parameter, working-correlation choices, and bootstrap-based confidence bands for the nonparametric component. Simulation studies and an application to the epilepsy longitudinal study illustrate the finite-sample performance.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.CO",
        "stat.ML"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.14981v1",
      "arxivUrl": "http://arxiv.org/abs/2602.14981v1",
      "doi": null
    },
    {
      "id": "2602.14952",
      "title": "Locally Adaptive Multi-Objective Learning",
      "authors": [
        "Jivat Neet Kaur",
        "Isaac Gibbs",
        "Michael I. Jordan"
      ],
      "abstract": "We consider the general problem of learning a predictor that satisfies multiple objectives of interest simultaneously, a broad framework that captures a range of specific learning goals including calibration, regret, and multiaccuracy. We work in an online setting where the data distribution can change arbitrarily over time. Existing approaches to this problem aim to minimize the set of objectives over the entire time horizon in a worst-case sense, and in practice they do not necessarily adapt to distribution shifts. Earlier work has aimed to alleviate this problem by incorporating additional objectives that target local guarantees over contiguous subintervals. Empirical evaluation of these proposals is, however, scarce. In this article, we consider an alternative procedure that achieves local adaptivity by replacing one part of the multi-objective learning method with an adaptive online algorithm. Empirical evaluations on datasets from energy forecasting and algorithmic fairness show that our proposed method improves upon existing approaches and achieves unbiased predictions over subgroups, while remaining robust under distribution shift.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "categories": [
        "cs.LG",
        "math.OC",
        "stat.ME",
        "stat.ML"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2602.14952v1",
      "arxivUrl": "http://arxiv.org/abs/2602.14952v1",
      "doi": null
    },
    {
      "id": "2602.14942",
      "title": "Balanced Stochastic Block Model for Community Detection in Signed Networks",
      "authors": [
        "Yichao Chen",
        "Weijing Tang",
        "Ji Zhu"
      ],
      "abstract": "Community detection, discovering the underlying communities within a network from observed connections, is a fundamental problem in network analysis, yet it remains underexplored for signed networks. In signed networks, both edge connection patterns and edge signs are informative, and structural balance theory (e.g., triangles aligned with ``the enemy of my enemy is my friend'' and ``the friend of my friend is my friend'' are more prevalent) provides a global higher-order principle that guides community formation. We propose a Balanced Stochastic Block Model (BSBM), which incorporates balance theory into the network generating process such that balanced triangles are more likely to occur. We develop a fast profile pseudo-likelihood estimation algorithm with provable convergence and establish that our estimator achieves strong consistency under weaker signal conditions than methods for the binary SBM that rely solely on edge connectivity. Extensive simulation studies and two real-world signed networks demonstrate strong empirical performance.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.14942v1",
      "arxivUrl": "http://arxiv.org/abs/2602.14942v1",
      "doi": null
    },
    {
      "id": "2602.14877",
      "title": "When to repeat a biomarker test? Decomposing sources of variation from conditionally repeated measurements",
      "authors": [
        "Supun Manathunga",
        "Mart P. Janssen",
        "Yu Luo",
        "W. Alton Russell",
        "Mart Pothast"
      ],
      "abstract": "Repeating an imperfect biomarker test based on an initial result can introduce bias and influence misclassification risk. For example, in some blood donation settings, blood donors' hemoglobin is remeasured when the initial measurement falls below a minimum threshold for donor eligibility. This paper explores methods that use data resulting from processes with conditionally repeated biomarker measurement to decompose the variation in observed measurements of a continuous biomarker into population variability and variability arising from the measurement procedure. We present two frequentist approaches with analytical solutions, but these approaches perform poorly in a dataset of conditionally repeated blood donor hemoglobin measurements where normality assumptions are not met. We then develop a Bayesian hierarchical framework that allows for different distributional assumptions, which we apply to the blood donor hemoglobin dataset. Using a Bayesian hierarchical model that assumes normally distributed population hemoglobin and heavy tailed $t$-distributed measurement variation, we found that the total measurement variation accounted for 22\\% of the total variance among females and 25\\% among males, with population standard deviations of $1.07\\, \\rm g/dL$ for female donors and $1.28\\, \\rm g/dL$ for male donors. Our Bayesian framework can use data resulting from any clinical process with conditionally repeated biomarker measurements to estimate individuals' misclassification risk after one or more noisy continuous measurements and inform evidence-based conditional retesting decision rules.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "categories": [
        "stat.AP"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.14877v1",
      "arxivUrl": "http://arxiv.org/abs/2602.14877v1",
      "doi": null
    },
    {
      "id": "2602.14862",
      "title": "The Well-Tempered Classifier: Some Elementary Properties of Temperature Scaling",
      "authors": [
        "Pierre-Alexandre Mattei",
        "Bruno Loureiro"
      ],
      "abstract": "Temperature scaling is a simple method that allows to control the uncertainty of probabilistic models. It is mostly used in two contexts: improving the calibration of classifiers and tuning the stochasticity of large language models (LLMs). In both cases, temperature scaling is the most popular method for the job. Despite its popularity, a rigorous theoretical analysis of the properties of temperature scaling has remained elusive. We investigate here some of these properties. For classification, we show that increasing the temperature increases the uncertainty in the model in a very general sense (and in particular increases its entropy). However, for LLMs, we challenge the common claim that increasing temperature increases diversity. Furthermore, we introduce two new characterisations of temperature scaling. The first one is geometric: the tempered model is shown to be the information projection of the original model onto the set of models with a given entropy. The second characterisation clarifies the role of temperature scaling as a submodel of more general linear scalers such as matrix scaling and Dirichlet calibration: we show that temperature scaling is the only linear scaler that does not change the hard predictions of the model.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.IT",
        "cs.LG",
        "stat.ME"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2602.14862v1",
      "arxivUrl": "http://arxiv.org/abs/2602.14862v1",
      "doi": null
    },
    {
      "id": "2602.14861",
      "title": "Bias analysis of a linear order-statistic inequality index estimator: Unbiasedness under gamma populations",
      "authors": [
        "Roberto Vila",
        "Helton Saulo"
      ],
      "abstract": "This paper studies a class of rank-based inequality measures built from linear combinations of expected order statistics. The proposed framework unifies several well-known indices, including the classical Gini coefficient, the $m$th Gini index, extended $m$th Gini index and $S$-Gini index, and also connects to spectral inequality measures through an integral representation. We investigate the finite-sample behavior of a natural U-statistic-type estimator that averages weighted order-statistic contrasts over all subsamples of fixed size and normalizes by the sample mean. A general bias decomposition is derived in terms of components that isolate the effect of random normalization on each rank level, yielding analytical expressions that can be evaluated under broad non-negative distributions via Laplace-transform methods. Under mild moment conditions, the estimator is shown to be asymptotically unbiased. Moreover, we prove exact unbiasedness under gamma populations for any sample size, extending earlier unbiasedness results for Gini-type estimators. A Monte Carlo study is performed to numerically check that the theoretical unbiasednes under gamma populations.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "categories": [
        "math.ST",
        "stat.ME"
      ],
      "primaryCategory": "math.ST",
      "pdfUrl": "https://arxiv.org/pdf/2602.14861v1",
      "arxivUrl": "http://arxiv.org/abs/2602.14861v1",
      "doi": null
    },
    {
      "id": "2602.14835",
      "title": "The Global Representativeness Index: A Total Variation Distance Framework for Measuring Demographic Fidelity in Survey Research",
      "authors": [
        "Evan Hadfield"
      ],
      "abstract": "Global survey research increasingly informs high-stakes decisions in AI governance and cross-cultural policy, yet no standardized metric quantifies how well a sample's demographic composition matches its target population. Response rates and demographic quotas -- the prevailing proxies for sample quality -- measure effort and coverage but not distributional fidelity. This paper introduces the Global Representativeness Index (GRI), a framework grounded in Total Variation Distance that scores any survey sample against population benchmarks across multiple demographic dimensions on a [0, 1] scale. Validation on seven waves of the Global Dialogues survey (N = 7,500 across 60+ countries) finds fine-grained demographic GRI scores of only 0.33--0.36 -- roughly 43% of the theoretical maximum at that sample size. Cross-validation on the World Values Survey (seven waves, N = 403,000), Afrobarometer Round 9 (N = 53,000), and Latinobarometro (N = 19,000) reveals that even large probability surveys score below 0.22 on fine-grained global demographics when country coverage is limited. The GRI connects to classical survey statistics through the design effect; both metrics are recommended as a minimum summary of sample quality, since GRI quantifies demographic distance symmetrically while effective N captures the asymmetric inferential cost of underrepresentation. The framework is released as an open-source Python library with UN and Pew Research Center population benchmarks, applicable to survey research, machine learning dataset auditing, and AI evaluation benchmarks.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "categories": [
        "stat.ME",
        "cs.CY",
        "stat.AP"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.14835v1",
      "arxivUrl": "http://arxiv.org/abs/2602.14835v1",
      "doi": null
    },
    {
      "id": "2602.14828",
      "title": "Exploring the limits of pre-trained embeddings in machine-guided protein design: a case study on predicting AAV vector viability",
      "authors": [
        "Ana F. Rodrigues",
        "Lucas Ferraz",
        "Laura Balbi",
        "Pedro Giesteira Cotovio",
        "Catia Pesquita"
      ],
      "abstract": "Effective representations of protein sequences are widely recognized as a cornerstone of machine learning-based protein design. Yet, protein bioengineering poses unique challenges for sequence representation, as experimental datasets typically feature few mutations, which are either sparsely distributed across the entire sequence or densely concentrated within localized regions. This limits the ability of sequence-level representations to extract functionally meaningful signals. In addition, comprehensive comparative studies remain scarce, despite their crucial role in clarifying which representations best encode relevant information and ultimately support superior predictive performance. In this study, we systematically evaluate multiple ProtBERT and ESM2 embedding variants as sequence representations, using the adeno-associated virus capsid as a case study and prototypical example of bioengineering, where functional optimization is targeted through highly localized sequence variation within an otherwise large protein. Our results reveal that, prior to fine-tuning, amino acid-level embeddings outperform sequence-level representations in supervised predictive tasks, whereas the latter tend to be more effective in unsupervised settings. However, optimal performance is only achieved when embeddings are fine-tuned with task-specific labels, with sequence-level representations providing the best performance. Moreover, our findings indicate that the extent of sequence variation required to produce notable shifts in sequence representations exceeds what is typically explored in bioengineering studies, showing the need for fine-tuning in datasets characterized by sparse or highly localized mutations.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "categories": [
        "q-bio.QM",
        "cs.LG"
      ],
      "primaryCategory": "q-bio.QM",
      "pdfUrl": "https://arxiv.org/pdf/2602.14828v1",
      "arxivUrl": "http://arxiv.org/abs/2602.14828v1",
      "doi": null
    },
    {
      "id": "2602.14813",
      "title": "The empirical distribution of sequential LS factors in Multi-level Dynamic Factor Models",
      "authors": [
        "Gian Pietro Bellocca",
        "Ignacio Garrón",
        "Vladimir Rodríguez-Caballero",
        "Esther Ruiz"
      ],
      "abstract": "The research question we answer in this paper is whether the asymptotic distribution derived by Bai (2003) for Principal Components (PC) factors in dynamic factor models (DFMs) can approximate the empirical distribution of the sequential Least Squares (SLS) estimator of global and group-specific factors in multi-level dynamic factor models (ML-DFMs). Monte Carlo experiments confirm that under general forms of the idiosyncratic covariance matrix, the finite-sample distribution of SLS global and group-specific factors can be well approximated using the asymptotic distribution of PC factors. We also analyse the performance of alternative estimators of the asymptotic mean squared error (MSE) of the SLS factors and show that the MSE estimator that allows for idiosyncratic cross-sectional correlation and accounts for estimation uncertainty of factor loadings is best.",
      "published": "2026-02-16",
      "updated": "2026-02-17",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.14813v2",
      "arxivUrl": "http://arxiv.org/abs/2602.14813v2",
      "doi": null
    },
    {
      "id": "2602.14616",
      "title": "Higher-Order Hit-&-Run Samplers for Linearly Constrained Densities",
      "authors": [
        "Richard D. Paul",
        "Anton Stratmann",
        "Johann F. Jadebeck",
        "Martin Beyß",
        "Hanno Scharr",
        "David Rügamer",
        "Katharina Nöh"
      ],
      "abstract": "Markov chain Monte Carlo (MCMC) sampling of densities restricted to linearly constrained domains is an important task arising in Bayesian treatment of inverse problems in the natural sciences. While efficient algorithms for uniform polytope sampling exist, much less work has dealt with more complex constrained densities. In particular, gradient information as used in unconstrained MCMC is not necessarily helpful in the constrained case, where the gradient may push the proposal's density out of the polytope. In this work, we propose a novel constrained sampling algorithm, which combines strengths of higher-order information, like the target's log-density's gradients and curvature, with the Hit-&-Run proposal, a simple mechanism which guarantees the generation of feasible proposals, fulfilling the linear constraints. Our extensive experiments demonstrate improved sampling efficiency on complex constrained densities over various constrained and unconstrained samplers.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "categories": [
        "stat.CO",
        "q-bio.QM",
        "stat.AP"
      ],
      "primaryCategory": "stat.CO",
      "pdfUrl": "https://arxiv.org/pdf/2602.14616v1",
      "arxivUrl": "http://arxiv.org/abs/2602.14616v1",
      "doi": null
    },
    {
      "id": "2602.14607",
      "title": "A Bayesian Approach to Low-Discrepancy Subset Selection",
      "authors": [
        "Nathan Kirk"
      ],
      "abstract": "Low-discrepancy designs play a central role in quasi-Monte Carlo methods and are increasingly influential in other domains such as machine learning, robotics and computer graphics, to name a few. In recent years, one such low-discrepancy construction method called subset selection has received a lot of attention. Given a large population, one optimally selects a small low-discrepancy subset with respect to a discrepancy-based objective. Versions of this problem are known to be NP-hard. In this text, we establish, for the first time, that the subset selection problem with respect to kernel discrepancies is also NP-hard. Motivated by this intractability, we propose a Bayesian Optimization procedure for the subset selection problem utilizing the recent notion of deep embedding kernels. We demonstrate the performance of the BO algorithm to minimize discrepancy measures and note that the framework is broadly applicable any design criteria.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "categories": [
        "stat.ME",
        "cs.LG",
        "math.NA",
        "stat.CO"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.14607v1",
      "arxivUrl": "http://arxiv.org/abs/2602.14607v1",
      "doi": null
    },
    {
      "id": "2602.14440",
      "title": "CAIRO: Decoupling Order from Scale in Regression",
      "authors": [
        "Harri Vanhems",
        "Yue Zhao",
        "Peng Shi",
        "Archer Y. Yang"
      ],
      "abstract": "Standard regression methods typically optimize a single pointwise objective, such as mean squared error, which conflates the learning of ordering with the learning of scale. This coupling renders models vulnerable to outliers and heavy-tailed noise. We propose CAIRO (Calibrate After Initial Rank Ordering), a framework that decouples regression into two distinct stages. In the first stage, we learn a scoring function by minimizing a scale-invariant ranking loss; in the second, we recover the target scale via isotonic regression. We theoretically characterize a class of \"Optimal-in-Rank-Order\" objectives -- including variants of RankNet and Gini covariance -- and prove that they recover the ordering of the true conditional mean under mild assumptions. We further show that subsequent monotone calibration guarantees recovery of the true regression function. Empirically, CAIRO combines the representation learning of neural networks with the robustness of rank-based statistics. It matches the performance of state-of-the-art tree ensembles on tabular benchmarks and significantly outperforms standard regression objectives in regimes with heavy-tailed or heteroskedastic noise.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "categories": [
        "stat.ME",
        "cs.LG",
        "stat.ML"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.14440v1",
      "arxivUrl": "http://arxiv.org/abs/2602.14440v1",
      "doi": null
    },
    {
      "id": "2602.14414",
      "title": "The Role of Measured Covariates in Assessing Sensitivity to Unmeasured Confounding",
      "authors": [
        "Abhinandan Dalal",
        "Iris Horng",
        "Yang Feng",
        "Dylan S. Small"
      ],
      "abstract": "Sensitivity analysis is widely used to assess the robustness of causal conclusions in observational studies, yet its interaction with the structure of measured covariates is often overlooked. When latent confounders cannot be directly adjusted for and are instead controlled using proxy variables, strong associations between exposure and measured proxies can amplify sensitivity to residual confounding. We formalize this phenomenon in linear regression settings by showing that a simple ratio involving the exposure model coefficient and residual exposure variance provides an observable measure of this increased sensitivity. Applying our framework to smoking and lung cancer, we document how growing socioeconomic stratification in smoking behavior over time leads to heightened sensitivity to unmeasured confounding in more recent data. These results highlight the importance of multicollinearity when interpreting sensitivity analyses based on proxy adjustment.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "categories": [
        "stat.ME",
        "econ.EM",
        "stat.AP"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.14414v1",
      "arxivUrl": "http://arxiv.org/abs/2602.14414v1",
      "doi": null
    },
    {
      "id": "2602.14387",
      "title": "Automatic Variance Adjustment for Small Area Estimation",
      "authors": [
        "Jon Wakefield",
        "Jitong Jiang",
        "Yunhan Wu"
      ],
      "abstract": "Small area estimation (SAE) is a common endeavor and is used in a variety of disciplines. In low- and middle-income countries (LMICs), in which household surveys provide the most reliable and timely source of data, SAE is vital for highlighting disparities in health and demographic indicators. Weighted estimators are ideal for inference, but for fine geographical partitions in which there are insufficient data, SAE models are required. The most common approach is Fay-Herriot area-level modeling in which the data requirements are a weighted estimate and an associated variance estimate. The latter can be undefined or unstable when data are sparse and so we propose a principled modification which is based on augmenting the available data with a prior sample from a hypothetical survey. This adjustment is generally available, respects the design and is simple to implement. We examine the empirical properties of the adjustment through simulation and illustrate its use with wasting data from a 2018 Zambian Demographic and Health Survey. The modification is implemented as an automatic remedy in the R package surveyPrev, which provides a comprehensive suite of tools for conducing SAE in LMICs.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.14387v1",
      "arxivUrl": "http://arxiv.org/abs/2602.14387v1",
      "doi": null
    },
    {
      "id": "2602.14349",
      "title": "Same Prompt, Different Outcomes: Evaluating the Reproducibility of Data Analysis by LLMs",
      "authors": [
        "Jiaxin Cui",
        "Rohan Alexander"
      ],
      "abstract": "We systematically evaluate the reproducibility of data analysis conducted by Large Language Models (LLMs). We evaluate two prompting strategies, six models, and four temperature settings, with ten independent executions per configuration, yielding 480 total attempts. We assess the completion, concordance, validity, and consistency of each attempt and find considerable variation in the analytical results even for consistent configurations. This suggests, as with human data analysis, the data analysis conducted by LLMs can vary, even given the same task, data, and settings. Our results mean that if an LLM is being used to conduct data analysis, then it should be run multiple times independently and the distribution of results considered.",
      "published": "2026-02-15",
      "updated": "2026-02-15",
      "categories": [
        "stat.AP"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.14349v1",
      "arxivUrl": "http://arxiv.org/abs/2602.14349v1",
      "doi": null
    },
    {
      "id": "2602.14328",
      "title": "Conformational landscapes in cryo-ET data based on MD simulations",
      "authors": [
        "Slavica Jonic"
      ],
      "abstract": "Cryo-electron tomography (cryo-ET) provides a unique window into molecular organization in cellular environments (in situ). However, the interpretation of molecular structural information is complicated by several intrinsic properties of cryo-ET data, such as noise, missing wedge, and continuous conformational variability of the molecules. Additionally, in crowded in situ environments, the number of particles extracted is sometimes small and precludes extensive classification into discrete states. These challenges shift the emphasis from high-resolution structure determination toward validation and interpretation of low-resolution density maps, and analysis of conformational flexibility. Molecular Dynamics (MD) simulations are particularly well suited to this task, as they provide a physically grounded way to explore continuous conformation transitions consistent with both experimental data and molecular energetics. This review focuses on the roles of MD simulations in cryo-ET, emphasizing their use in emerging methods for conformational landscape determination and their contribution to gain new biological insight.",
      "published": "2026-02-15",
      "updated": "2026-02-15",
      "categories": [
        "q-bio.BM",
        "q-bio.QM"
      ],
      "primaryCategory": "q-bio.BM",
      "pdfUrl": "https://arxiv.org/pdf/2602.14328v1",
      "arxivUrl": "http://arxiv.org/abs/2602.14328v1",
      "doi": null
    },
    {
      "id": "2602.14303",
      "title": "A New SMP Transformed Standard Weibull Distribution for Health Data Modelling",
      "authors": [
        "Isqeel Ogunsola",
        "Nurudeen Ajadi",
        "Gboyega Adepoju"
      ],
      "abstract": "New methods of extending base distributions are always invoke to increase their adaptability in modeling real life data. Recently, SMP method was introduced but Weibull distribution is yet to be explored through this method. First, we provide updated review on SMP transformed distributions. We then proposed and developed another extended Weibull distribution through this technique named SMPtW. Importantly, twelve of its statistical properties - reliability measures, quantile function, moment, stress-strength, mean waiting time, moment generating function, characteristics function, renyi entropy, order statistics, mean residual life and mode, were derived and studied extensively. The hazard function has a decreasing, increasing and constant shapes. We found a relation between the quantile of SMPtW and that of SMP Pareto distribution despite their difference in density functions. We adopt the inverse transform approach in random number generation and through simulation we evaluate maximum likelihood estimates (MLE) performance of its parameters. The result showed that MLE is consistent all through. The performance of the distribution was then examined using health dataset compared with five similar distributions. The results showed that three parameters SMPtW performed best among the competing models.",
      "published": "2026-02-15",
      "updated": "2026-02-15",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.14303v1",
      "arxivUrl": "http://arxiv.org/abs/2602.14303v1",
      "doi": null
    },
    {
      "id": "2602.14288",
      "title": "Dual-Channel Closed Loop Supply Chain Competition: A Stackelberg--Nash Approach",
      "authors": [
        "Gurkirat Wadhwa"
      ],
      "abstract": "In many consumer electronics and appliance markets, manufacturers sell products through competing retailers while simultaneously relying on take-back programs to recover used items for remanufacturing. Designing such programs is challenging when firms compete on prices and consumers differ in their willingness to return products. Motivated by these settings, this paper develops a game theoretic framework to analyze pricing and take-back decisions in a dual-channel closed loop supply chain (CLSC) with two competing manufacturers and two competing retailers. Manufacturers act as Stackelberg leaders, simultaneously determining wholesale prices and consumer take-back bonuses, while retailers engage in Nash competition over retail prices. The model integrates three key elements: (i) segmented linear demand with cross-price effects, (ii) deterministic product returns, and (iii) an inertia responsiveness allocation mechanism governing the distribution of returned products between manufacturers. Closed form Nash equilibria are derived for the retailer subgame, along with symmetric Stackelberg equilibria for manufacturers. We derive a feasibility threshold for take-back incentives, identifying conditions under which firms optimally offer positive bonuses to consumers. The results further demonstrate that higher remanufacturing value or return rates lead the manufacturers to lower wholesale prices in order to expand sales and capture additional return volumes, while high consumer inertia weakens incentives for active collection. Numerical experiments illustrate and reinforce the analytical results, highlighting how consumer behavior, market structure and product substitutability influence prices, bonuses, and return volumes. Overall, the study provides managerial insights for designing effective take-back programs and coordinating pricing decisions in competitive circular supply chains.",
      "published": "2026-02-15",
      "updated": "2026-02-15",
      "categories": [
        "econ.EM"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.14288v1",
      "arxivUrl": "http://arxiv.org/abs/2602.14288v1",
      "doi": null
    },
    {
      "id": "2602.14286",
      "title": "Online LLM watermark detection via e-processes",
      "authors": [
        "Weijie Su",
        "Ruodu Wang",
        "Zinan Zhao"
      ],
      "abstract": "Watermarking for large language models (LLMs) has emerged as an effective tool for distinguishing AI-generated text from human-written content. Statistically, watermark schemes induce dependence between generated tokens and a pseudo-random sequence, reducing watermark detection to a hypothesis testing problem on independence. We develop a unified framework for LLM watermark detection based on e-processes, providing anytime-valid guarantees for online testing. We propose various methods to construct empirically adaptive e-processes that can enhance the detection power. In addition, theoretical results are established to characterize the power properties of the proposed procedures. Some experiments demonstrate that the proposed framework achieves competitive performance compared to existing watermark detection methods.",
      "published": "2026-02-15",
      "updated": "2026-02-15",
      "categories": [
        "stat.ME",
        "stat.ML"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.14286v1",
      "arxivUrl": "http://arxiv.org/abs/2602.14286v1",
      "doi": null
    },
    {
      "id": "2602.14203",
      "title": "Evaluating the Impact of COVID-19 on Transportation Infrastructure Funding",
      "authors": [
        "Lu Gao",
        "Pan Lu",
        "Fengxiang Qiao",
        "Joshua Qiang Li",
        "Yunpeng Zhang",
        "Yihao Ren"
      ],
      "abstract": "The coronavirus disease 2019 (COVID-19) pandemic has caused a reduction in business and routine activity and resulted in less motor fuel consumption. Thus, the gas tax revenue is reduced which is the major funding resource supporting the rehabilitation and maintenance of transportation infrastructure systems. The focus of this study is to evaluate the impact of the COVID-19 pandemic on transportation infrastructure funds in the United States through analyzing the motor fuel consumption data. Machine learning models were developed by integrating COVID-19 scenarios, fuel consumptions, and demographic data. The best model achieves an R2-score of more than 95% and captures the fluctuations of fuel consumption during the pandemic. Using the developed model, we project future motor gas consumption for each state. For some states, the gas tax revenues are going to be 10%-15% lower than the pre-pandemic level for at least one or two years.",
      "published": "2026-02-15",
      "updated": "2026-02-15",
      "categories": [
        "stat.AP"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.14203v1",
      "arxivUrl": "http://arxiv.org/abs/2602.14203v1",
      "doi": "10.1061/9780784484364.012"
    },
    {
      "id": "2602.14198",
      "title": "Zipf-Mandelbrot Scaling in Korean Court Music: Universal Patterns in Music",
      "authors": [
        "Byeongchan Choi",
        "Junwon You",
        "Myung Ock Kim",
        "Jae-Hun Jung"
      ],
      "abstract": "Zipf's law, originally discovered in natural language and later generalized to the Zipf-Mandelbrot law, describes a power-law relationship between the frequency of a Zipfian element and its rank. Due to the semantic characteristics of this law, it has also been observed in musical data. However, most such studies have focused on Western music, and its applicability to non-Western music remains not well investigated. We analyzed 43 Korean court music pieces called Jeong-ak, spanning several centuries and written in the traditional Korean musical notation Jeongganbo. These pieces were transcribed into Western staff notation, and musical data such as pitch and duration were extracted. Using pitch, duration, and their paired combinations as Zipfian units, we found that Korean music also fits the Zipf-Mandelbrot law to a high degree, particularly for the paired pitch-duration unit. Korean music has evolved collectively over long periods, smoothing idiosyncratic variations and producing forms that are widely understandable among people. This collective evolution appears to have played a significant role in shaping the characteristics that lead to the satisfaction of Zipf-Mandelbrot law. Our findings provide additional evidence that Zipf-Mandelbrot scaling in musical data is universal across cultures. We further show that the joint distribution of two independent Zipfian data sets follows the Zipf-Mandelbrot law; in this sense, our result does not merely extend Zipf's law but deepens our understanding of how scaling laws behave under composition and interaction, offering a more unified perspective on rank-based statistical regularities.",
      "published": "2026-02-15",
      "updated": "2026-02-15",
      "categories": [
        "stat.AP"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.14198v1",
      "arxivUrl": "http://arxiv.org/abs/2602.14198v1",
      "doi": null
    }
  ],
  "metadata": {
    "lastUpdated": "2026-02-18T02:52:41.377Z",
    "totalPapers": 50,
    "categories": [
      "stat.ME",
      "stat.AP",
      "econ.EM",
      "q-bio.QM"
    ],
    "queryDate": "2026-02-18"
  }
}