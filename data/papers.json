{
  "papers": [
    {
      "id": "2602.13888",
      "title": "Mixture-of-experts Wishart model for covariance matrices with an application to Cancer drug screening",
      "authors": [
        "The Tien Mai",
        "Zhi Zhao"
      ],
      "abstract": "Covariance matrices arise naturally in different scientific fields, including finance, genomics, and neuroscience, where they encode dependence structures and reveal essential features of complex multivariate systems. In this work, we introduce a comprehensive Bayesian framework for analyzing heterogeneous covariance data through both classical mixture models and a novel mixture-of-experts Wishart (MoE-Wishart) model. The proposed MoE-Wishart model extends standard Wishart mixtures by allowing mixture weights to depend on predictors through a multinomial logistic gating network. This formulation enables the model to capture complex, nonlinear heterogeneity in covariance structures and to adapt subpopulation membership probabilities to covariate-dependent patterns. To perform inference, we develop an efficient Gibbs-within-Metropolis-Hastings sampling algorithm tailored to the geometry of the Wishart likelihood and the gating network. We additionally derive an Expectation-Maximization algorithm for maximum likelihood estimation in the mixture-of-experts setting. Extensive simulation studies demonstrate that the proposed Bayesian and maximum likelihood estimators achieve accurate subpopulation recovery and estimation under a range of heterogeneous covariance scenarios. Finally, we present an innovative application of our methodology to a challenging dataset: cancer drug sensitivity profiles, illustrating the ability of the MoE-Wishart model to leverage covariance across drug dosages and replicate measurements. Our methods are implemented in the \\texttt{R} package \\texttt{moewishart} available at https://github.com/zhizuio/moewishart .",
      "published": "2026-02-14",
      "updated": "2026-02-14",
      "categories": [
        "stat.ME",
        "stat.AP",
        "stat.CO",
        "stat.ML"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.13888v1",
      "arxivUrl": "http://arxiv.org/abs/2602.13888v1",
      "doi": null
    },
    {
      "id": "2602.13872",
      "title": "Predicting fixed-sample test decisions enables anytime-valid inference",
      "authors": [
        "Chris Holmes",
        "Stephen Walker"
      ],
      "abstract": "Statistical hypothesis tests typically use prespecified sample sizes, yet data often arrive sequentially. Interim analyses invalidate classical error guarantees, while existing sequential methods require rigid testing preschedules or incur substantial losses in statistical power. We introduce a simple procedure that transforms any fixed-sample hypothesis test into an anytime-valid test while ensuring Type-I error control and near-optimal power with substantial sample savings when the null hypothesis is false. At each step, the procedure predicts the probability that a classical test would reject the null hypothesis at its fixed-sample size, treating future observations as missing data under the null hypothesis. Thresholding this probability yields an anytime-valid stopping rule. In areas such as clinical trials, stopping early and safely can ensure that subjects receive the best treatments and accelerate the development of effective therapies.",
      "published": "2026-02-14",
      "updated": "2026-02-14",
      "categories": [
        "stat.ME",
        "stat.ML"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.13872v1",
      "arxivUrl": "http://arxiv.org/abs/2602.13872v1",
      "doi": null
    },
    {
      "id": "2602.13871",
      "title": "Ensemble-Conditional Gaussian Processes (Ens-CGP): Representation, Geometry, and Inference",
      "authors": [
        "Sai Ravela",
        "Jae Deok Kim",
        "Kenneth Gee",
        "Xingjian Yan",
        "Samson Mercier",
        "Lubna Albarghouty",
        "Anamitra Saha"
      ],
      "abstract": "We formulate Ensemble-Conditional Gaussian Processes (Ens-CGP), a finite-dimensional synthesis that centers ensemble-based inference on the conditional Gaussian law. Conditional Gaussian processes (CGP) arise directly from Gaussian processes under conditioning and, in linear-Gaussian settings, define the full posterior distribution for a Gaussian prior and linear observations. Classical Kalman filtering is a recursive algorithm that computes this same conditional law under dynamical assumptions; the conditional Gaussian law itself is therefore the underlying representational object, while the filter is one computational realization. In this sense, CGP provides the probabilistic foundation for Kalman-type methods as well as equivalent formulations as a strictly convex quadratic program (MAP estimation), RKHS-regularized regression, and classical regularization. Ens-CGP is the ensemble instantiation of this object, obtained by treating empirical ensemble moments as a (possibly low-rank) Gaussian prior and performing exact conditioning. By separating representation (GP -> CGP -> Ens-CGP) from computation (Kalman filters, EnKF variants, and iterative ensemble schemes), the framework links an earlier-established representational foundation for inference to ensemble-derived priors and clarifies the relationships among probabilistic, variational, and ensemble perspectives.",
      "published": "2026-02-14",
      "updated": "2026-02-14",
      "categories": [
        "math.ST",
        "cs.IT",
        "cs.LG",
        "math.OC",
        "stat.AP",
        "stat.ML"
      ],
      "primaryCategory": "math.ST",
      "pdfUrl": "https://arxiv.org/pdf/2602.13871v1",
      "arxivUrl": "http://arxiv.org/abs/2602.13871v1",
      "doi": null
    },
    {
      "id": "2602.13852",
      "title": "Experimentation Accelerator: Interpretable Insights and Creative Recommendations for A/B Testing with Content-Aware ranking",
      "authors": [
        "Zhengmian Hu",
        "Lei Shi",
        "Ritwik Sinha",
        "Justin Grover",
        "David Arbour"
      ],
      "abstract": "Modern online experimentation faces two bottlenecks: scarce traffic forces tough choices on which variants to test, and post-hoc insight extraction is manual, inconsistent, and often content-agnostic. Meanwhile, organizations underuse historical A/B results and rich content embeddings that could guide prioritization and creative iteration. We present a unified framework to (i) prioritize which variants to test, (ii) explain why winners win, and (iii) surface targeted opportunities for new, higher-potential variants. Leveraging treatment embeddings and historical outcomes, we train a CTR ranking model with fixed effects for contextual shifts that scores candidates while balancing value and content diversity. For better interpretability and understanding, we project treatments onto curated semantic marketing attributes and re-express the ranker in this space via a sign-consistent, sparse constrained Lasso, yielding per-attribute coefficients and signed contributions for visual explanations, top-k drivers, and natural-language insights. We then compute an opportunity index combining attribute importance (from the ranker) with under-expression in the current experiment to flag missing, high-impact attributes. Finally, LLMs translate ranked opportunities into concrete creative suggestions and estimate both learning and conversion potential, enabling faster, more informative, and more efficient test cycles. These components have been built into a real Adobe product, called \\textit{Experimentation Accelerator}, to provide AI-based insights and opportunities to scale experimentation for customers. We provide an evaluation of the performance of the proposed framework on some real-world experiments by Adobe business customers that validate the high quality of the generation pipeline.",
      "published": "2026-02-14",
      "updated": "2026-02-14",
      "categories": [
        "cs.AI",
        "stat.AP"
      ],
      "primaryCategory": "cs.AI",
      "pdfUrl": "https://arxiv.org/pdf/2602.13852v1",
      "arxivUrl": "http://arxiv.org/abs/2602.13852v1",
      "doi": null
    },
    {
      "id": "2602.13729",
      "title": "Semi-supervised linear regression with missing covariates",
      "authors": [
        "Benedict M. Risebrow",
        "Thomas B. Berrett"
      ],
      "abstract": "Missing values in datasets are common in applied statistics. For regression problems, theoretical work thus far has largely considered the issue of missing covariates as distinct from missing responses. However, in practice, many datasets have both forms of missingness. Motivated by this gap, we study linear regression with a labelled dataset containing missing covariates, potentially alongside an unlabelled dataset. We consider both structured (blockwise-missing) and unstructured missingness patterns, along with sparse and non-sparse regression parameters. For the non-sparse case, we provide an estimator based on imputing the missing data combined with a reweighting step. For the high-dimensional sparse case, we use a modified version of the Dantzig selector. We provide non-asymptotic upper bounds on the risk of both procedures. These are matched by several new minimax lower bounds, demonstrating the rate optimality of our estimators. Notably, even when the linear model is well-specified, our results characterise substantial differences in the minimax rates when unlabelled data is present relative to the fully supervised setting. Particular consequences of our sparse and non-sparse results include the first matching upper and lower bounds on the minimax rate for the supervised setting when either unstructured or structured missingness is present. Our theory is coupled with extensive simulations and a semi-synthetic application to the California housing dataset.",
      "published": "2026-02-14",
      "updated": "2026-02-14",
      "categories": [
        "math.ST",
        "stat.ME"
      ],
      "primaryCategory": "math.ST",
      "pdfUrl": "https://arxiv.org/pdf/2602.13729v1",
      "arxivUrl": "http://arxiv.org/abs/2602.13729v1",
      "doi": null
    },
    {
      "id": "2602.13722",
      "title": "The Accuracy Smoothness Dilemma in Prediction: a Novel Multivariate M-SSA Forecast Approach",
      "authors": [
        "Marc Wildi"
      ],
      "abstract": "Forecasting presents a complex estimation challenge, as it involves balancing multiple, often conflicting, priorities and objectives. Conventional forecast optimization methods typically emphasize a single metric--such as minimizing the mean squared error (MSE)--which may neglect other crucial aspects of predictive performance. To address this limitation, the recently developed Smooth Sign Accuracy (SSA) framework extends the traditional MSE approach by simultaneously accounting for sign accuracy, MSE, and the frequency of sign changes in the predictor. This addresses a fundamental trade-off--the so-called accuracy-smoothness (AS) dilemma--in prediction. We extend this approach to the multivariate M-SSA, leveraging the original criterion to incorporate cross-sectional information across multiple time series. As a result, the M-SSA criterion enables the integration of various design objectives related to AS forecasting performance, effectively generalizing conventional MSE-based metrics. To demonstrate its practical applicability and versatility, we explore the application of the M-SSA in three primary domains: forecasting, real-time signal extraction (nowcasting), and smoothing. These case studies illustrate the framework's capacity to adapt to different contexts while effectively managing inherent trade-offs in predictive modelling.",
      "published": "2026-02-14",
      "updated": "2026-02-14",
      "categories": [
        "econ.EM",
        "stat.ME"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.13722v1",
      "arxivUrl": "http://arxiv.org/abs/2602.13722v1",
      "doi": null
    },
    {
      "id": "2602.13635",
      "title": "Backward Smoothing versus Fixed-Lag Smoothing in Particle Filters",
      "authors": [
        "Genshiro Kitagawa"
      ],
      "abstract": "Particle smoothing enables state estimation in nonlinear and non-Gaussian state-space models, but its practical use is often limited by high computational cost. Backward smoothing methods such as the Forward Filter Backward Smoother (FFBS) and its marginal form (FFBSm) can achieve high accuracy, yet typically require quadratic computational complexity in the number of particles. This paper examines the accuracy--computational cost trade-offs of particle smoothing methods through a trend-estimation example. Fixed-lag smoothing, FFBS, and FFBSm are compared under Gaussian and heavy-tailed (Cauchy-type) system noise, with particular attention to O(m) approximations of FFBSm based on subsampling and local neighborhood restrictions. The results show that FFBS and FFBSm outperform fixed-lag smoothing at a fixed particle number, while fixed-lag smoothing often achieves higher accuracy under equal computational time. Moreover, efficient FFBSm approximations are effective for Gaussian transitions but become less advantageous for heavy-tailed dynamics.",
      "published": "2026-02-14",
      "updated": "2026-02-14",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.13635v1",
      "arxivUrl": "http://arxiv.org/abs/2602.13635v1",
      "doi": null
    },
    {
      "id": "2602.13619",
      "title": "Locally Private Parametric Methods for Change-Point Detection",
      "authors": [
        "Anuj Kumar Yadav",
        "Cemre Cadir",
        "Yanina Shkel",
        "Michael Gastpar"
      ],
      "abstract": "We study parametric change-point detection, where the goal is to identify distributional changes in time series, under local differential privacy. In the non-private setting, we derive improved finite-sample accuracy guarantees for a change-point detection algorithm based on the generalized log-likelihood ratio test, via martingale methods. In the private setting, we propose two locally differentially private algorithms based on randomized response and binary mechanisms, and analyze their theoretical performance. We derive bounds on detection accuracy and validate our results through empirical evaluation. Our results characterize the statistical cost of local differential privacy in change-point detection and show how privacy degrades performance relative to a non-private benchmark. As part of this analysis, we establish a structural result for strong data processing inequalities (SDPI), proving that SDPI coefficients for Rényi divergences and their symmetric variants (Jeffreys-Rényi divergences) are achieved by binary input distributions. These results on SDPI coefficients are also of independent interest, with applications to statistical estimation, data compression, and Markov chain mixing.",
      "published": "2026-02-14",
      "updated": "2026-02-14",
      "categories": [
        "stat.ML",
        "cs.IT",
        "cs.LG",
        "stat.ME"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2602.13619v1",
      "arxivUrl": "http://arxiv.org/abs/2602.13619v1",
      "doi": null
    },
    {
      "id": "2602.13538",
      "title": "Empirical Bayes data integreation for multi-response regression",
      "authors": [
        "Antik Chakraborty",
        "Fei Xue"
      ],
      "abstract": "Motivated by applications in tissue-wide association studies (TWAS), we develop a flexible and theoretically grounded empirical Bayes approach for integrating %vector-valued outcomes data obtained from different sources. We propose a linear shrinkage estimator that effectively shrinks singular values of a data matrix. This problem is closely connected to estimating covariance matrices under a specific loss, for which we develop asymptotically optimal estimators. The basic linear shrinkage estimator is then extended to a local linear shrinkage estimator, offering greater flexibility. Crucially, the proposed method works under sparse/dense or low-rank/non low-rank parameter settings unlike well-known sparse or reduced rank estimators in the literature. Furthermore, the empirical Bayes approach offers greater scalability in computation compared to intensive full Bayes procedures. The method is evaluated through an extensive set of numerical experiments, and applied to a real TWAS data obtained from the Genotype-Tissue Expression (GTEx) project.",
      "published": "2026-02-14",
      "updated": "2026-02-14",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.13538v1",
      "arxivUrl": "http://arxiv.org/abs/2602.13538v1",
      "doi": "10.5705/ss.202025.0115"
    },
    {
      "id": "2602.13537",
      "title": "Cluster-Robust Inference for Quadratic Forms",
      "authors": [
        "Michal Kolesár",
        "Pengjin Min",
        "Wenjie Wang",
        "Yichong Zhang"
      ],
      "abstract": "This paper studies inference for quadratic forms of linear regression coefficients with clustered data and many covariates. Our framework covers three important special cases: instrumental variables regression with many instruments and controls, inference on variance components, and testing multiple restrictions in a linear regression. Naïve plug-in estimators are known to be biased. We study a leave-one-cluster-out estimator that is unbiased, and provide sufficient conditions for its asymptotic normality. For inference, we establish the consistency of a leave-three-cluster-out variance estimator under primitive conditions. In addition, we develop a novel leave-two-cluster-out variance estimator that is computationally simpler and guaranteed to be conservative under weaker conditions. Our analysis allows cluster sizes to diverge with the sample size, accommodates strong within-cluster dependence, and permits the dimension of the covariates to diverge with the sample size, potentially at the same rate.",
      "published": "2026-02-14",
      "updated": "2026-02-14",
      "categories": [
        "econ.EM"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.13537v1",
      "arxivUrl": "http://arxiv.org/abs/2602.13537v1",
      "doi": null
    },
    {
      "id": "2602.13533",
      "title": "Estimation and Inference of the Win Ratio for Two Hierarchical Endpoints Subject to Censoring and Missing Data",
      "authors": [
        "Yi Liu",
        "Huiman Barnhart",
        "Sean O'Brien",
        "Yuliya Lokhnygina",
        "Roland A. Matsouaka"
      ],
      "abstract": "The win ratio (WR) is a widely used metric to compare treatments in randomized clinical trials with hierarchically ordered endpoints. Counting-based approaches, such as Pocock's algorithm, are the standard for WR estimation. However, this algorithm treats participants with censored or missing data inadequately, which may lead to biased and inefficient estimates, particularly in the presence of heterogeneous censoring or missing data between treatment groups. Although recent extensions have addressed some of these limitations for hierarchical time-to-event endpoints, no existing methods -- aside from the computationally intensive multiple imputation approach -- can accommodate settings that include non-survival endpoints that are subject to missing data. In this paper, we propose a simple nonparametric maximum likelihood estimator (NPMLE) of WR for two hierarchical endpoints that are subject to censoring and missing data. Our method uses all observed data, avoids strong parametric assumptions, and comes with a closed-form asymptotic variance estimator. We demonstrate its performance using simulation studies and two data examples, based on the HEART-FID and ISCHEMIA trials. The proposed method provides a consistent estimator, improves estimation efficiency, and is robust under non-informative censoring and missing at random (MAR) assumptions, offering a flexible alternative to existing WR estimation methods. A user-friendly R package, WinRS, is available to facilitate implementation.",
      "published": "2026-02-14",
      "updated": "2026-02-14",
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.AP"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.13533v1",
      "arxivUrl": "http://arxiv.org/abs/2602.13533v1",
      "doi": null
    },
    {
      "id": "2602.13518",
      "title": "Towards Semiparametric Bandwidth Selectors for Kernel Density Estimators",
      "authors": [
        "Nils Lid Hjort"
      ],
      "abstract": "There is an intense and partly recent literature focussing on the problem of selecting the bandwidth parameter for kernel density estimators. Available methods are largely `very nonparametric', in the sense of not requiring any knowledge about the underlying density, or `very parametric', like the normality-based reference rule. This report aims at widening the scope towards the inclusion of many semiparametric bandwidth selectors, via Hermite type expansions aroundthe normal distribution. The resulting bandwidths may be seen as carrying out suitable corrections on the normal reference rule, requiring a low number of extra coefficients to be estimated from data. The present report introduces and discusses some basic ideas and develops the necessary initial theory, but modestly chooses to stop short of giving precise recommendations for specific procedures among the many possible constructions. This will require some further analysis, numerical work, and some simulation-based exploration.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.13518v1",
      "arxivUrl": "http://arxiv.org/abs/2602.13518v1",
      "doi": null
    },
    {
      "id": "2602.13475",
      "title": "Efficient and Debiased Learning of Average Hazard Under Non-Proportional Hazards",
      "authors": [
        "Xiang Meng",
        "Lu Tian",
        "Kenneth Kehl",
        "Hajime Uno"
      ],
      "abstract": "The hazard ratio from the Cox proportional hazards model is a ubiquitous summary of treatment effect. However, when hazards are non-proportional, the hazard ratio can lose a stable causal interpretation and become study-dependent because it effectively averages time-varying effects with weights determined by follow-up and censoring. We consider the average hazard (AH) as an alternative causal estimand: a population-level person-time event rate that remains well-defined and interpretable without assuming proportional hazards. Although AH can be estimated nonparametrically and regression-style adjustments have been proposed, existing approaches do not provide a general framework for flexible, high-dimensional nuisance estimation with valid sqrt{n} inference. We address this gap by developing a semiparametric, doubly robust framework for covariate-adjusted AH. We establish pathwise differentiability of AH in the nonparametric model, derive its efficient influence function, and construct cross-fitted, debiased estimators that leverage machine learning for nuisance estimation while retaining asymptotically normal, sqrt{n}-consistent inference under mild product-rate conditions. Simulations demonstrate that the proposed estimator achieves small bias and near-nominal confidence-interval coverage across proportional and non-proportional hazards settings, including crossing-hazards regimes where Cox-based summaries can be unstable. We illustrate practical utility in comparative effectiveness research by comparing immunotherapy regimens for advanced melanoma using SEER-Medicare linked data.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "stat.ME",
        "stat.AP",
        "stat.ML"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.13475v1",
      "arxivUrl": "http://arxiv.org/abs/2602.13475v1",
      "doi": null
    },
    {
      "id": "2602.13453",
      "title": "Post-Matching Two-Way Fixed Effects Estimation",
      "authors": [
        "Yihong Liu",
        "Gonzalo Vazquez-Bare"
      ],
      "abstract": "When estimating treatment effects with two-way fixed effects (2WFE) models, researchers often use matching as a pre-processing step when the parallel trends assumption is thought to hold conditionally on covariates. Specifically, in a first step, each treated unit is matched to one or more untreated units based on observed time-invariant covariates. In the second step, treatment effects are estimated with a 2WFE regression in the matched sample, reweighting the untreated units by the number of times they are matched. We formally analyze this common practice and highlight two problems. First, when different treatment cohorts enter treatment in different time periods, the post-matching 2WFE estimator that pools all treated cohorts has an asymptotic bias, even when the treatment effect is constant across units and over time. Second, failing to account for the variability introduced by the matching procedure yields invalid standard error estimators, which can be biased upwards or downwards depending on the data generating process. We propose simple post-matching difference-in-differences estimators that compare each treated cohort to the never-treated separately, instead of pooling all treated cohorts. We provide conditions under which these estimators are consistent for well-defined causal parameters, and derive valid standard errors that account for the matching step. We illustrate our results with simulations and with an empirical application.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "econ.EM"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.13453v1",
      "arxivUrl": "http://arxiv.org/abs/2602.13453v1",
      "doi": null
    },
    {
      "id": "2602.13450",
      "title": "Inference From Random Restarts",
      "authors": [
        "Moeen Nehzati",
        "Diego Cussen"
      ],
      "abstract": "Algorithms for computing equilibria, optima, and fixed points in nonconvex problems often depend sensitively on practitioner-chosen initial conditions. When uniqueness of a solution is of interest, a common heuristic is to run such algorithms from many randomly selected initial conditions and to interpret repeated convergence to the same output as evidence of a unique solution or a dominant basin of attraction. Despite its widespread use, this practice lacks a formal inferential foundation. We provide a simple probabilistic framework for interpreting such numerical evidence. First, we give sufficient conditions under which an algorithm's terminal output is a measurable function of its initial condition, allowing probabilistic reasoning over outcomes. Second, we provide sufficient conditions ensuring that an algorithm admits only finitely many possible terminal outcomes. While these conditions may be difficult to verify on a case-by-case basis, we give simple sufficient conditions for broad classes of problems under which almost all instances admit only finitely many outcomes (in the sense of prevalence). Standard algorithms such as gradient descent and damped fixed-point iteration applied to sufficiently smooth functions satisfy these conditions. Within this framework, repeated solver runs correspond to independent samples from the induced distribution over outcomes. We adopt a Bayesian approach to infer basin sizes and the probability of solution uniqueness from repeated identical outputs, and we establish convergence rates for the resulting posterior beliefs. Finally, we apply our framework to settings in the existing industrial organization literature, where random-restart heuristics are used. Our results formalize and qualify these arguments, clarifying when repeated convergence provides meaningful evidence for uniqueness and when it does not.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "econ.EM",
        "stat.AP",
        "stat.ME"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.13450v1",
      "arxivUrl": "http://arxiv.org/abs/2602.13450v1",
      "doi": null
    },
    {
      "id": "2602.13442",
      "title": "Measuring Neural Network Complexity via Effective Degrees of Freedom",
      "authors": [
        "Jia Zhou",
        "Douglas Landsittel"
      ],
      "abstract": "Quantifying the complexity of feed-forward neural networks (FFNNs) remains challenging due to their nonlinear, hierarchical structure and numerous parameters. We apply generalized degrees of freedom (GDF) to measure model complexity in FFNNs with binary outcomes, adapting the algorithm for discrete responses. We compare GDF with both the effective number of parameters derived via log-likelihood cross-validation and the null degrees of freedom of Landsittel et al. Through simulation studies and a real data analysis, we demonstrate that GDF provides a robust assessment of model complexity for neural network models, as it depends only on the sensitivity of fitted values to perturbations in the observed responses rather than on assumptions about the likelihood. In contrast, cross-validation-based estimates of model complexity and the null degrees of freedom rely on the correctness of the assumed likelihood and may exhibit substantial variability. We find that GDF, cross-validation-based measures, and null degrees of freedom yield similar assessments of model complexity only when the fitted model adequately represents the data-generating mechanism. These findings highlight GDF as a stable and broadly applicable measure of model complexity for neural networks in statistical modeling.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "stat.ME",
        "stat.ML"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.13442v1",
      "arxivUrl": "http://arxiv.org/abs/2602.13442v1",
      "doi": null
    },
    {
      "id": "2602.13419",
      "title": "Protect$^*$: Steerable Retrosynthesis through Neuro-Symbolic State Encoding",
      "authors": [
        "Shreyas Vinaya Sathyanarayana",
        "Shah Rahil Kirankumar",
        "Sharanabasava D. Hiremath",
        "Bharath Ramsundar"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable potential in scientific domains like retrosynthesis; yet, they often lack the fine-grained control necessary to navigate complex problem spaces without error. A critical challenge is directing an LLM to avoid specific, chemically sensitive sites on a molecule - a task where unconstrained generation can lead to invalid or undesirable synthetic pathways. In this work, we introduce Protect$^*$, a neuro-symbolic framework that grounds the generative capabilities of Large Language Models (LLMs) in rigorous chemical logic. Our approach combines automated rule-based reasoning - using a comprehensive database of 55+ SMARTS patterns and 40+ characterized protecting groups - with the generative intuition of neural models. The system operates via a hybrid architecture: an ``automatic mode'' where symbolic logic deterministically identifies and guards reactive sites, and a ``human-in-the-loop mode'' that integrates expert strategic constraints. Through ``active state tracking,'' we inject hard symbolic constraints into the neural inference process via a dedicated protection state linked to canonical atom maps. We demonstrate this neuro-symbolic approach through case studies on complex natural products, including the discovery of a novel synthetic pathway for Erythromycin B, showing that grounding neural generation in symbolic logic enables reliable, expert-level autonomy.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "q-bio.BM"
      ],
      "primaryCategory": "q-bio.QM",
      "pdfUrl": "https://arxiv.org/pdf/2602.13419v1",
      "arxivUrl": "http://arxiv.org/abs/2602.13419v1",
      "doi": null
    },
    {
      "id": "2602.13398",
      "title": "Accelerated Discovery of Cryoprotectant Cocktails via Multi-Objective Bayesian Optimization",
      "authors": [
        "Daniel Emerson",
        "Nora Gaby-Biegel",
        "Purva Joshi",
        "Yoed Rabin",
        "Rebecca D. Sandlin",
        "Levent Burak Kara"
      ],
      "abstract": "Designing cryoprotectant agent (CPA) cocktails for vitrification is challenging because formulations must be concentrated enough to suppress ice formation yet non-toxic enough to preserve cell viability. This tradeoff creates a large, multi-objective design space in which traditional discovery is slow, often relying on expert intuition or exhaustive experimentation. We present a data-efficient framework that accelerates CPA cocktail design by combining high-throughput screening with an active-learning loop based on multi-objective Bayesian optimization. From an initial set of measured cocktails, we train probabilistic surrogate models to predict concentration and viability and quantify uncertainty across candidate formulations. We then iteratively select the next experiments by prioritizing cocktails expected to improve the Pareto front, maximizing expected Pareto improvement under uncertainty, and update the models as new assay results are collected. Wet-lab validation shows that our approach efficiently discovers cocktails that simultaneously achieve high CPA concentrations and high post-exposure viability. Relative to a naive strategy and a strong baseline, our method improves dominated hypervolume by 9.5\\% and 4.5\\%, respectively, while reducing the number of experiments needed to reach high-quality solutions. In complementary synthetic studies, it recovers a comparably strong set of Pareto-optimal solutions using only 30\\% of the evaluations required by the prior state-of-the-art multi-objective approach, which amounts to saving approximately 10 weeks of experimental time. Because the framework assumes only a suitable assay and defined formulation space, it can be adapted to different CPA libraries, objective definitions, and cell lines to accelerate cryopreservation development.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "cs.LG",
        "q-bio.QM"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2602.13398v1",
      "arxivUrl": "http://arxiv.org/abs/2602.13398v1",
      "doi": null
    },
    {
      "id": "2602.13380",
      "title": "Robust Design in the Presence of Aleatoric and Epistemic Uncertainty",
      "authors": [
        "Luis G. Crespo"
      ],
      "abstract": "This paper proposes strategies for designing a system whose computational model is subject to aleatory and epistemic uncertainty. Aleatory variables, which are caused by randomness in physical parameters, are draws from a possibly unknown distribution; whereas epistemic variables, which are caused by ignorance in the value of fixed parameters, are free to take any value in a bounded set. Chance-constrained formulations enforcing the system requirements at a finite number of realizations of the uncertain parameters are proposed. These formulations trade off a lower objective value against a reduced robustness by eliminating an optimally chosen subset of such realizations. Risk-aware designs are obtained by accounting for the severity of the requirement violations resulting from this elimination process. Furthermore, we propose a computationally efficient design approach in which the training dataset is sequentially updated according to the results of high-fidelity reliability analyses of suboptimal designs. Robustness is evaluated by using Monte Carlo analysis and Robust Scenario Theory, with the latter approach accounting for the infinitely many values that the epistemic variables can take.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.13380v1",
      "arxivUrl": "http://arxiv.org/abs/2602.13380v1",
      "doi": null
    },
    {
      "id": "2602.13158",
      "title": "A new mixture model for spatiotemporal exceedances with flexible tail dependence",
      "authors": [
        "Ryan Li",
        "Brian J. Reich",
        "Emily C. Hector",
        "Reetam Majumder"
      ],
      "abstract": "We propose a new model and estimation framework for spatiotemporal streamflow exceedances above a threshold that flexibly captures asymptotic dependence and independence in the tail of the distribution. We model streamflow using a mixture of processes with spatial, temporal and spatiotemporal asymptotic dependence regimes. A censoring mechanism allows us to use only observations above a threshold to estimate marginal and joint probabilities of extreme events. As the likelihood is intractable, we use simulation-based inference powered by random forests to estimate model parameters from summary statistics of the data. Simulations and modeling of streamflow data from the U.S. Geological Survey illustrate the feasibility and practicality of our approach.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.13158v1",
      "arxivUrl": "http://arxiv.org/abs/2602.13158v1",
      "doi": null
    },
    {
      "id": "2602.13152",
      "title": "Detecting Parameter Instabilities in Functional Concurrent Linear Regression",
      "authors": [
        "Rupsa Basu",
        "Sven Otto"
      ],
      "abstract": "We develop methodology to detect structural breaks in the slope function of a concurrent functional linear regression model for functional time series in $C[0,1]$. Our test is based on a CUSUM process of regressor-weighted OLS residual functions. To accommodate both global and local changes, we propose $L^2$- and sup-norm versions, with the sup-norm particularly sensitive to spike-like changes. Under Hölder regularity and weak dependence conditions, we establish a functional strong invariance principle, derive the asymptotic null distribution, and show that the resulting tests are consistent against a broad class of alternatives with breaks in the slope function. Simulation studies illustrate finite-sample size and power. We apply the method to sports data obtained via body-worn sensors from running athletes, focusing on hip and knee joint-angle trajectories recorded during a fatiguing run. As fatigue accumulates, runners adapt their movement patterns, and sufficiently pronounced adjustments are expected to appear as a change point in the regression relationship. In this manner, we illustrate how the proposed tests support interpretable inference for biomechanical functional time series.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.13152v1",
      "arxivUrl": "http://arxiv.org/abs/2602.13152v1",
      "doi": null
    },
    {
      "id": "2602.13121",
      "title": "LinkedNN: a neural model of linkage disequilibrium decay for recent effective population size inference",
      "authors": [
        "Chris C R Smith"
      ],
      "abstract": "Summary: A bioinformatics tool is presented for estimating recent effective population size by using a neural network to automatically compute linkage disequilibrium-related features as a function of genomic distance between polymorphisms. The new method outperforms existing deep learning and summary statistic-based approaches using relatively few sequenced individuals and variant sites, making it particularly valuable for molecular ecology applications with sparse, unphased data. Availability and implementation: The program is available as an easily installable Python package with documentation here: https://pypi.org/project/linkedNN/. The open source code is available from: https://github.com/the-smith-lab/LinkedNN.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "q-bio.PE",
        "q-bio.QM"
      ],
      "primaryCategory": "q-bio.PE",
      "pdfUrl": "https://arxiv.org/pdf/2602.13121v1",
      "arxivUrl": "http://arxiv.org/abs/2602.13121v1",
      "doi": null
    },
    {
      "id": "2602.13098",
      "title": "Barron-Wiener-Laguerre models",
      "authors": [
        "Rahul Manavalan",
        "Filip Tronarp"
      ],
      "abstract": "We propose a probabilistic extension of Wiener-Laguerre models for causal operator learning. Classical Wiener-Laguerre models parameterize stable linear dynamics using orthonormal Laguerre bases and apply a static nonlinear map to the resulting features. While structurally efficient and interpretable, they provide only deterministic point estimates. We reinterpret the nonlinear component through the lens of Barron function approximation, viewing two-layer networks, random Fourier features, and extreme learning machines as discretizations of integral representations over parameter measures. This perspective naturally admits Bayesian inference on the nonlinear map and yields posterior predictive uncertainty. By combining Laguerre-parameterized causal dynamics with probabilistic Barron-type nonlinear approximators, we obtain a structured yet expressive class of causal operators equipped with uncertainty quantification. The resulting framework bridges classical system identification and modern measure-based function approximation, providing a principled approach to time-series modeling and nonlinear systems identification.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "stat.ME",
        "cs.LG"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.13098v1",
      "arxivUrl": "http://arxiv.org/abs/2602.13098v1",
      "doi": null
    },
    {
      "id": "2602.12992",
      "title": "Stratified Sampling for Model-Assisted Estimation with Surrogate Outcomes",
      "authors": [
        "Reagan Mozer",
        "Nicole E. Pashley",
        "Luke Miratrix"
      ],
      "abstract": "In many randomized trials, outcomes such as essays or open-ended responses must be manually scored as a preliminary step to impact analysis, a process that is costly and limiting. Model-assisted estimation offers a way to combine surrogate outcomes generated by machine learning or large language models with a human-coded subset, yet typical implementations use simple random sampling and therefore overlook systematic variation in surrogate prediction error. We extend this framework by incorporating stratified sampling to more efficiently allocate human coding effort. We derive the exact variance of the stratified model-assisted estimator, characterize conditions under which stratification improves precision, and identify a Neyman-type optimal allocation rule that oversamples strata with larger residual variance. We evaluate our methods through a comprehensive simulation study to assess finite-sample performance. Overall, we find stratification consistently improves efficiency when surrogate prediction errors exhibit structured bias or heteroskedasticity. We also present two empirical applications, one using data from an education RCT and one using a large observational corpus, to illustrate how these methods can be implemented in practice using ChatGPT-generated surrogate outcomes. Overall, this framework provides a practical design-based approach for leveraging surrogate outcomes and strategically allocating human coding effort to obtain unbiased estimates with greater efficiency. While motivated by text-as-data applications, the methodology applies broadly to any setting where outcome measurement is costly or prohibitive, and can be applied to comparisons across groups or estimating the mean of a single group.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.12992v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12992v1",
      "doi": null
    },
    {
      "id": "2602.12974",
      "title": "Statistical Opportunities in Neuroimaging",
      "authors": [
        "Jian Kang",
        "Thomas Nichols",
        "Lexin Li",
        "Martin A. Lindquist",
        "Hongtu Zhu"
      ],
      "abstract": "Neuroimaging has profoundly enhanced our understanding of the human brain by characterizing its structure, function, and connectivity through modalities like MRI, fMRI, EEG, and PET. These technologies have enabled major breakthroughs across the lifespan, from early brain development to neurodegenerative and neuropsychiatric disorders. Despite these advances, the brain is a complex, multiscale system, and neuroimaging measurements are correspondingly high-dimensional. This creates major statistical challenges, including measurement noise, motion-related artifacts, substantial inter-subject and site/scanner variability, and the sheer scale of modern studies. This paper explores statistical opportunities and challenges in neuroimaging across four key areas: (i) brain development from birth to age 20, (ii) the adult and aging brain, (iii) neurodegeneration and neuropsychiatric disorders, and (iv) brain encoding and decoding. After a quick tutorial on major imaging technologies, we review cutting-edge studies, underscore data and modeling challenges, and highlight research opportunities for statisticians. We conclude by emphasizing that close collaboration among statisticians, neuroscientists, and clinicians is essential for translating neuroimaging advances into improved diagnostics, deeper mechanistic insight, and more personalized treatments.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "stat.AP",
        "cs.CV",
        "stat.ME"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.12974v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12974v1",
      "doi": null
    },
    {
      "id": "2602.12900",
      "title": "A unified testing approach for log-symmetry using Fourier methods",
      "authors": [
        "Ganesh Vishnu Avhad",
        "Sudheesh K. Kattumannil"
      ],
      "abstract": "Continuous and strictly positive data that exhibit skewness and outliers frequently arise in many applied disciplines. Log-symmetric distributions provide a flexible framework for modeling such data. In this article, we develop new goodness-of-fit tests for log-symmetric distributions based on a recent characterization. These tests utilize the characteristic function as a novel tool and are constructed using an $L^2$-type weighted distance measure. The asymptotic properties of the resulting test statistic are studied. The finite-sample performance of the proposed method is assessed via Monte Carlo simulations and compared with existing procedures. The results under a range of alternative distributions indicate superior empirical power, while the proposed test also exhibits substantial computational efficiency compared to existing methods. The methodology is further illustrated using real data sets to demonstrate practical applicability.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.12900v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12900v1",
      "doi": null
    },
    {
      "id": "2602.12845",
      "title": "Small area estimation using incomplete auxiliary information",
      "authors": [
        "Donatas Šlevinskas",
        "Ieva Burakauskaitė",
        "Andrius Čiginas"
      ],
      "abstract": "Auxiliary information is increasingly available from administrative and other data sources, but it is often incomplete and of non-probability origin. We propose a two-step small area estimation approach in which the first step relies on design-based model calibration and exploits a large non-probability source providing a noisy proxy of the study variable for only part of the population. A unit-level measurement-error working model is fitted on the linked overlap between the probability survey and the external source, and its predictions are incorporated through domain-specific model-calibration constraints to obtain approximately design-unbiased domain totals. These totals and their variance estimates are then used in a Fay-Herriot area-level model with exactly known covariates to produce empirical best linear unbiased predictors. The approach is demonstrated in three enterprise survey settings from official statistics by integrating probability sample data with (i) administrative records, (ii) a cut-off data source, and (iii) web-scraped online information. Empirical comparisons show consistent improvements in domain-level precision over direct estimation and over a Fay-Herriot benchmark that directly incorporates the proxy information as an error-prone covariate. These gains are achieved without modeling the selection mechanism of the non-probability sample.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.12845v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12845v1",
      "doi": null
    },
    {
      "id": "2602.12842",
      "title": "Some bivariate distributions on a discrete torus with application to wind direction datasets",
      "authors": [
        "Brajesh Kumar Dhakad",
        "Jayant Jha",
        "Debepsita Mukherjee"
      ],
      "abstract": "Many datasets are observed on a finite set of equally spaced directions instead of the exact angles, such as the wind direction data. However, in the statistical literature, bivariate models are only available for continuous circular random variables. This article presents two bivariate circular distributions, namely bivariate wrapped geometric (BWG) and bivariate generalized wrapped geometric (BGWG), for analyzing bivariate discrete circular data. We consider wrapped geometric distributions and a trigonometric function to construct the models. The models are analytically tractable due to the exact closed-form expressions for the trigonometric moments. We thoroughly discuss the distributional properties of the models, including the interpretation of parameters and dependence structure. The estimation methodology based on maximizing the likelihood functions is illustrated for simulated datasets. Finally, the proposed distributions are utilized to analyze pairwise wind direction measurements obtained at different stations in India, and the interpretations for the fitted models are briefly discussed.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.12842v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12842v1",
      "doi": null
    },
    {
      "id": "2602.12782",
      "title": "Empirical Validation of a Dual-Defense Mechanism Reshaping Wholesale Electricity Price Dynamics in Singapore",
      "authors": [
        "Huang Zhenyu",
        "Yuan Zhao"
      ],
      "abstract": "While ex-ante screening and static price caps are global standards for mitigating price volatility, Singapore's electricity market employs a unique dual-defense mechanism integrating vesting contracts (VC) with a temporary price cap (TPC). Using high-frequency data from 2021 to 2024, this paper evaluates this mechanism and yields three primary findings. First, a structural trade-off exists within the VC framework: while VC quantity (VCQ) suppresses average prices, it paradoxically exacerbates instability via liquidity squeezes. Conversely, VC price (VCP) functions as a tail-risk anchor, dominating at extreme quantiles where VCQ efficacy wanes. Second, a structural break around the 2023 reform reveals a fundamental re-mapping of price dynamics; the previously positive pass-through from offer ratios to clearing prices was largely neutralized post-reform. Furthermore, diagnostics near the TPC threshold show no systematic evidence of strategic bid shading, confirming the TPC's operational integrity. Third, the dual-defense mechanism exhibits a critical synergy that resolves the volatility trade-off. The TPC reverses the volatility penalty of high VCQ, shifting the elasticity of conditional volatility from a destabilizing 0.636 to a stabilizing -0.213. This synergy enables the framework to enhance tail-risk control while eliminating liquidity-related stability costs. We conclude that this dual-defense mechanism successfully decouples price suppression from liquidity risks, thereby maximizing market stability.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "eess.SY",
        "econ.EM"
      ],
      "primaryCategory": "eess.SY",
      "pdfUrl": "https://arxiv.org/pdf/2602.12782v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12782v1",
      "doi": null
    },
    {
      "id": "2602.12730",
      "title": "EMERALD-UI: An interactive web application to unveil novel protein biology hidden in the suboptimal-alignment space",
      "authors": [
        "Andrei Preoteasa",
        "Andreas Grigorjew",
        "Alexandru I. Tomescu",
        "Hajk-Georg Drost"
      ],
      "abstract": "Life over the past four billion years has been shaped by proteins and their capacity to assemble into three dimensional conformations. Protein sequence alignments have been the enabling technology for exploring the evolution and functional adaptation of proteins across the tree of life. Recent advancements in scaling the prediction of three dimensional protein structures from primary sequence alone, revealed that different modes of conservation and function operate on the sequence and structure level. This difference in protein conservation patterns and their underlying functional change that could emerge in suboptimal alignment configurations is often ignored in optimal protein alignment approaches. We introduce EMERALD-UI, an open-source interactive web application which is designed to reveal unexplored biology by visualising stable structural conformations or protein regions hidden in the suboptimal alignment space. Availability: EMERALD-UI is available at https://algbio.github.io/emerald-ui/. Contact: hdrost001@dundee.ac.uk or alexandru.tomescu@helsinki.fi.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "q-bio.QM"
      ],
      "primaryCategory": "q-bio.QM",
      "pdfUrl": "https://arxiv.org/pdf/2602.12730v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12730v1",
      "doi": null
    },
    {
      "id": "2602.12710",
      "title": "On the relation between Global VAR Models and Matrix Time Series Models with Multiple Terms",
      "authors": [
        "Dietmar Bauer Kurtulus Kidik"
      ],
      "abstract": "Matrix valued time series (MaTS) and global vector autoregressive (GVAR) models both impose restrictions on the general VAR for multidimensional data sets, in order to bring down the number of parameters. Both models are motivated from a different viewpoint such that on first sight they do not have much in common. When investigating the models more closely, however, one notices many connections between the two model sets. This paper investigates the relations between the restrictions imposed by the two models. We show that under appropriate restrictions in both models we obtain a joint framework allowing to gain insight into the nature of GVARs from the viewpoint of MaTS.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "math.ST",
        "stat.AP"
      ],
      "primaryCategory": "math.ST",
      "pdfUrl": "https://arxiv.org/pdf/2602.12710v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12710v1",
      "doi": null
    },
    {
      "id": "2602.12702",
      "title": "Modelling multivariate ordinal time series using pairwise likelihood",
      "authors": [
        "Anna Nalpantidi",
        "Dimitris Karlis"
      ],
      "abstract": "We assume that we have multiple ordinal time series and we would like to specify their joint distribution. In general it is difficult to create multivariate distribution that can be easily used to jointly model ordinal variables and the problem becomes even more complex in the case of time series, since we have to take into consideration not only the autocorrelation of each time series and the dependence between time series, but also cross-correlation. Starting from the simplest case of two ordinal time series, we propose using copulas to specify their joint distribution. We extend our approach in higher dimensions, by approximating full likelihood with composite likelihood and especially conditional pairwise likelihood, where each bivariate model is specified by copulas. We suggest maximizing each bivariate model independently to avoid computational issues and synthesize individual estimates using weighted mean. Weights are related to the Hessian matrix of each bivariate model. Simulation studies showed that model fits well under different sample sizes. Forecasting approach is also discussed. A small real data application about unemployment state of different countries of European Union is presented to illustrate our approach.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.12702v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12702v1",
      "doi": null
    },
    {
      "id": "2602.12682",
      "title": "A Causal Framework for Quantile Residual Lifetime",
      "authors": [
        "Taekwon Hong",
        "Woojung Bae",
        "Sang Kyu Lee",
        "Dongrak Choi",
        "Jong-Hyeon Jeong"
      ],
      "abstract": "Estimating prognosis conditional on surviving an initial high-risk period is crucial in clinical research. Yet, standard metrics such as hazard ratios are often difficult to interpret, while mean-based summaries are sensitive to outliers and censoring. We propose a formal causal framework for estimating quantiles of residual lifetime among individuals surviving to a landmark time $t_0$. Our primary estimand, the \"Observed Survivor Quantile Contrast\" (OSQC), targets pragmatic prognostic differences within the observed survivor population. To estimate the OSQC, we develop a doubly robust estimator that combines propensity scores, outcome regression, and inverse probability of censoring weights, ensuring consistency under confounding and informative censoring provided that the censoring model is correctly specified and at least one additional nuisance model is correctly specified. Recognizing that the OSQC conflates causal efficacy and compositional selection, we also introduce a reweighting-based supplementary estimator for the \"Principal Survivor Quantile Contrast\" (PSQC) to disentangle these mechanisms under stronger assumptions. Extensive simulations demonstrate the robustness of the proposed estimators and clarify the role of post-treatment selection. We illustrate the framework using data from the SUPPORT study to assess the impact of right heart catheterization on residual lifetime among intensive care unit survivors, and from the NSABP B-14 trial to examine post-surgical prognosis under adjuvant tamoxifen therapy across multiple landmark times.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.12682v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12682v1",
      "doi": null
    },
    {
      "id": "2602.12577",
      "title": "Conjugate Variational Inference for Large Mixed Multinomial Logit Models and Consumer Choice",
      "authors": [
        "Weiben Zhang",
        "Ruben Loaiza-Maya",
        "Michael Stanley Smith",
        "Worapree Maneesoonthorn"
      ],
      "abstract": "Heterogeneity in multinomial choice data is often accounted for using logit models with random coefficients. Such models are called \"mixed\", but they can be difficult to estimate for large datasets. We review current Bayesian variational inference (VI) methods that can do so, and propose a new VI method that scales more effectively. The key innovation is a step that updates efficiently a Gaussian approximation to the conditional posterior of the random coefficients, addressing a bottleneck within the variational optimization. The approach is used to estimate three types of mixed logit models: standard, nested and bundle variants. We first demonstrate the improvement of our new approach over existing VI methods using simulations. Our method is then applied to a large scanner panel dataset of pasta choice. We find consumer response to price and promotion variables exhibits substantial heterogeneity at the grocery store and product levels. Store size, premium and geography are found to be drivers of store level estimates of price elasticities. Extension to bundle choice with pasta sauce improves model accuracy further. Predictions from the mixed models are more accurate than those from fixed coefficients equivalents, and our VI method provides insights in circumstances which other methods find challenging.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "stat.ME",
        "stat.AP"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.12577v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12577v1",
      "doi": null
    },
    {
      "id": "2602.12504",
      "title": "Toggling the Defiers to Relax Monotonicity: The Difference-in-Instrumental-Variables Estimand",
      "authors": [
        "Johann Caro-Burnett"
      ],
      "abstract": "Standard instrumental variables (IV) methods identify a Local Average Treatment Effect under monotonicity, which rules out defiers. In many empirical environments, however, distinct instruments may induce heterogeneous and even opposing behavioral responses. This paper introduces the Difference-in-Instrumental-Variables (DIIV) estimand, which exploits two instruments with opposing compliance patterns to recover a point-identified and behaviorally interpretable causal effect without imposing monotonicity. The estimand yields a convex combination of the marginal treatment effects on compliers and defiers, with weights reflecting differential shifts in treatment take-up across instruments. When monotonicity holds, DIIV coincides with the standard IV estimand. The approach can be implemented using simple linear transformations and standard two-stage least squares procedures. Applications using replication data illustrate its applicability in practice.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "econ.EM"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.12504v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12504v1",
      "doi": null
    },
    {
      "id": "2602.12490",
      "title": "Transformer-based CoVaR: Systemic Risk in Textual Information",
      "authors": [
        "Junyu Chen",
        "Tom Boot",
        "Lingwei Kong",
        "Weining Wang"
      ],
      "abstract": "Conditional Value-at-Risk (CoVaR) quantifies systemic financial risk by measuring the loss quantile of one asset, conditional on another asset experiencing distress. We develop a Transformer-based methodology that integrates financial news articles directly with market data to improve CoVaR estimates. Unlike approaches that use predefined sentiment scores, our method incorporates raw text embeddings generated by a large language model (LLM). We prove explicit error bounds for our Transformer CoVaR estimator, showing that accurate CoVaR learning is possible even with small datasets. Using U.S. market returns and Reuters news items from 2006--2013, our out-of-sample results show that textual information impacts the CoVaR forecasts. With better predictive performance, we identify a pronounced negative dip during market stress periods across several equity assets when comparing the Transformer-based CoVaR to both the CoVaR without text and the CoVaR using traditional sentiment measures. Our results show that textual data can be used to effectively model systemic risk without requiring prohibitively large data sets.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "econ.EM",
        "q-fin.RM",
        "stat.ML"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.12490v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12490v1",
      "doi": null
    },
    {
      "id": "2602.12483",
      "title": "Quantile Randomized Kaczmarz Algorithm with Whitelist Trust Mechanism",
      "authors": [
        "Sofiia Shvaiko",
        "Longxiu Huang",
        "Elizaveta Rebrova"
      ],
      "abstract": "Randomized Kaczmarz (RK) is a simple and fast solver for consistent overdetermined systems, but it is known to be fragile under noise. We study overdetermined $m\\times n$ linear systems with a sparse set of corrupted equations, $ {\\bf A}{\\bf x}^\\star = {\\bf b}, $where only $\\tilde{\\bf b} = {\\bf b} + \\boldsymbol{\\varepsilon}$ is observed with $\\|\\boldsymbol{\\varepsilon}\\|_0 \\le βm$. The recently introduced QuantileRK (QRK) algorithm addresses this issue by testing residuals against a quantile threshold, but computing a per-iteration quantile across many rows is costly. In this work we (i) reanalyze QRK and show that its convergence rate improves monotonically as the corruption fraction $β$ decreases; (ii) propose a simple online detector that flags and removes unreliable rows, which reduces the effective $β$ and speeds up convergence; and (iii) make the method practical by estimating quantiles from a small random subsample of rows, preserving robustness while lowering the per-iteration cost. Simulations on imaging and synthetic data demonstrate the efficiency of the proposed method.",
      "published": "2026-02-12",
      "updated": "2026-02-12",
      "categories": [
        "math.NA",
        "stat.ME"
      ],
      "primaryCategory": "math.NA",
      "pdfUrl": "https://arxiv.org/pdf/2602.12483v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12483v1",
      "doi": null
    },
    {
      "id": "2602.12435",
      "title": "Scalable Changepoint Detection for Large Spatiotemporal Data on the Sphere",
      "authors": [
        "Samantha Shi-Jun",
        "Bo Li"
      ],
      "abstract": "We propose a novel Bayesian framework for changepoint detection in large-scale spherical spatiotemporal data, with broad applicability in environmental and climate sciences. Our approach models changepoints as spatially dependent categorical variables using a multinomial probit model (MPM) with a latent Gaussian process, effectively capturing complex spatial correlation structures on the sphere. To handle the high dimensionality inherent in global datasets, we leverage stochastic partial differential equations (SPDE) and spherical harmonic transformations for efficient representation and scalable inference, drastically reducing computational burden while maintaining high accuracy. Through extensive simulation studies, we demonstrate the efficiency and robustness of the proposed method for changepoint estimation, as well as the significant computational gains achieved through the combined use of the MPM and truncated spectral representations of latent processes. Finally, we apply our method to global aerosol optical depth data, successfully identifying changepoints associated with a major atmospheric event.",
      "published": "2026-02-12",
      "updated": "2026-02-12",
      "categories": [
        "stat.ME",
        "stat.CO"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.12435v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12435v1",
      "doi": null
    },
    {
      "id": "2602.12234",
      "title": "Batch-based Bayesian Optimal Experimental Design in Linear Inverse Problems",
      "authors": [
        "Sofia Mäkinen",
        "Andrew B. Duncan",
        "Tapio Helin"
      ],
      "abstract": "Experimental design is central to science and engineering. A ubiquitous challenge is how to maximize the value of information obtained from expensive or constrained experimental settings. Bayesian optimal experimental design (OED) provides a principled framework for addressing such questions. In this paper, we study experimental design problems such as the optimization of sensor locations over a continuous domain in the context of linear Bayesian inverse problems. We focus in particular on batch design, that is, the simultaneous optimization of multiple design variables, which leads to a notoriously difficult non-convex optimization problem. We tackle this challenge using a promising strategy recently proposed in the frequentist setting, which relaxes A-optimal design to the space of finite positive measures. Our main contribution is the rigorous identification of the Bayesian inference problem corresponding to this relaxed A-optimal OED formulation. Moreover, building on recent work, we develop a Wasserstein gradient-flow -based optimization algorithm for the expected utility and introduce novel regularization schemes that guarantee convergence to an empirical measure. These theoretical results are supported by numerical experiments demonstrating both convergence and the effectiveness of the proposed regularization strategy.",
      "published": "2026-02-12",
      "updated": "2026-02-12",
      "categories": [
        "stat.ME",
        "math.OC"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.12234v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12234v1",
      "doi": null
    },
    {
      "id": "2602.12317",
      "title": "Free Lunch in Medical Image Foundation Model Pre-training via Randomized Synthesis and Disentanglement",
      "authors": [
        "Yuhan Wei",
        "Yuting He",
        "Linshan Wu",
        "Fuxiang Huang",
        "Junlin Hou",
        "Hao Chen"
      ],
      "abstract": "Medical image foundation models (MIFMs) have demonstrated remarkable potential for a wide range of clinical tasks, yet their development is constrained by the scarcity, heterogeneity, and high cost of large-scale annotated datasets. Here, we propose RaSD (Randomized Synthesis and Disentanglement), a scalable framework for pre-training MIFMs entirely on synthetic data. By modeling anatomical structures and appearance variations with randomized Gaussian distributions, RaSD exposes models to sufficient multi-scale structural and appearance perturbations, forcing them to rely on invariant and task-relevant anatomical cues rather than dataset-specific textures, thereby enabling robust and transferable representation learning. We pre-trained RaSD on 1.2 million 3D volumes and 9.6 million 2D images, and extensively evaluated the resulting models across 6 imaging modalities, 48 datasets, and 56 downstream tasks. Across all evaluated downstream tasks, RaSD consistently outperforms training-from-scratch models, achieves the best performance on 17 tasks, and remains comparable to models pre-trained on large real datasets in most others. These results demonstrate that the capacity of synthetic data alone to drive robust representation learning. Our findings establish a paradigm shift in medical AI, demonstrating that synthetic data can serve as a \"free lunch\" for scalable, privacy-preserving, and clinically generalizable foundation models.",
      "published": "2026-02-12",
      "updated": "2026-02-12",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG"
      ],
      "primaryCategory": "q-bio.QM",
      "pdfUrl": "https://arxiv.org/pdf/2602.12317v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12317v1",
      "doi": null
    },
    {
      "id": "2602.12072",
      "title": "Enhanced Forest Inventories for Habitat Mapping: A Case Study in the Sierra Nevada Mountains of California",
      "authors": [
        "Maxime Turgeon",
        "Michael Kieser",
        "Dwight Wolfe",
        "Bruce MacArthur"
      ],
      "abstract": "Traditional forest inventory systems, originally designed to quantify merchantable timber volume, often lack the spatial resolution and structural detail required for modern multi-resource ecosystem management. In this manuscript, we present an Enhanced Forest Inventory (EFI) and demonstrate its utility for high-resolution wildlife habitat mapping. The project area covers 270,000 acres of the Eldorado National Forest in California's Sierra Nevada. By integrating 118 ground-truth Forest Inventory and Analysis (FIA) plots with multi-modal remote sensing data (LiDAR, aerial photography, and Sentinel-2 satellite imagery), we developed predictive models for key forest attributes. Our methodology employed a two-tier segmentation approach, partitioning the landscape into approximately 575,000 reporting units with an average size of 0.5 acre to capture forest heterogeneity. We utilized an Elastic-Net Regression framework and automated feature selection to relate remote sensing metrics to ground-measured variables such as basal area, stems per acre, and canopy cover. These physical metrics were translated into functional habitat attributes to evaluate suitability for two focal species: the California Spotted Owl (Strix occidentalis occidentalis) and the Pacific Fisher (Pekania pennanti). Our analysis identified 25,630 acres of nesting and 26,622 acres of foraging habitat for the owl, and 25,636 acres of likely habitat for the fisher based on structural requirements like large-diameter trees and high canopy closure. The results demonstrate that EFIs provide a critical bridge between forestry and conservation ecology, offering forest managers a spatially explicit tool to monitor ecosystem health and manage vulnerable species in complex environments.",
      "published": "2026-02-12",
      "updated": "2026-02-12",
      "categories": [
        "stat.AP"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.12072v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12072v1",
      "doi": null
    },
    {
      "id": "2602.12043",
      "title": "Improved Inference for CSDID Using the Cluster Jackknife",
      "authors": [
        "Sunny R. Karim",
        "Morten Ørregaard Nielsen",
        "James G. MacKinnon",
        "Matthew D. Webb"
      ],
      "abstract": "Obtaining reliable inferences with traditional difference-in-differences (DiD) methods can be difficult. Problems can arise when both outcomes and errors are serially correlated, when there are few clusters or few treated clusters, when cluster sizes vary greatly, and in various other cases. In recent years, recognition of the ``staggered adoption'' problem has shifted the focus away from inference towards consistent estimation of treatment effects. One of the most popular new estimators is the CSDID procedure of Callaway and Sant'Anna (2021). We find that the issues of over-rejection with few clusters and/or few treated clusters are at least as severe for CSDID as for traditional DiD methods. We also propose using a cluster jackknife for inference with CSDID, which simulations suggest greatly improves inference. We provide software packages in Stata csdidjack and R didjack to calculate cluster-jackknife standard errors easily.",
      "published": "2026-02-12",
      "updated": "2026-02-12",
      "categories": [
        "econ.EM",
        "stat.ME",
        "stat.ML"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.12043v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12043v1",
      "doi": null
    },
    {
      "id": "2602.12310",
      "title": "ChemRecon: a Consolidated Meta-Database Platform for Biochemical Data Integration",
      "authors": [
        "Casper Asbjørn Eriksen",
        "Jakob Lykke Andersen",
        "Rolf Fagerberg",
        "Daniel Merkle"
      ],
      "abstract": "In this paper, we present ChemRecon, a meta-database and Python interface for integrating and exploring biochemical data across multiple heterogeneous resources by consolidating compounds, reactions, enzymes, molecular structures, and atom-to-atom maps from several major databases into a single, consistent ontology. ChemRecon enables unified querying, cross-database analysis, and the construction of graph-based representations of sets of related database entries by the traversal of inter-database connections. This facilitates information extraction which is impossible within any single database, including deriving consensus information from conflicting sources, of which identifying the most probable molecular structure associated with a given compound is just one example. The Python interface is available via pip from the Python Package Index (https://pypi.org/project/chemrecon/). ChemRecon is open-source and the source code is hosted at GitLab (https://gitlab.com/casbjorn/chemrecon). Documentation and additional information is available at https://chemrecon.org.",
      "published": "2026-02-12",
      "updated": "2026-02-12",
      "categories": [
        "q-bio.QM"
      ],
      "primaryCategory": "q-bio.QM",
      "pdfUrl": "https://arxiv.org/pdf/2602.12310v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12310v1",
      "doi": null
    },
    {
      "id": "2602.12026",
      "title": "Protein Circuit Tracing via Cross-layer Transcoders",
      "authors": [
        "Darin Tsui",
        "Kunal Talreja",
        "Daniel Saeedi",
        "Amirali Aghazadeh"
      ],
      "abstract": "Protein language models (pLMs) have emerged as powerful predictors of protein structure and function. However, the computational circuits underlying their predictions remain poorly understood. Recent mechanistic interpretability methods decompose pLM representations into interpretable features, but they treat each layer independently and thus fail to capture cross-layer computation, limiting their ability to approximate the full model. We introduce ProtoMech, a framework for discovering computational circuits in pLMs using cross-layer transcoders that learn sparse latent representations jointly across layers to capture the model's full computational circuitry. Applied to the pLM ESM2, ProtoMech recovers 82-89% of the original performance on protein family classification and function prediction tasks. ProtoMech then identifies compressed circuits that use <1% of the latent space while retaining up to 79% of model accuracy, revealing correspondence with structural and functional motifs, including binding, signaling, and stability. Steering along these circuits enables high-fitness protein design, surpassing baseline methods in more than 70% of cases. These results establish ProtoMech as a principled framework for protein circuit tracing.",
      "published": "2026-02-12",
      "updated": "2026-02-12",
      "categories": [
        "cs.LG",
        "q-bio.QM"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2602.12026v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12026v1",
      "doi": null
    },
    {
      "id": "2602.12023",
      "title": "Decomposition of Spillover Effects Under Misspecification:Pseudo-true Estimands and a Local--Global Extension",
      "authors": [
        "Yechan Park",
        "Xiaodong Yang"
      ],
      "abstract": "Applied work with interference typically models outcomes as functions of own treatment and a low-dimensional exposure mapping of others' treatments, even when that mapping may be misspecified. This raises a basic question: what policy object are exposure-based estimands implicitly targeting, and how should we interpret their direct and spillover components relative to the underlying policy question? We take as primitive the marginal policy effect, defined as the effect of a small change in the treatment probability under the actual experimental design, and show that any researcher-chosen exposure mapping induces a unique pseudo-true outcome model. This model is the best approximation to the underlying potential outcomes that depends only on the user-chosen exposure. Utilizing that representation, the marginal policy effect admits a canonical decomposition into exposure-based direct and spillover effects, and each component provides its optimal approximation to the corresponding oracle objects that would be available if interference were fully known. We then focus on a setting that nests important empirical and theoretical applications in which both local network spillovers and global spillovers, such as market equilibrium, operate. There, the marginal policy effect further decomposes asymptotically into direct, local, and global channels. An important implication is that many existing methods are more robust than previously understood once we reinterpret their targets as channel-specific components of this pseudo-true policy estimand. Simulations and a semi-synthetic experiment calibrated to a large cash-transfer experiment show that these components can be recovered in realistic experimental designs.",
      "published": "2026-02-12",
      "updated": "2026-02-12",
      "categories": [
        "econ.EM",
        "math.ST",
        "stat.ML"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.12023v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12023v1",
      "doi": null
    },
    {
      "id": "2602.11873",
      "title": "Temporally resolved aortic 3D shape reconstruction from a limited number of cine 2D MRI slices",
      "authors": [
        "Gloria Wolkerstorfer",
        "Stefano Buoso",
        "Rabea Schlenker",
        "Jochen von Spiczak",
        "Robert Manka",
        "Sebastian Kozerke"
      ],
      "abstract": "Background and Objective: We propose a shape reconstruction framework to generate time-resolved, patient-specific 3D aortic geometries from a limited number of standard cine 2D magnetic resonance imaging (MRI) acquisitions. A statistical shape model of the aorta is coupled with differentiable volumetric mesh optimization to obtain personalized aortic meshes. Methods: The statistical shape model was constructed from retrospective data and optimized 2D slice placements along the aortic arch were identified. Cine 2D MRI slices were then acquired in 30 subjects (19 volunteers, 11 aortic stenosis patients). After manual segmentation, time-resolved aortic models were generated via differentiable volumetric mesh optimization to derive vessel shape features, centerline parameters, and radial wall strain. In 10 subjects, additional 4D flow MRI was acquired to compare peak-systolic shapes. Results: Anatomically accurate aortic geometries were obtained from as few as six cine 2D MRI slices, achieving a mean +/- standard deviation Dice score of (89.9 +/- 1.6) %, Intersection over Union of (81.7 +/- 2.7) %, Hausdorff distance of (7.3 +/- 3.3) mm, and Chamfer distance of (3.7 +/- 0.6) mm relative to 4D flow MRI. The mean absolute radius error was (0.8 +/- 0.6) mm. Significant age-related differences were observed for all shape features, including radial strain, which decreased progressively ((11.00 +/- 3.11) x 10-2 vs. (3.74 +/- 1.25) x 10-2 vs. (2.89 +/- 0.87) x 10-2 for young, mid-age, and elderly groups). Conclusion: The proposed method enables efficient extraction of time-resolved 3D aortic meshes from limited sets of standard cine 2D MRI acquisitions, suitable for computational shape and strain analysis.",
      "published": "2026-02-12",
      "updated": "2026-02-12",
      "categories": [
        "eess.IV",
        "physics.med-ph",
        "stat.ME"
      ],
      "primaryCategory": "eess.IV",
      "pdfUrl": "https://arxiv.org/pdf/2602.11873v1",
      "arxivUrl": "http://arxiv.org/abs/2602.11873v1",
      "doi": null
    },
    {
      "id": "2602.11712",
      "title": "Potential-energy gating for robust state estimation in bistable stochastic systems",
      "authors": [
        "Luigi Simeone"
      ],
      "abstract": "We introduce potential-energy gating, a method for robust state estimation in systems governed by double-well stochastic dynamics. The observation noise covariance of a Bayesian filter is modulated by the local value of a known or assumed potential energy function: observations are trusted when the state is near a potential minimum and progressively discounted as it approaches the barrier separating metastable wells. This physics-based mechanism differs from statistical robust filters, which treat all state-space regions identically, and from constrained filters, which bound states rather than modulating observation trust. The approach is especially relevant in non-ergodic or data-scarce settings where only a single realization is available and statistical methods alone cannot learn the noise structure. We implement gating within Extended, Unscented, Ensemble, and Adaptive Kalman filters and particle filters, requiring only two additional hyperparameters. Monte Carlo benchmarks (100 replications) on a Ginzburg-Landau double-well with 10% outlier contamination show 57-80% RMSE improvement over the standard Extended Kalman Filter, all statistically significant (p < 10^{-15}, Wilcoxon test). A naive topological baseline using only well positions achieves 57%, confirming that the continuous energy landscape adds ~21 percentage points. The method is robust to misspecification: even with 50% parameter errors, improvement never falls below 47%. Comparing externally forced and spontaneous Kramers-type transitions, gating retains 68% improvement under noise-induced transitions whereas the naive baseline degrades to 30%. As an empirical illustration, we apply the framework to Dansgaard-Oeschger events in the NGRIP delta-18O ice-core record, estimating asymmetry gamma = -0.109 (bootstrap 95% CI: [-0.220, -0.011]) and showing that outlier fraction explains 91% of the variance in filter improvement.",
      "published": "2026-02-12",
      "updated": "2026-02-14",
      "categories": [
        "cs.LG",
        "cs.CE",
        "nlin.CD",
        "physics.data-an",
        "stat.ME"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2602.11712v2",
      "arxivUrl": "http://arxiv.org/abs/2602.11712v2",
      "doi": null
    },
    {
      "id": "2602.11711",
      "title": "Estimation of instrument and noise parameters for inverse problem based on prior diffusion model",
      "authors": [
        "Jean-François Giovannelli"
      ],
      "abstract": "This article addresses the issue of estimating observation parameters (response and error parameters) in inverse problems. The focus is on cases where regularization is introduced in a Bayesian framework and the prior is modeled by a diffusion process. In this context, the issue of posterior sampling is well known to be thorny, and a recent paper proposes a notably simple and effective solution. Consequently, it offers an remarkable additional flexibility when it comes to estimating observation parameters. The proposed strategy enables us to define an optimal estimator for both the observation parameters and the image of interest. Furthermore, the strategy provides a means of quantifying uncertainty. In addition, MCMC algorithms allow for the efficient computation of estimates and properties of posteriors, while offering some guarantees. The paper presents several numerical experiments that clearly confirm the computational efficiency and the quality of both estimates and uncertainties quantification.",
      "published": "2026-02-12",
      "updated": "2026-02-12",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.NA",
        "stat.AP"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2602.11711v1",
      "arxivUrl": "http://arxiv.org/abs/2602.11711v1",
      "doi": null
    },
    {
      "id": "2602.11679",
      "title": "Provable Offline Reinforcement Learning for Structured Cyclic MDPs",
      "authors": [
        "Kyungbok Lee",
        "Angelica Cristello Sarteau",
        "Michael R. Kosorok"
      ],
      "abstract": "We introduce a novel cyclic Markov decision process (MDP) framework for multi-step decision problems with heterogeneous stage-specific dynamics, transitions, and discount factors across the cycle. In this setting, offline learning is challenging: optimizing a policy at any stage shifts the state distributions of subsequent stages, propagating mismatch across the cycle. To address this, we propose a modular structural framework that decomposes the cyclic process into stage-wise sub-problems. While generally applicable, we instantiate this principle as CycleFQI, an extension of fitted Q-iteration enabling theoretical analysis and interpretation. It uses a vector of stage-specific Q-functions, tailored to each stage, to capture within-stage sequences and transitions between stages. This modular design enables partial control, allowing some stages to be optimized while others follow predefined policies. We establish finite-sample suboptimality error bounds and derive global convergence rates under Besov regularity, demonstrating that CycleFQI mitigates the curse of dimensionality compared to monolithic baselines. Additionally, we propose a sieve-based method for asymptotic inference of optimal policy values under a margin condition. Experiments on simulated and real-world Type 1 Diabetes data sets demonstrate CycleFQI's effectiveness.",
      "published": "2026-02-12",
      "updated": "2026-02-12",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "math.OC",
        "stat.ME"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2602.11679v1",
      "arxivUrl": "http://arxiv.org/abs/2602.11679v1",
      "doi": null
    },
    {
      "id": "2602.11618",
      "title": "How Well Do Large-Scale Chemical Language Models Transfer to Downstream Tasks?",
      "authors": [
        "Tatsuya Sagawa",
        "Ryosuke Kojima"
      ],
      "abstract": "Chemical Language Models (CLMs) pre-trained on large scale molecular data are widely used for molecular property prediction. However, the common belief that increasing training resources such as model size, dataset size, and training compute improves both pretraining loss and downstream task performance has not been systematically validated in the chemical domain. In this work, we evaluate this assumption by pretraining CLMs while scaling training resources and measuring transfer performance across diverse molecular property prediction (MPP) tasks. We find that while pretraining loss consistently decreases with increased training resources, downstream task performance shows limited improvement. Moreover, alternative metrics based on the Hessian or loss landscape also fail to estimate downstream performance in CLMs. We further identify conditions under which downstream performance saturates or degrades despite continued improvements in pretraining metrics, and analyze the underlying task dependent failure modes through parameter space visualizations. These results expose a gap between pretraining based evaluation and downstream performance, and emphasize the need for model selection and evaluation strategies that explicitly account for downstream task characteristics.",
      "published": "2026-02-12",
      "updated": "2026-02-12",
      "categories": [
        "cs.LG",
        "q-bio.QM"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2602.11618v1",
      "arxivUrl": "http://arxiv.org/abs/2602.11618v1",
      "doi": null
    }
  ],
  "metadata": {
    "lastUpdated": "2026-02-17T02:49:36.482Z",
    "totalPapers": 50,
    "categories": [
      "stat.ME",
      "stat.AP",
      "econ.EM",
      "q-bio.QM"
    ],
    "queryDate": "2026-02-17"
  }
}