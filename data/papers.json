{
  "papers": [
    {
      "id": "2602.21068",
      "title": "Detecting Where Effects Occur by Testing Hypotheses in Order",
      "authors": [
        "Jake Bowers",
        "David Kim",
        "Nuole Chen"
      ],
      "abstract": "Experimental evaluations of public policies often randomize a new intervention within many sites or blocks. After a report of an overall result -- statistically significant or not -- the natural question from a policy maker is: \\emph{where} did any effects occur? Standard adjustments for multiple testing provide little power to answer this question. In simulations modeled after a 44-block education trial, the Hommel adjustment -- among the most powerful procedures controlling the family-wise error rate (FWER) -- detects effects in only 11\\% of truly non-null blocks. We develop a procedure that tests hypotheses top-down through a tree: test the overall null at the root, then groups of blocks, then individual blocks, stopping any branch where the null is not rejected. In the same 44-block design, this approach detects effects in 44\\% of non-null blocks -- roughly four times the detection rate. A stopping rule and valid tests at each node suffice for weak FWER control. We show that the strong-sense FWER depends on how rejection probabilities accumulate along paths through the tree. This yields a diagnostic: when power decays fast enough relative to branching, no adjustment is needed; otherwise, an adaptive $α$-adjustment restores control. We apply the method to 25 MDRC education trials and provide an R package, \\texttt{manytestsr}.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.AP"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.21068v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21068v1",
      "doi": null
    },
    {
      "id": "2602.21036",
      "title": "Empirically Calibrated Conditional Independence Tests",
      "authors": [
        "Milleno Pan",
        "Antoine de Mathelin",
        "Wesley Tansey"
      ],
      "abstract": "Conditional independence tests (CIT) are widely used for causal discovery and feature selection. Even with false discovery rate (FDR) control procedures, they often fail to provide frequentist guarantees in practice. We highlight two common failure modes: (i) in small samples, asymptotic guarantees for many CITs can be inaccurate and even correctly specified models fail to estimate the noise levels and control the error, and (ii) when sample sizes are large but models are misspecified, unaccounted dependencies skew the test's behavior and fail to return uniform p-values under the null. We propose Empirically Calibrated Conditional Independence Tests (ECCIT), a method that measures and corrects for miscalibration. For a chosen base CIT (e.g., GCM, HRT), ECCIT optimizes an adversary that selects features and response functions to maximize a miscalibration metric. ECCIT then fits a monotone calibration map that adjusts the base-test p-values in proportion to the observed miscalibration. Across empirical benchmarks on synthetic and real data, ECCIT achieves valid FDR with higher power than existing calibration strategies while remaining test agnostic.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.ME",
        "cs.LG",
        "stat.ML"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.21036v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21036v1",
      "doi": null
    },
    {
      "id": "2602.21031",
      "title": "Exchangeable Gaussian Processes for Staggered-Adoption Policy Evaluation",
      "authors": [
        "Hayk Gevorgyan",
        "Konstantinos Kalogeropoulos",
        "Angelos Alexopoulos"
      ],
      "abstract": "We study the use of exchangeable multi-task Gaussian processes (GPs) for causal inference in panel data, applying the framework to two settings: one with a single treated unit subject to a once-and-for-all treatment and another with multiple treated units and staggered treatment adoption. Our approach models the joint evolution of outcomes for treated and control units through a GP prior that ensures exchangeability across units while allowing for flexible nonlinear trends over time. The resulting posterior predictive distribution for the untreated potential outcomes of the treated unit provides a counterfactual path, from which we derive pointwise and cumulative treatment effects, along with credible intervals to quantify uncertainty. We implement several variations of the exchangeable GP model using different kernel functions. To assess prediction accuracy, we conduct a placebo-style validation within the pre-intervention window by selecting a ``fake'' intervention date. Ultimately, this study illustrates how exchangeable GPs serve as a flexible tool for policy evaluation in panel data settings and proposes a novel approach to staggered-adoption designs with a large number of treated and control units.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.ME",
        "stat.ML"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.21031v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21031v1",
      "doi": null
    },
    {
      "id": "2602.21029",
      "title": "On the non-uniformity of the 2026 FIFA World Cup draw",
      "authors": [
        "László Csató",
        "Martin Becker",
        "Karel Devriesere",
        "Dries Goossens"
      ],
      "abstract": "The group stage of a sports tournament is often made more appealing by introducing additional constraints in the group draw that promote an attractive and balanced group composition. For example, the number of intra-regional group matches is minimised in several World Cups. However, under such constraints, the traditional draw procedure may become non-uniform, meaning that the feasible allocations of the teams into groups are not equally likely to occur. Our paper quantifies this non-uniformity of the 2026 FIFA World Cup draw for the official draw procedure, as well as for 47 reasonable alternatives implied by all permutations of the four pots and two group labelling policies. We show why simulating with a recursive backtracking algorithm is intractable, and propose a workable implementation using integer programming. The official draw mechanism is found to be optimal based on four measures of non-uniformity. Nonetheless, non-uniformity can be more than halved if the organiser aims to treat the best teams drawn from the first pot equally.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.AP",
        "math.OC",
        "physics.soc-ph"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.21029v1",
      "arxivUrl": "http://arxiv.org/abs/2602.21029v1",
      "doi": null
    },
    {
      "id": "2602.20965",
      "title": "Estimating the Partially Linear Zero-Inflated Poisson Regression Model: a Robust Approach Using a EM-like Algorithm",
      "authors": [
        "María José Llop",
        "Andrea Bergesio",
        "Anne-Françoise Yao"
      ],
      "abstract": "Count data with an excessive number of zeros frequently arise in fields such as economics, medicine, and public health. Traditional count models often fail to adequately handle such data, especially when the relationship between the response and some predictors is nonlinear. To overcome these limitations, the partially linear zero-inflated Poisson (PLZIP) model has been proposed as a flexible alternative. However, all existing estimation approaches for this model are based on likelihood, which is known to be highly sensitive to outliers and slight deviations from the model assumptions. This article presents the first robust estimation method specifically developed for the PLZIP model. An Expectation-Maximization-like algorithm is used to take advantage of the mixture nature of the model and to address extreme observations in both the response and the covariates. Results of the algorithm convergence and the consistency of the estimators are proved. A simulation study under various contamination schemes showed the robustness and efficiency of the proposed estimators in finite samples, compared to classical estimators. Finally, the application of the methodology is illustrated through an example using real data.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.20965v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20965v1",
      "doi": null
    },
    {
      "id": "2602.20939",
      "title": "A Statistical Framework for Detecting Emergent Narratives in Longitudinal Text Corpora",
      "authors": [
        "Cynthia Medeiros",
        "John Quigley",
        "Matthew Revie"
      ],
      "abstract": "Narratives about economic events and policies are widely recognised as influential drivers of economic and business behaviour. Yet the statistical identification of narrative emergence remains underdeveloped. Narratives evolve gradually, exhibit subtle shifts in content, and may exert influence disproportionate to their observable frequency, making it difficult to determine when observed changes reflect genuine structural shifts rather than routine variation in language use. We propose a statistical framework for detecting narrative emergence in longitudinal text corpora using Latent Dirichlet Allocation (LDA). We define emergence as a sustained increase in a topic's relative prominence over time and articulate a statistical framework for interpreting such trajectories, recognising that topic proportions are latent, model-estimated quantities. We illustrate the approach using a corpus of academic publications in economics spanning 1970-2018, where Nobel Prize-recognised contributions serve as externally observable signals of influential narratives. Topics associated with these contributions display sustained increases in estimated prevalence that coincide with periods of heightened citation activity and broader disciplinary recognition. These findings indicate that model-based topic trajectories can reflect identifiable shifts in economic discourse and provide a statistically grounded basis for analysing thematic change in longitudinal textual data.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.20939v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20939v1",
      "doi": null
    },
    {
      "id": "2602.20912",
      "title": "A Corrected Welch Satterthwaite Equation. And: What You Always Wanted to Know About Kish's Effective Sample but Were Afraid to Ask",
      "authors": [
        "Matthias von Davier"
      ],
      "abstract": "This article presents a corrected version of the Satterthwaite (1941, 1946) approximation for the degrees of freedom of a weighted sum of independent variance components. The original formula is known to yield biased estimates when component degrees of freedom are small. The correction, derived from exact moment matching, adjusts for the bias by incorporating a factor that accounts for the estimation of fourth moments. We show that Kish's (1965) effective sample size formula emerges as a special case when all variance components are equal, and component degrees of freedom are ignored. Simulation studies demonstrate that the corrected estimator closely matches the expected degrees of freedom even for small component sizes, while the original Satterthwaite estimator exhibits substantial downward bias. Additional applications are discussed, including jackknife variance estimation, multiple imputation total variance, and the Welch test for unequal variances.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.AP"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.20912v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20912v1",
      "doi": null
    },
    {
      "id": "2602.20896",
      "title": "On Stein's test of uniformity on the hypersphere",
      "authors": [
        "Paul Axmann",
        "Bruno Ebner",
        "Eduardo García-Portugués"
      ],
      "abstract": "We propose a new test of uniformity on the hypersphere based on a Stein characterization associated with the Laplace--Beltrami operator. We identify a sufficient class of test functions for this characterization, linked to the moment generating function. Exploiting the operator's eigenfunctions to obtain a harmonic decomposition in terms of Gegenbauer polynomials, we show that the proposed procedure belongs to the class of Sobolev tests. We derive closed-form expressions for the distribution of the test statistic under the null hypothesis and under fixed alternatives. To enhance power against a range of alternatives, we introduce a tuning parameter into the characterization and study its impact on rejection probabilities. We discuss data-driven strategies for selecting this parameter to maximize rejection rates for a given alternative and compare the resulting performance with that of related parametric tests. Additional numerical experiments compare the proposed test with competing Sobolev-class procedures, highlighting settings in which it offers clear advantages.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "math.ST",
        "stat.ME"
      ],
      "primaryCategory": "math.ST",
      "pdfUrl": "https://arxiv.org/pdf/2602.20896v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20896v1",
      "doi": null
    },
    {
      "id": "2602.20885",
      "title": "Combining Information Across Diverse Sources: The II-CC-FF Paradigm",
      "authors": [
        "Céline Cunen",
        "Nils Lid Hjort"
      ],
      "abstract": "We introduce and develop a general paradigm for combining information across diverse data sources. In broad terms, suppose $φ$ is a parameter of interest, built up via components $ψ_1,\\ldots,ψ_k$ from data sources $1,\\ldots,k$. The proposed scheme has three steps. First, the Independent Inspection (II) step amounts to investigating each separate data source, translating statistical information to a confidence distribution $C_j(ψ_j)$ for the relevant focus parameter $ψ_j$ associated with data source $j$. Second, Confidence Conversion (CC) techniques are used to translate the confidence distributions to confidence log-likelihood functions, say $\\ell_{{\\rm con},j}(ψ_j)$. Finally, the Focused Fusion (FF) step uses relevant and context-driven techniques to construct a confidence distribution for the primary focus parameter $φ=φ(ψ_1,\\ldots,ψ_k)$, acting on the combined confidence log-likelihood. In traditional setups, the II-CC-FF strategy amounts to versions of meta-analysis, and turns out to be competitive against state-of-the-art methods. Its potential lies in applications to harder problems, however. Illustrations are presented, related to actual applications.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.20885v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20885v1",
      "doi": null
    },
    {
      "id": "2602.20875",
      "title": "Efficient Online Learning in Interacting Particle Systems",
      "authors": [
        "Louis Sharrock",
        "Nikolas Kantas",
        "Grigorios A. Pavliotis"
      ],
      "abstract": "We introduce a new method for online parameter estimation in stochastic interacting particle systems, based on continuous observation of a small number of particles from the system. Our method recursively updates the model parameters using a stochastic approximation of the gradient of the asymptotic log likelihood, which is computed using the continuous stream of observations. Under suitable assumptions, we rigorously establish convergence of our method to the stationary points of the asymptotic log-likelihood of the interacting particle system. We consider asymptotics both in the limit as the time horizon $t\\rightarrow\\infty$, for a fixed and finite number of particles, and in the joint limit as the number of particles $N\\rightarrow\\infty$ and the time horizon $t\\rightarrow\\infty$. Under additional assumptions on the asymptotic log-likelihood, we also establish an $\\mathrm{L}^2$ convergence rate and a central limit theorem. Finally, we present several numerical examples of practical interest, including a model for systemic risk, a model of interacting FitzHugh--Nagumo neurons, and a Cucker--Smale flocking model. Our numerical results corroborate our theoretical results, and also suggest that our estimator is effective even in cases where the assumptions required for our theoretical analysis do not hold.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "math.ST",
        "math.OC",
        "math.PR",
        "stat.ME",
        "stat.ML"
      ],
      "primaryCategory": "math.ST",
      "pdfUrl": "https://arxiv.org/pdf/2602.20875v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20875v1",
      "doi": null
    },
    {
      "id": "2602.20856",
      "title": "Stochastic Discount Factors with Cross-Asset Spillovers",
      "authors": [
        "Doron Avramov",
        "Xin He"
      ],
      "abstract": "This paper develops a unified framework that links firm-level predictive signals, cross-asset spillovers, and the stochastic discount factor (SDF). Signals and spillovers are jointly estimated by maximizing the Sharpe ratio, yielding an interpretable SDF that both ranks characteristic relevance and uncovers the direction of predictive influence across assets. Out-of-sample, the SDF consistently outperforms self-predictive and expected-return benchmarks across investment universes and market states. The inferred information network highlights large, low-turnover firms as net transmitters. The framework offers a clear, economically grounded view of the informational architecture underlying cross-sectional return dynamics.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "q-fin.CP",
        "econ.EM",
        "q-fin.PM",
        "stat.ML"
      ],
      "primaryCategory": "q-fin.CP",
      "pdfUrl": "https://arxiv.org/pdf/2602.20856v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20856v1",
      "doi": null
    },
    {
      "id": "2602.20844",
      "title": "Maximum entropy based testing in network models: ERGMs and constrained optimization",
      "authors": [
        "Subhrosekhar Ghosh",
        "Rathindra Nath Karmakar",
        "Samriddha Lahiry"
      ],
      "abstract": "Stochastic network models play a central role across a wide range of scientific disciplines, and questions of statistical inference arise naturally in this context. In this paper we investigate goodness-of-fit and two-sample testing procedures for statistical networks based on the principle of maximum entropy (MaxEnt). Our approach formulates a constrained entropy-maximization problem on the space of networks, subject to prescribed structural constraints. The resulting test statistics are defined through the Lagrange multipliers associated with the constrained optimization problem, which, to our knowledge, is novel in the statistical networks literature. We establish consistency in the classical regime where the number of vertices is fixed. We then consider asymptotic regimes in which the graph size grows with the sample size, developing tests for both dense and sparse settings. In the dense case, we analyze exponential random graph models (ERGM) (including the Erdös-Rènyi models), while in the sparse regime our theory applies to Erd{ö}s-R{è}nyi graphs. Our analysis leverages recent advances in nonlinear large deviation theory for random graphs. We further show that the proposed Lagrange-multiplier framework connects naturally to classical score tests for constrained maximum likelihood estimation. The results provide a unified entropy-based framework for network model assessment across diverse growth regimes.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "math.ST",
        "cs.IT",
        "math.PR",
        "stat.ME",
        "stat.ML"
      ],
      "primaryCategory": "math.ST",
      "pdfUrl": "https://arxiv.org/pdf/2602.20844v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20844v1",
      "doi": null
    },
    {
      "id": "2602.20834",
      "title": "Confidence Distributions and Related Themes",
      "authors": [
        "Nils Lid Hjort",
        "Tore Schweder"
      ],
      "abstract": "This is the guest editors' general introduction to a Special Issue of the Journal of Statistical Planning and Inference, dedicated to confidence distributions and related themes. Confidence distributions (CDs) are distributions for parameters of interest, constructed via a statistical model after analysing the data. As such they serve the same purpose for the frequentist statisticians as the posterior distributions for the Bayesians. There have been several attempts in the literature to put up a clear theory for such confidence distributions, from Fisher's fiducial inference and onwards. There are certain obstacles and difficulties involved in these attempts, both conceptually and operationally, which have contributed to the CDs being slow in entering statistical mainstream. Recently there is a renewed surge of interest in CDs and various related themes, however, reflected in both series of new methodological research, advanced applications to substantive sciences, and dissemination and communication via workshops and conferences. The present special issue of the JSPI is a collection of papers emanating from the {\\it Inference With Confidence} workshop in Oslo, May 2015. Several of the papers appearing here were first presented at that workshop. The present collection includes however also new research papers from other scholars in the field.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.20834v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20834v1",
      "doi": null
    },
    {
      "id": "2602.20795",
      "title": "Hawkes Identification with a Prescribed Causal Basis: Closed-Form Estimators and Asymptotics",
      "authors": [
        "Xinhui Rong",
        "Girish N. Nair"
      ],
      "abstract": "Driven by the recent surge in neural-inspired modeling, point processes have gained significant traction in systems and control. While the Hawkes process is the standard model for characterizing random event sequences with memory, identifying its unknown kernels is often hindered by nonlinearity. Approaches using prescribed basis kernels have emerged to enable linear parameterization, yet they typically rely on iterative likelihood methods and lack rigorous analysis under model misspecification. This paper justifies a closed-form Least Squares identification framework for Hawkes processes with prescribed kernels. We guarantee estimator existence via the almost-sure positive definiteness of the empirical Gram matrix and prove convergence to the true parameters under correct specification, or to well-defined pseudo-true parameters under misspecification. Furthermore, we derive explicit Central Limit Theorems for both regimes, providing a complete and interpretable asymptotic theory. We demonstrate these theoretical findings through comparative numerical simulations.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.ME",
        "eess.SY"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.20795v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20795v1",
      "doi": null
    },
    {
      "id": "2602.20651",
      "title": "Sparse Bayesian Deep Functional Learning with Structured Region Selection",
      "authors": [
        "Xiaoxian Zhu",
        "Yingmeng Li",
        "Shuangge Ma",
        "Mengyun Wu"
      ],
      "abstract": "In modern applications such as ECG monitoring, neuroimaging, wearable sensing, and industrial equipment diagnostics, complex and continuously structured data are ubiquitous, presenting both challenges and opportunities for functional data analysis. However, existing methods face a critical trade-off: conventional functional models are limited by linearity, whereas deep learning approaches lack interpretable region selection for sparse effects. To bridge these gaps, we propose a sparse Bayesian functional deep neural network (sBayFDNN). It learns adaptive functional embeddings through a deep Bayesian architecture to capture complex nonlinear relationships, while a structured prior enables interpretable, region-wise selection of influential domains with quantified uncertainty. Theoretically, we establish rigorous approximation error bounds, posterior consistency, and region selection consistency. These results provide the first theoretical guarantees for a Bayesian deep functional model, ensuring its reliability and statistical rigor. Empirically, comprehensive simulations and real-world studies confirm the effectiveness and superiority of sBayFDNN. Crucially, sBayFDNN excels in recognizing intricate dependencies for accurate predictions and more precisely identifies functionally meaningful regions, capabilities fundamentally beyond existing approaches.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "cs.LG",
        "stat.AP",
        "stat.ML"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2602.20651v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20651v1",
      "doi": null
    },
    {
      "id": "2602.20611",
      "title": "Amortized Bayesian inference for actigraph time sheet data from mobile devices",
      "authors": [
        "Daniel Zhou",
        "Sudipto Banerjee"
      ],
      "abstract": "Mobile data technologies use ``actigraphs'' to furnish information on health variables as a function of a subject's movement. The advent of wearable devices and related technologies has propelled the creation of health databases consisting of human movement data to conduct research on mobility patterns and health outcomes. Statistical methods for analyzing high-resolution actigraph data depend on the specific inferential context, but the advent of Artificial Intelligence (AI) frameworks require that the methods be congruent to transfer learning and amortization. This article devises amortized Bayesian inference for actigraph time sheets. We pursue a Bayesian approach to ensure full propagation of uncertainty and its quantification using a hierarchical dynamic linear model. We build our analysis around actigraph data from the Physical Activity through Sustainable Transport Approaches in Los Angeles (PASTA-LA) study conducted by the Fielding School of Public Health in the University of California, Los Angeles. Apart from achieving probabilistic imputation of actigraph time sheets, we are also able to statistically learn about the time-varying impact of explanatory variables on the magnitude of acceleration (MAG) for a cohort of subjects.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2602.20611v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20611v1",
      "doi": null
    },
    {
      "id": "2602.20581",
      "title": "Using Prior Studies to Design Experiments: An Empirical Bayes Approach",
      "authors": [
        "Zhiheng You"
      ],
      "abstract": "We develop an empirical Bayes framework for experimental design that leverages information from prior related studies. When a researcher has access to estimates from previous studies on similar parameters, they can use empirical Bayes to estimate an informative prior over the parameter of interest in the new study. We show how this prior can be incorporated into a decision-theoretic experimental design framework to choose optimal design. The approach is illustrated via propensity score designs in stratified randomized experiments. Our theoretical results show that the empirical Bayes design achieves oracle-optimal performance as the number of prior studies grows, and characterize the rate at which regret vanishes. To illustrate the approach, we present two empirical applications--oncology drug trials and the Tennessee Project STAR experiment. Our framework connects the Bayesian meta-analysis literature to experimental design and provides practical guidance for researchers seeking to design more efficient experiments.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "econ.EM"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.20581v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20581v1",
      "doi": null
    },
    {
      "id": "2602.20572",
      "title": "Local Fréchet regression with toroidal predictors",
      "authors": [
        "Chang Jun Im",
        "Jeong Min Jeon"
      ],
      "abstract": "We provide the first regression framework that simultaneously accommodates responses taking values in a general metric space and predictors lying on a general torus. We propose intrinsic local constant and local linear estimators that respect the underlying geometries of both the response and predictor spaces. Our local linear estimator is novel even in the case of scalar responses. We further establish their asymptotic properties, including consistency and convergence rates. Simulation studies, together with an application to real data, illustrate the superior performance of the proposed methodology.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.20572v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20572v1",
      "doi": null
    },
    {
      "id": "2602.20549",
      "title": "Sample-efficient evidence estimation of score based priors for model selection",
      "authors": [
        "Frederic Wang",
        "Katherine L. Bouman"
      ],
      "abstract": "The choice of prior is central to solving ill-posed imaging inverse problems, making it essential to select one consistent with the measurements $y$ to avoid severe bias. In Bayesian inverse problems, this could be achieved by evaluating the model evidence $p(y \\mid M)$ under different models $M$ that specify the prior and then selecting the one with the highest value. Diffusion models are the state-of-the-art approach to solving inverse problems with a data-driven prior; however, directly computing the model evidence with respect to a diffusion prior is intractable. Furthermore, most existing model evidence estimators require either many pointwise evaluations of the unnormalized prior density or an accurate clean prior score. We propose \\method, an estimator of the model evidence of a diffusion prior by integrating over the time-marginals of posterior sampling methods. Our method leverages the large amount of intermediate samples naturally obtained during the reverse diffusion sampling process to obtain an accurate estimation of the model evidence using only a handful of posterior samples (e.g., 20). We also demonstrate how to implement our estimator in tandem with recent diffusion posterior sampling methods. Empirically, our estimator matches the model evidence when it can be computed analytically, and it is able to both select the correct diffusion model prior and diagnose prior misfit under different highly ill-conditioned, non-linear inverse problems, including a real-world black hole imaging problem.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "cs.LG",
        "cs.CV",
        "stat.ME"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2602.20549v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20549v1",
      "doi": null
    },
    {
      "id": "2602.20503",
      "title": "Error-Controlled Borrowing from External Data Using Wasserstein Ambiguity Sets",
      "authors": [
        "Yui Kimura",
        "Shu Tamano"
      ],
      "abstract": "Incorporating external data can improve the efficiency of clinical trials, but distributional mismatches between current and external populations threaten the validity of inference. While numerous dynamic borrowing methods exist, the calibration of their borrowing parameters relies mainly on ad hoc, simulation-based tuning. To overcome this, we propose BOND (Borrowing under Optimal Nonparametric Distributional robustness), a framework that formalizes data noncommensurability through Wasserstein ambiguity sets centered at the current-trial distribution. By deriving sharp, closed-form bounds on the worst-case mean drift for both continuous and binary outcomes, we construct a distributionally robust, bias-corrected Wald statistic that ensures asymptotic type I error control uniformly over the ambiguity set. Importantly, BOND determines the optimal borrowing strength by maximizing a worst-case power proxy, converting heuristic parameter tuning into a transparent, analytically tractable optimization problem. Furthermore, we demonstrate that many prominent borrowing methods can be reparameterized via an effective borrowing weight, rendering our calibration framework broadly applicable. Simulation studies and a real-world clinical trial application confirm that BOND preserves the nominal size under unmeasured heterogeneity while achieving efficiency gains over standard borrowing methods.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.ME",
        "stat.AP"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.20503v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20503v1",
      "doi": null
    },
    {
      "id": "2602.20498",
      "title": "Fast Algorithms for Exact Confidence Intervals in Randomized Experiments with Binary Outcomes",
      "authors": [
        "Peng Zhang"
      ],
      "abstract": "We construct exact confidence intervals for the average treatment effect in randomized experiments with binary outcomes using sequences of randomization tests. Our approach does not rely on large-sample approximations and is valid for all sample sizes. Under a balanced Bernoulli design or a matched-pairs design, we show that exact confidence intervals can be computed using only $O(\\log n)$ randomization tests, yielding an exponential reduction in the number of tests compared to brute-force. We further prove an information-theoretic lower bound showing that this rate is optimal. In contrast, under balanced complete randomization, the most efficient known procedures require $O(n\\log n)$ randomization tests (Aronow et al., 2023), establishing a sharp separation between these designs. In addition, we extend our algorithm to general Bernoulli designs using $O(n^2)$ randomization tests.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.20498v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20498v1",
      "doi": null
    },
    {
      "id": "2602.20495",
      "title": "Unveiling Scaling Laws of Parameter Identifiability and Uncertainty Quantification in Data-Driven Biological Modeling",
      "authors": [
        "Shun Wang",
        "Wenrui Hao"
      ],
      "abstract": "Integrating high-dimensional biological data into data-driven mechanistic modeling requires rigorous practical identifiability to ensure interpretability and generalizability. However, coordinate identifiability analysis often suffers from numerical instabilities near singular local minimizers. We present a computational framework that uncovers fundamental scaling laws governing practical identifiability through asymptotic analysis. By synthesizing Fisher information with perturbed Hessian matrices, we establish a hierarchical approach to quantify coordinate identifiability and inform uncertainty quantification within non-identifiable subspaces across different orders. Supported by rigorous mathematical analysis and validated on synthetic and real-world data, our framework was applied to HIV-host dynamics and spatiotemporal amyloid-beta propagation. These applications demonstrate the framework's efficiency in elucidating critical mechanisms underlying HIV diagnostics and Alzheimer's disease progression. In the era of large-scale mechanistic digital twins, our framework provides the scaling laws for data-driven modeling in terms of both parameter identifiability and uncertainty, ensuring that data-driven inferences are grounded in verifiable biological reality.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "q-bio.QM"
      ],
      "primaryCategory": "q-bio.QM",
      "pdfUrl": "https://arxiv.org/pdf/2602.20495v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20495v1",
      "doi": null
    },
    {
      "id": "2602.20448",
      "title": "Posterior Mode Guided Dimension Reduction for Bayesian Model Averaging in Heavy-Tailed Linear Regression",
      "authors": [
        "Shamriddha De",
        "Joyee Ghosh"
      ],
      "abstract": "For large model spaces, the potential entrapment of Markov chain Monte Carlo (MCMC) based methods with spike-and-slab priors poses significant challenges in posterior computation in regression models. On the other hand, maximum a posteriori (MAP) estimation, which is a more computationally viable alternative, fails to provide uncertainty quantification. To address these problems simultaneously and efficiently, this paper proposes a hybrid method that blends MAP estimation with MCMC-based stochastic search algorithms within a heavy-tailed error framework. Under hyperbolic errors, the current work develops a two-step expectation conditional maximization (ECM) guided MCMC algorithm. In the first step, we conduct an ECM-based posterior maximization and perform variable selection, thereby identifying a reduced model space in a high posterior probability region. In the second step, we execute a Gibbs sampler on the reduced model space for posterior computation. Such a method is expected to improve the efficiency of posterior computation and enhance its inferential richness. Through simulation studies and benchmark real life examples, our proposed method is shown to exhibit several advantages in variable selection and uncertainty quantification over various state-of-the-art methods.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "stat.ME",
        "stat.CO"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.20448v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20448v1",
      "doi": null
    },
    {
      "id": "2602.20396",
      "title": "cc-Shapley: Measuring Multivariate Feature Importance Needs Causal Context",
      "authors": [
        "Jörg Martin",
        "Stefan Haufe"
      ],
      "abstract": "Explainable artificial intelligence promises to yield insights into relevant features, thereby enabling humans to examine and scrutinize machine learning models or even facilitating scientific discovery. Considering the widespread technique of Shapley values, we find that purely data-driven operationalization of multivariate feature importance is unsuitable for such purposes. Even for simple problems with two features, spurious associations due to collider bias and suppression arise from considering one feature only in the observational context of the other, which can lead to misinterpretations. Causal knowledge about the data-generating process is required to identify and correct such misleading feature attributions. We propose cc-Shapley (causal context Shapley), an interventional modification of conventional observational Shapley values leveraging knowledge of the data's causal structure, thereby analyzing the relevance of a feature in the causal context of the remaining features. We show theoretically that this eradicates spurious association induced by collider bias. We compare the behavior of Shapley and cc-Shapley values on various, synthetic, and real-world datasets. We observe nullification or reversal of associations compared to univariate feature importance when moving from observational to cc-Shapley.",
      "published": "2026-02-23",
      "updated": "2026-02-23",
      "categories": [
        "cs.LG",
        "stat.ME"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2602.20396v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20396v1",
      "doi": null
    },
    {
      "id": "2602.20383",
      "title": "Detecting and Mitigating Group Bias in Heterogeneous Treatment Effects",
      "authors": [
        "Joel Persson",
        "Jurriën Bakker",
        "Dennis Bohle",
        "Stefan Feuerriegel",
        "Florian von Wangenheim"
      ],
      "abstract": "Heterogeneous treatment effects (HTEs) are increasingly estimated using machine learning models that produce highly personalized predictions of treatment effects. In practice, however, predicted treatment effects are rarely interpreted, reported, or audited at the individual level but, instead, are often aggregated to broader subgroups, such as demographic segments, risk strata, or markets. We show that such aggregation can induce systematic bias of the group-level causal effect: even when models for predicting the individual-level conditional average treatment effect (CATE) are correctly specified and trained on data from randomized experiments, aggregating the predicted CATEs up to the group level does not, in general, recover the corresponding group average treatment effect (GATE). We develop a unified statistical framework to detect and mitigate this form of group bias in randomized experiments. We first define group bias as the discrepancy between the model-implied and experimentally identified GATEs, derive an asymptotically normal estimator, and then provide a simple-to-implement statistical test. For mitigation, we propose a shrinkage-based bias-correction, and show that the theoretically optimal and empirically feasible solutions have closed-form expressions. The framework is fully general, imposes minimal assumptions, and only requires computing sample moments. We analyze the economic implications of mitigating detected group bias for profit-maximizing personalized targeting, thereby characterizing when bias correction alters targeting decisions and profits, and the trade-offs involved. Applications to large-scale experimental data at major digital platforms validate our theoretical results and demonstrate empirical performance.",
      "published": "2026-02-23",
      "updated": "2026-02-23",
      "categories": [
        "stat.ME",
        "cs.LG",
        "econ.EM"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.20383v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20383v1",
      "doi": null
    },
    {
      "id": "2602.20371",
      "title": "Semi-parametric Bayesian inference under Neyman orthogonality",
      "authors": [
        "Magid Sabbagh",
        "David A. Stephens"
      ],
      "abstract": "The validity of two-step or plug-in inference methods is questioned in the Bayesian framework. We study semi-parametric models where the plug-in of a non-parametrically modelled nuisance component is used. We show that when the nuisance and targeted parameters satisfy a Neyman orthogonal score property, the approach of cutting feedback through a two-step procedure is a valid way of conducting Bayesian inference. Our method relies on a non-parametric Bayesian formulation based on the Dirichlet process and the Bayesian bootstrap. We show that the marginal posterior of the targeted parameter exhibits good frequentist properties despite not accounting for the inferential uncertainty of the nuisance parameter. We adopt this approach in Bayesian causal inference problems where the nuisance propensity score model is estimated to obtain marginal inference for the treatment effect parameter, and demonstrate that a plug-in of the propensity score has a negligible effect on marginal posterior inference for the causal contrast. We investigate the absence of Neyman orthogonality and exploit our findings to show that in conventional two-step procedures, the posterior distribution converges under weaker restrictions than those needed in the frequentist sequel. For a simple family of useful scores, we demonstrate that even in the absence of Neyman orthogonality, the posterior distribution is asymptotically unchanged by the estimation of the nuisance parameter, merely provided the latter estimator is consistent.",
      "published": "2026-02-23",
      "updated": "2026-02-23",
      "categories": [
        "math.ST",
        "stat.ME"
      ],
      "primaryCategory": "math.ST",
      "pdfUrl": "https://arxiv.org/pdf/2602.20371v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20371v1",
      "doi": null
    },
    {
      "id": "2602.20344",
      "title": "Hierarchical Molecular Representation Learning via Fragment-Based Self-Supervised Embedding Prediction",
      "authors": [
        "Jiele Wu",
        "Haozhe Ma",
        "Zhihan Guo",
        "Thanh Vinh Vo",
        "Tze Yun Leong"
      ],
      "abstract": "Graph self-supervised learning (GSSL) has demonstrated strong potential for generating expressive graph embeddings without the need for human annotations, making it particularly valuable in domains with high labeling costs such as molecular graph analysis. However, existing GSSL methods mostly focus on node- or edge-level information, often ignoring chemically relevant substructures which strongly influence molecular properties. In this work, we propose Graph Semantic Predictive Network (GraSPNet), a hierarchical self-supervised framework that explicitly models both atomic-level and fragment-level semantics. GraSPNet decomposes molecular graphs into chemically meaningful fragments without predefined vocabularies and learns node- and fragment-level representations through multi-level message passing with masked semantic prediction at both levels. This hierarchical semantic supervision enables GraSPNet to learn multi-resolution structural information that is both expressive and transferable. Extensive experiments on multiple molecular property prediction benchmarks demonstrate that GraSPNet learns chemically meaningful representations and consistently outperforms state-of-the-art GSSL methods in transfer learning settings.",
      "published": "2026-02-23",
      "updated": "2026-02-23",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2602.20344v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20344v1",
      "doi": null
    },
    {
      "id": "2602.20289",
      "title": "The Sim-to-Real Gap in MRS Quantification: A Systematic Deep Learning Validation for GABA",
      "authors": [
        "Zien Ma",
        "S. M. Shermer",
        "Oktay Karakuş",
        "Frank C. Langbein"
      ],
      "abstract": "Magnetic resonance spectroscopy (MRS) is used to quantify metabolites in vivo and estimate biomarkers for conditions ranging from neurological disorders to cancers. Quantifying low-concentration metabolites such as GABA ($γ$-aminobutyric acid) is challenging due to low signal-to-noise ratio (SNR) and spectral overlap. We investigate and validate deep learning for quantifying complex, low-SNR, overlapping signals from MEGA-PRESS spectra, devise a convolutional neural network (CNN) and a Y-shaped autoencoder (YAE), and select the best models via Bayesian optimisation on 10,000 simulated spectra from slice-profile-aware MEGA-PRESS simulations. The selected models are trained on 100,000 simulated spectra. We validate their performance on 144 spectra from 112 experimental phantoms containing five metabolites of interest (GABA, Glu, Gln, NAA, Cr) with known ground truth concentrations across solution and gel series acquired at 3 T under varied bandwidths and implementations. These models are further assessed against the widely used LCModel quantification tool. On simulations, both models achieve near-perfect agreement (small MAEs; regression slopes $\\approx 1.00$, $R^2 \\approx 1.00$). On experimental phantom data, errors initially increased substantially. However, modelling variable linewidths in the training data significantly reduced this gap. The best augmented deep learning models achieved a mean MAE for GABA over all phantom spectra of 0.151 (YAE) and 0.160 (FCNN) in max-normalised relative concentrations, outperforming the conventional baseline LCModel (0.220). A sim-to-real gap remains, but physics-informed data augmentation substantially reduced it. Phantom ground truth is needed to judge whether a method will perform reliably on real data.",
      "published": "2026-02-23",
      "updated": "2026-02-23",
      "categories": [
        "eess.SP",
        "cs.LG",
        "q-bio.QM"
      ],
      "primaryCategory": "eess.SP",
      "pdfUrl": "https://arxiv.org/pdf/2602.20289v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20289v1",
      "doi": null
    },
    {
      "id": "2602.20271",
      "title": "Uncertainty-Aware Delivery Delay Duration Prediction via Multi-Task Deep Learning",
      "authors": [
        "Stefan Faulkner",
        "Reza Zandehshahvar",
        "Vahid Eghbal Akhlaghi",
        "Sebastien Ouellet",
        "Carsten Jordan",
        "Pascal Van Hentenryck"
      ],
      "abstract": "Accurate delivery delay prediction is critical for maintaining operational efficiency and customer satisfaction across modern supply chains. Yet the increasing complexity of logistics networks, spanning multimodal transportation, cross-country routing, and pronounced regional variability, makes this prediction task inherently challenging. This paper introduces a multi-task deep learning model for delivery delay duration prediction in the presence of significant imbalanced data, where delayed shipments are rare but operationally consequential. The model embeds high-dimensional shipment features with dedicated embedding layers for tabular data, and then uses a classification-then-regression strategy to predict the delivery delay duration for on-time and delayed shipments. Unlike sequential pipelines, this approach enables end-to-end training, improves the detection of delayed cases, and supports probabilistic forecasting for uncertainty-aware decision making. The proposed approach is evaluated on a large-scale real-world dataset from an industrial partner, comprising more than 10 million historical shipment records across four major source locations with distinct regional characteristics. The proposed model is compared with traditional machine learning methods. Experimental results show that the proposed method achieves a mean absolute error of 0.67-0.91 days for delayed-shipment predictions, outperforming single-step tree-based regression baselines by 41-64% and two-step classify-then-regress tree-based models by 15-35%. These gains demonstrate the effectiveness of the proposed model in operational delivery delay forecasting under highly imbalanced and heterogeneous conditions.",
      "published": "2026-02-23",
      "updated": "2026-02-23",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC",
        "stat.AP"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2602.20271v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20271v1",
      "doi": null
    },
    {
      "id": "2602.20153",
      "title": "JUCAL: Jointly Calibrating Aleatoric and Epistemic Uncertainty in Classification Tasks",
      "authors": [
        "Jakob Heiss",
        "Sören Lambrecht",
        "Jakob Weissteiner",
        "Hanna Wutte",
        "Žan Žurič",
        "Josef Teichmann",
        "Bin Yu"
      ],
      "abstract": "We study post-calibration uncertainty for trained ensembles of classifiers. Specifically, we consider both aleatoric (label noise) and epistemic (model) uncertainty. Among the most popular and widely used calibration methods in classification are temperature scaling (i.e., pool-then-calibrate) and conformal methods. However, the main shortcoming of these calibration methods is that they do not balance the proportion of aleatoric and epistemic uncertainty. Not balancing these uncertainties can severely misrepresent predictive uncertainty, leading to overconfident predictions in some input regions while being underconfident in others. To address this shortcoming, we present a simple but powerful calibration algorithm Joint Uncertainty Calibration (JUCAL) that jointly calibrates aleatoric and epistemic uncertainty. JUCAL jointly calibrates two constants to weight and scale epistemic and aleatoric uncertainties by optimizing the negative log-likelihood (NLL) on the validation/calibration dataset. JUCAL can be applied to any trained ensemble of classifiers (e.g., transformers, CNNs, or tree-based methods), with minimal computational overhead, without requiring access to the models' internal parameters. We experimentally evaluate JUCAL on various text classification tasks, for ensembles of varying sizes and with different ensembling strategies. Our experiments show that JUCAL significantly outperforms SOTA calibration methods across all considered classification tasks, reducing NLL and predictive set size by up to 15% and 20%, respectively. Interestingly, even applying JUCAL to an ensemble of size 5 can outperform temperature-scaled ensembles of size up to 50 in terms of NLL and predictive set size, resulting in up to 10 times smaller inference costs. Thus, we propose JUCAL as a new go-to method for calibrating ensembles in classification.",
      "published": "2026-02-23",
      "updated": "2026-02-23",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2602.20153v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20153v1",
      "doi": null
    },
    {
      "id": "2602.20151",
      "title": "Conformal Risk Control for Non-Monotonic Losses",
      "authors": [
        "Anastasios N. Angelopoulos"
      ],
      "abstract": "Conformal risk control is an extension of conformal prediction for controlling risk functions beyond miscoverage. The original algorithm controls the expected value of a loss that is monotonic in a one-dimensional parameter. Here, we present risk control guarantees for generic algorithms applied to possibly non-monotonic losses with multidimensional parameters. The guarantees depend on the stability of the algorithm -- unstable algorithms have looser guarantees. We give applications of this technique to selective image classification, FDR and IOU control of tumor segmentations, and multigroup debiasing of recidivism predictions across overlapping race and sex groups using empirical risk minimization.",
      "published": "2026-02-23",
      "updated": "2026-02-23",
      "categories": [
        "stat.ME",
        "cs.LG",
        "math.ST",
        "stat.ML"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.20151v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20151v1",
      "doi": null
    },
    {
      "id": "2602.20118",
      "title": "Improving the Power of Bonferroni Adjustments under Joint Normality and Exchangeability",
      "authors": [
        "Caleb Hiltunen",
        "Yeonwoo Rho"
      ],
      "abstract": "Bonferroni's correction is a popular tool to address multiplicity but is notorious for its low power when tests are dependent. This paper proposes a practical modification of Bonferroni's correction when test statistics are jointly normal and exchangeable. This method is intuitive to practitioners and achieves higher power in sparse alternatives, as our simulations suggest. We also prove that this method successfully controls the family-wise error rate at any significance level.",
      "published": "2026-02-23",
      "updated": "2026-02-23",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.20118v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20118v1",
      "doi": null
    },
    {
      "id": "2602.20115",
      "title": "Compound decisions and empirical Bayes via Bayesian nonparametrics",
      "authors": [
        "Nikolaos Ignatiadis",
        "Sid Kankanala"
      ],
      "abstract": "We study the Gaussian sequence compound decision problem and analyze a Bayesian nonparametric estimator from an empirical Bayes, regret-based perspective. Motivated by sharp results for the classical nonparametric maximum likelihood estimator (NPMLE), we ask whether an analogous guarantee can be obtained using a standard Bayesian nonparametric prior. We show that a Dirichlet-process-based Bayesian procedure achieves near-optimal regret bounds. Our main results are stated in the compound decision framework, where the mean vector is treated as fixed, while we also provide parallel guarantees under a hierarchical model in which the means are drawn from a true unknown prior distribution. The posterior mean Bayes rule is, a fortiori, admissible, whereas we show that the NPMLE plug-in rule is inadmissible.",
      "published": "2026-02-23",
      "updated": "2026-02-23",
      "categories": [
        "math.ST",
        "econ.EM",
        "stat.ME"
      ],
      "primaryCategory": "math.ST",
      "pdfUrl": "https://arxiv.org/pdf/2602.20115v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20115v1",
      "doi": null
    },
    {
      "id": "2602.20071",
      "title": "Estimators of different delta coefficients based on the unbiased estimator of the expected proportions of agreements",
      "authors": [
        "A. Martín Andrés",
        "M. Álvarez Hernández"
      ],
      "abstract": "To measure the degree of agreement between two observers that independently classify $n$ subjects within $K$ categories, it is common to use different kappa type coefficients, the most common of which is the $κ_C$ coefficient (Cohen's kappa). As $κ_C$ has some weaknesses -such as its poor performance with highly unbalanced marginal distributions-, the $Δ$ coefficient is sometimes used, based on the $delta$ response model. This model allows us to obtain other parameters like: (a) the $α_i$ contribution of each $i$ category to the value of the global agreement $Δ=\\sum α_i$; and (b) the consistency $\\mathcal{S}_i$ in the category $i$ (degree of agreement in the category $i$), a more appropriate parameter than the kappa value obtained by collapsing the data into the category $i$. It has recently been shown that the classic estimator $\\hatκ_C$ underestimates $κ_C$, having obtained a new estimator $\\hatκ_{CU}$ which is less biased. This article demonstrates that something similar happens to the known estimators $\\hatΔ$, $\\hatα_i$, and $\\hat{\\mathcal{S}}_i$ of $Δ$, $α_i$ and $\\mathcal{S}_i$ (respectively), proposes new and less biased estimators $\\hatΔ_U$, $\\hatα_{iU}$, and $\\hat{\\mathcal{S}}_{iU}$, determines their variances, analyses the behaviour of all estimators, and concludes that the new estimators should be used when $n$ or $K$ are small (at least when $n\\leq 50$ or $K\\leq 3$). Additionally, the case where one of the raters is a gold standard is contemplated, in which situation two new parameters arise: the $conformity$ (the rater's capability to recognize a subject in the category $i$) and the $predictivity$ (the reliability of a response $i$ by the rater).",
      "published": "2026-02-23",
      "updated": "2026-02-23",
      "categories": [
        "math.ST",
        "stat.AP",
        "stat.ME"
      ],
      "primaryCategory": "math.ST",
      "pdfUrl": "https://arxiv.org/pdf/2602.20071v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20071v1",
      "doi": null
    },
    {
      "id": "2602.20029",
      "title": "Covariance estimation for derivatives of functional data using an additive penalty in P-splines",
      "authors": [
        "Yueyun Zhu",
        "Steven Golovkine",
        "Norma Bargary",
        "Andrew J. Simpkin"
      ],
      "abstract": "P-splines provide a flexible and computationally efficient smoothing framework and are commonly used for derivative estimation in functional data. Including an additive penalty term in P-splines has been shown to improve estimates of derivatives. We propose a method which incorporates the fast covariance estimation (FACE) algorithm with an additive penalty in P-splines. The proposed method is used to estimate derivatives of covariance for functional data, which play an important role in derivative-based functional principal component analysis (FPCA). Following this, we provide an algorithm for estimating the eigenfunctions and their corresponding scores in derivative-based FPCA. For comparison, we evaluate our algorithm against an existing function \\texttt{FPCAder()} in simulation. In addition, we extend the algorithm to multivariate cases, referred to as derivative multivariate functional principal component analysis (DMFPCA). DMFPCA is applied to joint angles in human movement data, where the derivative-based scores demonstrate strong performance in distinguishing locomotion tasks.",
      "published": "2026-02-23",
      "updated": "2026-02-23",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.20029v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20029v1",
      "doi": null
    },
    {
      "id": "2602.20007",
      "title": "Order Dependence in the Moving-Range Sigma Estimator: A Total-Variance Decomposition",
      "authors": [
        "Andrew T. Karl"
      ],
      "abstract": "In Individuals and Moving Range (I-MR) charts, the process standard deviation is often estimated by the span-2 average moving range, scaled by the usual constant $d_2$. Unlike the sample standard deviation, this estimator depends on the observation order: permuting the values can change the average moving range. We make this dependence explicit by modeling the order as an independent uniformly random permutation. A direct application of the law of total variance then decomposes its variance into a component due to ordering and a component due to the realized values. Averaging over all permutations yields a simple order-invariant baseline for the moving-range estimator: the sample Gini mean difference divided by $d_2$. Simulations quantify the resulting fraction of variance attributable to ordering under i.i.d. Normal sampling, and two NIST examples illustrate a typical ordering and an ordering with strong serial structure relative to random permutations of the same values.",
      "published": "2026-02-23",
      "updated": "2026-02-23",
      "categories": [
        "math.ST",
        "stat.ME"
      ],
      "primaryCategory": "math.ST",
      "pdfUrl": "https://arxiv.org/pdf/2602.20007v1",
      "arxivUrl": "http://arxiv.org/abs/2602.20007v1",
      "doi": null
    },
    {
      "id": "2602.19988",
      "title": "Change point analysis of high-dimensional data using random projections",
      "authors": [
        "Yi Xu",
        "Yeonwoo Rho"
      ],
      "abstract": "This paper develops a novel change point identification method for high-dimensional data using random projections. By projecting high-dimensional time series into a one-dimensional space, we are able to leverage the rich literature for univariate time series. We propose applying random projections multiple times and then combining the univariate test results using existing multiple comparison methods. Simulation results suggest that the proposed method tends to have better size and power, with more accurate location estimation. At the same time, random projections may introduce variability in the estimated locations. To enhance stability in practice, we recommend repeating the procedure, and using the mode of the estimated locations as a guide for the final change point estimate. An application to an Australian temperature dataset is presented. This study, though limited to the single change point setting, demonstrates the usefulness of random projections in change point analysis.",
      "published": "2026-02-23",
      "updated": "2026-02-23",
      "categories": [
        "stat.ME",
        "stat.CO"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.19988v1",
      "arxivUrl": "http://arxiv.org/abs/2602.19988v1",
      "doi": null
    },
    {
      "id": "2602.19954",
      "title": "A Two-Step Spatio-Temporal Framework for Turbine-Height Wind Estimation at Unmonitored Sites from Sparse Meteorological Data",
      "authors": [
        "Eamonn Organ",
        "Maeve Upton",
        "Denis Allard",
        "Lionel Benoit",
        "James Sweeney"
      ],
      "abstract": "Accurate estimates of wind speeds at wind turbine hub heights are crucial for both wind resource assessment and day-to-day management of electricity grids with high renewable penetration. In the absence of direct measurements, parametric models are commonly used to extrapolate wind speeds from observed heights to turbine heights. Recent literature has proposed extensions to allow for spatially or temporally varying vertical wind gradients, that is, the rate at which wind speed changes with height. However, these approaches typically assume that reference height and hub height measurements are available at the same locations, which limits their applicability in operational settings where meteorological stations and wind farms are spatially separated. In this paper, we develop a two-step spatio-temporal framework to estimate turbine height wind speeds using only open-access observations from sparse meteorological stations. First, a non-parametric generalized additive model is trained on reanalysis data to perform vertical height extrapolation. Second, a spatial Gaussian process model interpolates these hub-height estimates to wind farm locations while explicitly propagating uncertainty from the height extrapolation stage. The proposed framework enables the construction of high-resolution, sub-hourly turbine-height wind speed time series and spatial wind maps using data available in real time, capabilities not provided by existing reanalysis products. We further provide calibrated uncertainty estimates that account for both vertical extrapolation and spatial interpolation errors. The approach is validated using hub-height measurements from seven operational wind farms in Ireland, demonstrating improved accuracy relative to ERA5 reanalysis while relying solely on real-time, open-access data.",
      "published": "2026-02-23",
      "updated": "2026-02-23",
      "categories": [
        "stat.AP"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.19954v1",
      "arxivUrl": "http://arxiv.org/abs/2602.19954v1",
      "doi": null
    },
    {
      "id": "2602.19952",
      "title": "A Bayesian Framework for Post-disruption Travel Time Prediction in Metro Networks",
      "authors": [
        "Shayan Nazemi",
        "Aurélie Labbe",
        "Stefan Steiner",
        "Pratheepa Jeganathan",
        "Martin Trépanier",
        "Léo R. Belzile"
      ],
      "abstract": "Disruptions are an inherent feature of transportation systems, occurring unpredictably and with varying durations. Even after an incident is reported as resolved, disruptions can induce irregular train operations that generate substantial uncertainty in passenger waiting and travel times. Accurately forecasting post-disruption travel times therefore remains a critical challenge for transit operators and passenger information systems. This paper develops a Bayesian spatiotemporal modeling framework for post-disruption train travel times that explicitly captures train interactions, headway imbalance, and non-Gaussian distributional characteristics observed during recovery periods. The proposed model decomposes travel times into delay and journey components and incorporates a moving-average error structure to represent dependence between consecutive trains. Skew-normal and skew-$t$ distributions are employed to flexibly accommodate heteroskedasticity, skewness, and heavy-tailed behavior in post-disruption travel times. The framework is evaluated using high-resolution track-occupancy and disruption log data from the Montréal metro system, covering two lines in both travel directions. Empirical results indicate that post-disruption travel times exhibit pronounced distributional asymmetries that vary with traveled distance, as well as significant error dependence across trains. The proposed models consistently outperform baseline specifications in both point prediction accuracy and uncertainty quantification, with the skew-$t$ model demonstrating the most robust performance for longer journeys. These findings underscore the importance of incorporating both distributional flexibility and error dependence when forecasting post-disruption travel times in urban rail systems.",
      "published": "2026-02-23",
      "updated": "2026-02-23",
      "categories": [
        "stat.AP",
        "stat.ME",
        "stat.ML"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.19952v1",
      "arxivUrl": "http://arxiv.org/abs/2602.19952v1",
      "doi": null
    },
    {
      "id": "2602.19922",
      "title": "Transfer Learning with Network Embeddings under Structured Missingness",
      "authors": [
        "Mengyan Li",
        "Xiaoou Li",
        "Kenneth D Mandl",
        "Tianxi Cai"
      ],
      "abstract": "Modern data-driven applications increasingly rely on large, heterogeneous datasets collected across multiple sites. Differences in data availability, feature representation, and underlying populations often induce structured missingness, complicating efforts to transfer information from data-rich settings to those with limited data. Many transfer learning methods overlook this structure, limiting their ability to capture meaningful relationships across sites. We propose TransNEST (Transfer learning with Network Embeddings under STructured missingness), a framework that integrates graphical data from source and target sites with prior group structure to construct and refine network embeddings. TransNEST accommodates site-specific features, captures within-group heterogeneity and between-site differences adaptively, and improves embedding estimation under partial feature overlap. We establish the convergence rate for the TransNEST estimator and demonstrate strong finite-sample performance in simulations. We apply TransNEST to a multi-site electronic health record study, transferring feature embeddings from a general hospital system to a pediatric hospital system. Using a hierarchical ontology structure, TransNEST improves pediatric embeddings and supports more accurate pediatric knowledge extraction, achieving the best accuracy for identifying pediatric-specific relational feature pairs compared with benchmark methods.",
      "published": "2026-02-23",
      "updated": "2026-02-23",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.19922v1",
      "arxivUrl": "http://arxiv.org/abs/2602.19922v1",
      "doi": null
    },
    {
      "id": "2602.19851",
      "title": "Orthogonal Uplift Learning with Permutation-Invariant Representations for Combinatorial Treatments",
      "authors": [
        "Xinyan Su",
        "Jiacan Gao",
        "Mingyuan Ma",
        "Xiao Xu",
        "Xinrui Wan",
        "Tianqi Gu",
        "Enyun Yu",
        "Jiecheng Guo",
        "Zhiheng Zhang"
      ],
      "abstract": "We study uplift estimation for combinatorial treatments. Uplift measures the pure incremental causal effect of an intervention (e.g., sending a coupon or a marketing message) on user behavior, modeled as a conditional individual treatment effect. Many real-world interventions are combinatorial: a treatment is a policy that specifies context-dependent action distributions rather than a single atomic label. Although recent work considers structured treatments, most methods rely on categorical or opaque encodings, limiting robustness and generalization to rare or newly deployed policies. We propose an uplift estimation framework that aligns treatment representation with causal semantics. Each policy is represented by the mixture it induces over contextaction components and embedded via a permutation-invariant aggregation. This representation is integrated into an orthogonalized low-rank uplift model, extending Robinson-style decompositions to learned, vector-valued treatments. We show that the resulting estimator is expressive for policy-induced causal effects, orthogonally robust to nuisance estimation errors, and stable under small policy perturbations. Experiments on large-scale randomized platform data demonstrate improved uplift accuracy and stability in long-tailed policy regimes",
      "published": "2026-02-23",
      "updated": "2026-02-23",
      "categories": [
        "stat.ME",
        "cs.LG"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.19851v1",
      "arxivUrl": "http://arxiv.org/abs/2602.19851v1",
      "doi": null
    },
    {
      "id": "2602.19838",
      "title": "Optimality of the Half-Order Exponent in the Turing-Good Identities for Bayes Factors",
      "authors": [
        "Kensuke Okada"
      ],
      "abstract": "Bayes factors are widely computed by Monte Carlo, yet heavy-tailed sampling distributions can make numerical validation unreliable. The Turing--Good identities provide exact moment equalities for powers of a Bayes factor (a density ratio). When these identities are used as Good-check diagnostics, the power choice becomes a statistical design parameter. We develop a nonasymptotic variance theory for Monte Carlo evaluation of the identities and show that the half-order (square-root) power is uniquely minimax-stable: it equalizes variability across the two model orientations and is the only choice that guarantees finite second moments in a distribution-free worst-case sense over all mutually absolutely continuous model pairs. This yields a balanced two-sample half-order diagnostic that is symmetric in model labeling and has a uniform variance bound at fixed computational budget; in small-overlap regimes it is guaranteed to be no less efficient than the standard one-sided Turing check. Simulations for binomial Bayes factor workflows illustrate stable finite-sample behavior and sensitivity to simulator--evaluator mismatches. We further connect the half-order overlap viewpoint to stable primitives for normalizing-constant ratios and importance-sampling degeneracy summaries.",
      "published": "2026-02-23",
      "updated": "2026-02-23",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.19838v1",
      "arxivUrl": "http://arxiv.org/abs/2602.19838v1",
      "doi": null
    },
    {
      "id": "2602.19775",
      "title": "Exact Discrete Stochastic Simulation with Deep-Learning-Scale Gradient Optimization",
      "authors": [
        "Jose M. G. Vilar",
        "Leonor Saiz"
      ],
      "abstract": "Exact stochastic simulation of continuous-time Markov chains (CTMCs) is essential when discreteness and noise drive system behavior, but the hard categorical event selection in Gillespie-type algorithms blocks gradient-based learning. We eliminate this constraint by decoupling forward simulation from backward differentiation, with hard categorical sampling generating exact trajectories and gradients propagating through a continuous massively-parallel Gumbel-Softmax straight-through surrogate. Our approach enables accurate optimization at parameter scales over four orders of magnitude beyond existing simulators. We validate for accuracy, scalability, and reliability on a reversible dimerization model (0.09% error), a genetic oscillator (1.2% error), a 203,796-parameter gene regulatory network achieving 98.4% MNIST accuracy (a prototypical deep-learning multilayer perceptron benchmark), and experimental patch-clamp recordings of ion channel gating (R^2 = 0.987) in the single-channel regime. Our GPU implementation delivers 1.9 billion steps per second, matching the scale of non-differentiable simulators. By making exact stochastic simulation massively parallel and autodiff-compatible, our results enable high-dimensional parameter inference and inverse design across systems biology, chemical kinetics, physics, and related CTMC-governed domains.",
      "published": "2026-02-23",
      "updated": "2026-02-23",
      "categories": [
        "q-bio.QM",
        "cond-mat.stat-mech",
        "cs.LG",
        "physics.comp-ph",
        "q-bio.MN"
      ],
      "primaryCategory": "q-bio.QM",
      "pdfUrl": "https://arxiv.org/pdf/2602.19775v1",
      "arxivUrl": "http://arxiv.org/abs/2602.19775v1",
      "doi": null
    },
    {
      "id": "2602.19774",
      "title": "Spatio-temporal modeling of urban extreme rainfall events at high resolution",
      "authors": [
        "Chloé Serre-Combe",
        "Nicolas Meyer",
        "Thomas Opitz",
        "Gwladys Toulemonde"
      ],
      "abstract": "Modeling precipitation and its accumulation over time and space is essential for flood risk assessment. We here analyze rainfall data collected over several years through a microscale precipitation sensor network in Montpellier, France, by the OMSEV observatory. A novel spatio-temporal stochastic model is proposed for high-resolution urban rainfall and combines realistic marginal behavior and flexible extremal dependence structure. Rainfall intensities are described by the Extended Generalized Pareto Distribution (EGPD), capturing both moderate and extreme events without threshold selection. Based on spatial extreme-value theory, dependence during extreme episodes is modeled by an r-Pareto process with a non-separable variogram including episode-specific advection, allowing the displacement of rainfall cells to be represented explicitly. Parameters are estimated by a composite likelihood based on joint exceedances, and empirical advection velocities are derived from radar reanalysis. The model accurately reproduces the spatio-temporal structure of extreme rainfall observed in the Montpellier OMSEV network and enables realistic stochastic scenario generation for flood risk assessment.",
      "published": "2026-02-23",
      "updated": "2026-02-23",
      "categories": [
        "stat.AP"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.19774v1",
      "arxivUrl": "http://arxiv.org/abs/2602.19774v1",
      "doi": null
    },
    {
      "id": "2602.19761",
      "title": "Ensemble Machine Learning and Statistical Procedures for Dynamic Predictions of Time-to-Event Outcomes",
      "authors": [
        "Nina van Gerwen",
        "Sten Willemsen",
        "Bettina E. Hansen",
        "Christophe Corpechot",
        "Marco Carbone",
        "Cynthia Levy",
        "Maria-Carlota Londõno",
        "Atsushi Tanaka",
        "Palak Trivedi",
        "Alejandra Villamil",
        "Gideon Hirschfield",
        "Dimitris Rizopoulos"
      ],
      "abstract": "Dynamic predictions for longitudinal and time-to-event outcomes have become a versatile tool in precision medicine. Our work is motivated by the application of dynamic predictions in the decision-making process for primary biliary cholangitis patients. For these patients, serial biomarker measurements (e.g., bilirubin and alkaline phosphatase levels) are routinely collected to inform treating physicians of the risk of liver failure and guide clinical decision-making. Two popular statistical approaches to derive dynamic predictions are joint modelling and landmarking. However, recently, machine learning techniques have also been proposed. Each approach has its merits, and no single method exists to outperform all others. Consequently, obtaining the best possible survival estimates is challenging. Therefore, we extend the Super Learner framework to combine dynamic predictions from different models and procedures. Super Learner is an ensemble learning technique that allows users to combine different prediction algorithms to improve predictive accuracy and flexibility. It uses cross-validation and different objective functions of performance (e.g., squared loss) that suit specific applications to build the optimally weighted combination of predictions from a library of candidate algorithms. In our work, we pay special attention to appropriate objective functions for Super Learner to obtain the most optimal weighted combination of dynamic predictions. In our primary biliary cholangitis application, Super Learner presented unique benefits due to its ability to flexibly combine outputs from a diverse set of models with varying assumptions for equal or better predictive performance than any model fit separately.",
      "published": "2026-02-23",
      "updated": "2026-02-23",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.AP"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2602.19761v1",
      "arxivUrl": "http://arxiv.org/abs/2602.19761v1",
      "doi": null
    },
    {
      "id": "2602.19740",
      "title": "Volatility Spillovers in China's Real Estate Crisis: A Network Approach",
      "authors": [
        "Julia Manso"
      ],
      "abstract": "Sentiment towards the Chinese real estate sector has deteriorated following the introduction of financing constraints in 2020 with the ''three red lines.\" Forcing developers to restructure their debt, the policy triggered a cascade of financing troubles, defaults, and reduced housing demand, ultimately culminating in a prolonged real estate crisis. This paper utilizes a network approach in line with Demirer et al. (2018) and Diebold and Yilmaz (2014) to measure daily time-varying connectedness in the stock return volatilities of major Chinese real estate developers throughout the crisis. Focusing on spillover between companies as reflected by market perception, this paper examines how connectedness evolves over time across firms with different regional exposures and state-ownership statuses, filling a gap in the literature to elucidate where property demand and real estate firm trustworthiness have deteriorated most. An event-study analysis of four key moments of the crisis outlines distinct phases of market sentiment: with the introduction of the three red lines, connectedness primarily reflects shared exposure and a uniform shock to the market. Then, the early unrest surrounding Evergrande exposes strong regional differentiation, with firms concentrated in less developed regions receiving significant spillover. By one year into the crisis, previously stable regions receive higher levels of spillover, and there is evidence of a substitution effect towards private developers. Two years into the crisis, the market has much less homogeneity in effects across regions and state-ownership status: major shocks induce minimal network changes, reflecting how investors have already priced in their beliefs. This paper also offers one of the most extensive timelines of the Chinese real estate crisis to date, and a new R package, GephiForR, was created for the network visualization in this paper.",
      "published": "2026-02-23",
      "updated": "2026-02-23",
      "categories": [
        "econ.EM",
        "stat.AP"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.19740v1",
      "arxivUrl": "http://arxiv.org/abs/2602.19740v1",
      "doi": null
    },
    {
      "id": "2602.19738",
      "title": "Individualized Causal Effects under Network Interference with Combinatorial Treatments",
      "authors": [
        "Yunping Lu",
        "Haoang Chi",
        "Qirui Hu",
        "Zhiheng Zhang"
      ],
      "abstract": "Modern causal decision-making increasingly demands individualized treatment-effect estimation in networks where interventions are high-dimensional, combinatorial vectors. While network interference, effect heterogeneity, and multi-dimensional treatments have been studied separately, their intersection yields an exponentially large intervention space that makes standard identification tools and low-dimensional exposure mappings untenable. We bridge this gap with a unified framework that constructs a \\emph{global potential-outcome emulator} for unit-level inference. Our method combines (1) rooted network configurations to leverage local smoothness, (2) doubly robust orthogonalization to mitigate confounding from network position and covariates, and (3) sparse spectral learning to efficiently estimate response surfaces over the $2^p$-dimensional treatment space. We also decompose networked effects into own-treatment, structural, and interaction components, and provide finite-sample error bounds and asymptotic consistency guarantees. Overall, we show that individualized causal inference remains feasible in high-dimensional networked settings without collapsing the intervention space.",
      "published": "2026-02-23",
      "updated": "2026-02-23",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.19738v1",
      "arxivUrl": "http://arxiv.org/abs/2602.19738v1",
      "doi": null
    },
    {
      "id": "2602.19705",
      "title": "Model Selection in High-Dimensional Linear Regression using Boosting with Multiple Testing",
      "authors": [
        "George Kapetanios",
        "Vasilis Sarafidis",
        "Alexia Ventouri"
      ],
      "abstract": "High-dimensional regression specification and analysis is a complex and active area of research in statistics, machine learning, and econometrics. This paper proposes a new approach, Boosting with Multiple Testing (BMT), which combines forward stepwise variable selection with the multiple testing framework of Chudik et al (2018). At each stage, the model is updated by adding only the most significant regressor conditional on those already included, while a family-wise multiple testing filter is applied to the remaining candidates. In this way, the method retains the strong screening properties of Chudik et al (2018) while operating in a less greedy manner with respect to proxy and noise variables. Using sharp probability inequalities for heterogeneous strongly mixing processes from Dendramis et al (2022), we show that BMT enjoys oracle type properties relative to an approximating model that includes all true signals and excludes pure noise variables: this model is selected with probability tending to one, and the resulting estimator achieves standard parametric rates for prediction error and coefficient estimation. Additional results establish conditions under which BMT recovers the exact true model and avoids selection of proxy signals. Monte Carlo experiments indicate that BMT performs very well relative to OCMT and Lasso type procedures, delivering higher model selection accuracy and smaller RMSE for the estimated coefficients, especially under strong multicollinearity of the regressors. Two empirical illustrations based on a large set of macro-financial indicators as covariates, show that BMT yields sparse, interpretable specifications with favourable out-of-sample performance.",
      "published": "2026-02-23",
      "updated": "2026-02-23",
      "categories": [
        "econ.EM"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.19705v1",
      "arxivUrl": "http://arxiv.org/abs/2602.19705v1",
      "doi": null
    },
    {
      "id": "2602.19703",
      "title": "Testing Effect Homogeneity and Confounding in High-Dimensional Experimental and Observational Studies",
      "authors": [
        "Ana Armendariz",
        "Martin Huber"
      ],
      "abstract": "We propose a framework for testing the homogeneity of conditional average treatment effects (CATEs) across multiple experimental and observational studies. Our approach leverages multiple randomized trials to assess whether treatment effects vary with unobserved heterogeneity that differs across trials: if CATEs are homogeneous, this indicates the absence of interactions between treatment and unobservables in the mean effect. Comparing CATEs between experimental and observational data further allows evaluation of potential confounding: if the estimands coincide, there is no unobserved confounding; if they differ, deviations may arise from unobserved confounding, effect heterogeneity, or both. We extend the framework to settings with alternative identification strategies, namely instrumental variable settings and panel data with parallel trends assumptions based on differences in differences, where effects are identified only locally for subpopulations such as compliers or treated units. In these contexts, testing homogeneity is useful for assessing whether local effects can be extrapolated to the total population. We suggest a test based on double machine learning that accommodates high-dimensional covariates in a data-driven way and investigate its finite-sample performance through a simulation study. Finally, we apply the test to the International Stroke Trial (IST), a large multi-country randomized controlled trial in patients with acute ischaemic stroke that evaluated whether early treatment with aspirin altered subsequent clinical outcomes. Our methodology provides a flexible tool for both validating identification assumptions and understanding the generalizability of estimated treatment effects.",
      "published": "2026-02-23",
      "updated": "2026-02-23",
      "categories": [
        "econ.EM",
        "stat.ML"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.19703v1",
      "arxivUrl": "http://arxiv.org/abs/2602.19703v1",
      "doi": null
    },
    {
      "id": "2602.19658",
      "title": "On covariation estimation for multivariate continuous Itô semimartingales with noise in non-synchronous observation schemes",
      "authors": [
        "Kim Christensen",
        "Mark Podolskij",
        "Mathias Vetter"
      ],
      "abstract": "This paper presents a Hayashi-Yoshida type estimator for the covariation matrix of continuous Itô semimartingales observed with noise. The coordinates of the multivariate process are assumed to be observed at highly frequent non-synchronous points. The estimator of the covariation matrix is designed via a certain combination of the local averages and the Hayashi-Yoshida estimator. Our method does not require any synchronization of the observation scheme (as e.g. previous tick method or refreshing time method) and it is robust to some dependence structure of the noise process. We show the associated central limit theorem for the proposed estimator and provide a feasible asymptotic result. Our proofs are based on a blocking technique and a stable convergence theorem for semimartingales. Finally, we show simulation results for the proposed estimator to illustrate its finite sample properties.",
      "published": "2026-02-23",
      "updated": "2026-02-23",
      "categories": [
        "econ.EM"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.19658v1",
      "arxivUrl": "http://arxiv.org/abs/2602.19658v1",
      "doi": "10.1016/j.jmva.2013.05.002"
    }
  ],
  "metadata": {
    "lastUpdated": "2026-02-25T02:51:33.678Z",
    "totalPapers": 50,
    "categories": [
      "stat.ME",
      "stat.AP",
      "econ.EM",
      "q-bio.QM"
    ],
    "queryDate": "2026-02-25"
  }
}