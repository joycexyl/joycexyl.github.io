{
  "papers": [
    {
      "id": "2602.13158",
      "title": "A new mixture model for spatiotemporal exceedances with flexible tail dependence",
      "authors": [
        "Ryan Li",
        "Brian J. Reich",
        "Emily C. Hector",
        "Reetam Majumder"
      ],
      "abstract": "We propose a new model and estimation framework for spatiotemporal streamflow exceedances above a threshold that flexibly captures asymptotic dependence and independence in the tail of the distribution. We model streamflow using a mixture of processes with spatial, temporal and spatiotemporal asymptotic dependence regimes. A censoring mechanism allows us to use only observations above a threshold to estimate marginal and joint probabilities of extreme events. As the likelihood is intractable, we use simulation-based inference powered by random forests to estimate model parameters from summary statistics of the data. Simulations and modeling of streamflow data from the U.S. Geological Survey illustrate the feasibility and practicality of our approach.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.13158v1",
      "arxivUrl": "http://arxiv.org/abs/2602.13158v1",
      "doi": null
    },
    {
      "id": "2602.13152",
      "title": "Detecting Parameter Instabilities in Functional Concurrent Linear Regression",
      "authors": [
        "Rupsa Basu",
        "Sven Otto"
      ],
      "abstract": "We develop methodology to detect structural breaks in the slope function of a concurrent functional linear regression model for functional time series in $C[0,1]$. Our test is based on a CUSUM process of regressor-weighted OLS residual functions. To accommodate both global and local changes, we propose $L^2$- and sup-norm versions, with the sup-norm particularly sensitive to spike-like changes. Under Hölder regularity and weak dependence conditions, we establish a functional strong invariance principle, derive the asymptotic null distribution, and show that the resulting tests are consistent against a broad class of alternatives with breaks in the slope function. Simulation studies illustrate finite-sample size and power. We apply the method to sports data obtained via body-worn sensors from running athletes, focusing on hip and knee joint-angle trajectories recorded during a fatiguing run. As fatigue accumulates, runners adapt their movement patterns, and sufficiently pronounced adjustments are expected to appear as a change point in the regression relationship. In this manner, we illustrate how the proposed tests support interpretable inference for biomechanical functional time series.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.13152v1",
      "arxivUrl": "http://arxiv.org/abs/2602.13152v1",
      "doi": null
    },
    {
      "id": "2602.13121",
      "title": "LinkedNN: a neural model of linkage disequilibrium decay for recent effective population size inference",
      "authors": [
        "Chris C R Smith"
      ],
      "abstract": "Summary: A bioinformatics tool is presented for estimating recent effective population size by using a neural network to automatically compute linkage disequilibrium-related features as a function of genomic distance between polymorphisms. The new method outperforms existing deep learning and summary statistic-based approaches using relatively few sequenced individuals and variant sites, making it particularly valuable for molecular ecology applications with sparse, unphased data. Availability and implementation: The program is available as an easily installable Python package with documentation here: https://pypi.org/project/linkedNN/. The open source code is available from: https://github.com/the-smith-lab/LinkedNN.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "q-bio.PE",
        "q-bio.QM"
      ],
      "primaryCategory": "q-bio.PE",
      "pdfUrl": "https://arxiv.org/pdf/2602.13121v1",
      "arxivUrl": "http://arxiv.org/abs/2602.13121v1",
      "doi": null
    },
    {
      "id": "2602.13098",
      "title": "Barron-Wiener-Laguerre models",
      "authors": [
        "Rahul Manavalan",
        "Filip Tronarp"
      ],
      "abstract": "We propose a probabilistic extension of Wiener-Laguerre models for causal operator learning. Classical Wiener-Laguerre models parameterize stable linear dynamics using orthonormal Laguerre bases and apply a static nonlinear map to the resulting features. While structurally efficient and interpretable, they provide only deterministic point estimates. We reinterpret the nonlinear component through the lens of Barron function approximation, viewing two-layer networks, random Fourier features, and extreme learning machines as discretizations of integral representations over parameter measures. This perspective naturally admits Bayesian inference on the nonlinear map and yields posterior predictive uncertainty. By combining Laguerre-parameterized causal dynamics with probabilistic Barron-type nonlinear approximators, we obtain a structured yet expressive class of causal operators equipped with uncertainty quantification. The resulting framework bridges classical system identification and modern measure-based function approximation, providing a principled approach to time-series modeling and nonlinear systems identification.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "stat.ME",
        "cs.LG"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.13098v1",
      "arxivUrl": "http://arxiv.org/abs/2602.13098v1",
      "doi": null
    },
    {
      "id": "2602.12992",
      "title": "Stratified Sampling for Model-Assisted Estimation with Surrogate Outcomes",
      "authors": [
        "Reagan Mozer",
        "Nicole E. Pashley",
        "Luke Miratrix"
      ],
      "abstract": "In many randomized trials, outcomes such as essays or open-ended responses must be manually scored as a preliminary step to impact analysis, a process that is costly and limiting. Model-assisted estimation offers a way to combine surrogate outcomes generated by machine learning or large language models with a human-coded subset, yet typical implementations use simple random sampling and therefore overlook systematic variation in surrogate prediction error. We extend this framework by incorporating stratified sampling to more efficiently allocate human coding effort. We derive the exact variance of the stratified model-assisted estimator, characterize conditions under which stratification improves precision, and identify a Neyman-type optimal allocation rule that oversamples strata with larger residual variance. We evaluate our methods through a comprehensive simulation study to assess finite-sample performance. Overall, we find stratification consistently improves efficiency when surrogate prediction errors exhibit structured bias or heteroskedasticity. We also present two empirical applications, one using data from an education RCT and one using a large observational corpus, to illustrate how these methods can be implemented in practice using ChatGPT-generated surrogate outcomes. Overall, this framework provides a practical design-based approach for leveraging surrogate outcomes and strategically allocating human coding effort to obtain unbiased estimates with greater efficiency. While motivated by text-as-data applications, the methodology applies broadly to any setting where outcome measurement is costly or prohibitive, and can be applied to comparisons across groups or estimating the mean of a single group.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.12992v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12992v1",
      "doi": null
    },
    {
      "id": "2602.12974",
      "title": "Statistical Opportunities in Neuroimaging",
      "authors": [
        "Jian Kang",
        "Thomas Nichols",
        "Lexin Li",
        "Martin A. Lindquist",
        "Hongtu Zhu"
      ],
      "abstract": "Neuroimaging has profoundly enhanced our understanding of the human brain by characterizing its structure, function, and connectivity through modalities like MRI, fMRI, EEG, and PET. These technologies have enabled major breakthroughs across the lifespan, from early brain development to neurodegenerative and neuropsychiatric disorders. Despite these advances, the brain is a complex, multiscale system, and neuroimaging measurements are correspondingly high-dimensional. This creates major statistical challenges, including measurement noise, motion-related artifacts, substantial inter-subject and site/scanner variability, and the sheer scale of modern studies. This paper explores statistical opportunities and challenges in neuroimaging across four key areas: (i) brain development from birth to age 20, (ii) the adult and aging brain, (iii) neurodegeneration and neuropsychiatric disorders, and (iv) brain encoding and decoding. After a quick tutorial on major imaging technologies, we review cutting-edge studies, underscore data and modeling challenges, and highlight research opportunities for statisticians. We conclude by emphasizing that close collaboration among statisticians, neuroscientists, and clinicians is essential for translating neuroimaging advances into improved diagnostics, deeper mechanistic insight, and more personalized treatments.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "stat.AP",
        "cs.CV",
        "stat.ME"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.12974v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12974v1",
      "doi": null
    },
    {
      "id": "2602.12900",
      "title": "A unified testing approach for log-symmetry using Fourier methods",
      "authors": [
        "Ganesh Vishnu Avhad",
        "Sudheesh K. Kattumannil"
      ],
      "abstract": "Continuous and strictly positive data that exhibit skewness and outliers frequently arise in many applied disciplines. Log-symmetric distributions provide a flexible framework for modeling such data. In this article, we develop new goodness-of-fit tests for log-symmetric distributions based on a recent characterization. These tests utilize the characteristic function as a novel tool and are constructed using an $L^2$-type weighted distance measure. The asymptotic properties of the resulting test statistic are studied. The finite-sample performance of the proposed method is assessed via Monte Carlo simulations and compared with existing procedures. The results under a range of alternative distributions indicate superior empirical power, while the proposed test also exhibits substantial computational efficiency compared to existing methods. The methodology is further illustrated using real data sets to demonstrate practical applicability.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.12900v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12900v1",
      "doi": null
    },
    {
      "id": "2602.12845",
      "title": "Small area estimation using incomplete auxiliary information",
      "authors": [
        "Donatas Šlevinskas",
        "Ieva Burakauskaitė",
        "Andrius Čiginas"
      ],
      "abstract": "Auxiliary information is increasingly available from administrative and other data sources, but it is often incomplete and of non-probability origin. We propose a two-step small area estimation approach in which the first step relies on design-based model calibration and exploits a large non-probability source providing a noisy proxy of the study variable for only part of the population. A unit-level measurement-error working model is fitted on the linked overlap between the probability survey and the external source, and its predictions are incorporated through domain-specific model-calibration constraints to obtain approximately design-unbiased domain totals. These totals and their variance estimates are then used in a Fay-Herriot area-level model with exactly known covariates to produce empirical best linear unbiased predictors. The approach is demonstrated in three enterprise survey settings from official statistics by integrating probability sample data with (i) administrative records, (ii) a cut-off data source, and (iii) web-scraped online information. Empirical comparisons show consistent improvements in domain-level precision over direct estimation and over a Fay-Herriot benchmark that directly incorporates the proxy information as an error-prone covariate. These gains are achieved without modeling the selection mechanism of the non-probability sample.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.12845v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12845v1",
      "doi": null
    },
    {
      "id": "2602.12842",
      "title": "Some bivariate distributions on a discrete torus with application to wind direction datasets",
      "authors": [
        "Brajesh Kumar Dhakad",
        "Jayant Jha",
        "Debepsita Mukherjee"
      ],
      "abstract": "Many datasets are observed on a finite set of equally spaced directions instead of the exact angles, such as the wind direction data. However, in the statistical literature, bivariate models are only available for continuous circular random variables. This article presents two bivariate circular distributions, namely bivariate wrapped geometric (BWG) and bivariate generalized wrapped geometric (BGWG), for analyzing bivariate discrete circular data. We consider wrapped geometric distributions and a trigonometric function to construct the models. The models are analytically tractable due to the exact closed-form expressions for the trigonometric moments. We thoroughly discuss the distributional properties of the models, including the interpretation of parameters and dependence structure. The estimation methodology based on maximizing the likelihood functions is illustrated for simulated datasets. Finally, the proposed distributions are utilized to analyze pairwise wind direction measurements obtained at different stations in India, and the interpretations for the fitted models are briefly discussed.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.12842v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12842v1",
      "doi": null
    },
    {
      "id": "2602.12782",
      "title": "Empirical Validation of a Dual-Defense Mechanism Reshaping Wholesale Electricity Price Dynamics in Singapore",
      "authors": [
        "Huang Zhenyu",
        "Yuan Zhao"
      ],
      "abstract": "While ex-ante screening and static price caps are global standards for mitigating price volatility, Singapore's electricity market employs a unique dual-defense mechanism integrating vesting contracts (VC) with a temporary price cap (TPC). Using high-frequency data from 2021 to 2024, this paper evaluates this mechanism and yields three primary findings. First, a structural trade-off exists within the VC framework: while VC quantity (VCQ) suppresses average prices, it paradoxically exacerbates instability via liquidity squeezes. Conversely, VC price (VCP) functions as a tail-risk anchor, dominating at extreme quantiles where VCQ efficacy wanes. Second, a structural break around the 2023 reform reveals a fundamental re-mapping of price dynamics; the previously positive pass-through from offer ratios to clearing prices was largely neutralized post-reform. Furthermore, diagnostics near the TPC threshold show no systematic evidence of strategic bid shading, confirming the TPC's operational integrity. Third, the dual-defense mechanism exhibits a critical synergy that resolves the volatility trade-off. The TPC reverses the volatility penalty of high VCQ, shifting the elasticity of conditional volatility from a destabilizing 0.636 to a stabilizing -0.213. This synergy enables the framework to enhance tail-risk control while eliminating liquidity-related stability costs. We conclude that this dual-defense mechanism successfully decouples price suppression from liquidity risks, thereby maximizing market stability.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "eess.SY",
        "econ.EM"
      ],
      "primaryCategory": "eess.SY",
      "pdfUrl": "https://arxiv.org/pdf/2602.12782v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12782v1",
      "doi": null
    },
    {
      "id": "2602.12730",
      "title": "EMERALD-UI: An interactive web application to unveil novel protein biology hidden in the suboptimal-alignment space",
      "authors": [
        "Andrei Preoteasa",
        "Andreas Grigorjew",
        "Alexandru I. Tomescu",
        "Hajk-Georg Drost"
      ],
      "abstract": "Life over the past four billion years has been shaped by proteins and their capacity to assemble into three dimensional conformations. Protein sequence alignments have been the enabling technology for exploring the evolution and functional adaptation of proteins across the tree of life. Recent advancements in scaling the prediction of three dimensional protein structures from primary sequence alone, revealed that different modes of conservation and function operate on the sequence and structure level. This difference in protein conservation patterns and their underlying functional change that could emerge in suboptimal alignment configurations is often ignored in optimal protein alignment approaches. We introduce EMERALD-UI, an open-source interactive web application which is designed to reveal unexplored biology by visualising stable structural conformations or protein regions hidden in the suboptimal alignment space. Availability: EMERALD-UI is available at https://algbio.github.io/emerald-ui/. Contact: hdrost001@dundee.ac.uk or alexandru.tomescu@helsinki.fi.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "q-bio.QM"
      ],
      "primaryCategory": "q-bio.QM",
      "pdfUrl": "https://arxiv.org/pdf/2602.12730v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12730v1",
      "doi": null
    },
    {
      "id": "2602.12710",
      "title": "On the relation between Global VAR Models and Matrix Time Series Models with Multiple Terms",
      "authors": [
        "Dietmar Bauer Kurtulus Kidik"
      ],
      "abstract": "Matrix valued time series (MaTS) and global vector autoregressive (GVAR) models both impose restrictions on the general VAR for multidimensional data sets, in order to bring down the number of parameters. Both models are motivated from a different viewpoint such that on first sight they do not have much in common. When investigating the models more closely, however, one notices many connections between the two model sets. This paper investigates the relations between the restrictions imposed by the two models. We show that under appropriate restrictions in both models we obtain a joint framework allowing to gain insight into the nature of GVARs from the viewpoint of MaTS.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "math.ST",
        "stat.AP"
      ],
      "primaryCategory": "math.ST",
      "pdfUrl": "https://arxiv.org/pdf/2602.12710v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12710v1",
      "doi": null
    },
    {
      "id": "2602.12702",
      "title": "Modelling multivariate ordinal time series using pairwise likelihood",
      "authors": [
        "Anna Nalpantidi",
        "Dimitris Karlis"
      ],
      "abstract": "We assume that we have multiple ordinal time series and we would like to specify their joint distribution. In general it is difficult to create multivariate distribution that can be easily used to jointly model ordinal variables and the problem becomes even more complex in the case of time series, since we have to take into consideration not only the autocorrelation of each time series and the dependence between time series, but also cross-correlation. Starting from the simplest case of two ordinal time series, we propose using copulas to specify their joint distribution. We extend our approach in higher dimensions, by approximating full likelihood with composite likelihood and especially conditional pairwise likelihood, where each bivariate model is specified by copulas. We suggest maximizing each bivariate model independently to avoid computational issues and synthesize individual estimates using weighted mean. Weights are related to the Hessian matrix of each bivariate model. Simulation studies showed that model fits well under different sample sizes. Forecasting approach is also discussed. A small real data application about unemployment state of different countries of European Union is presented to illustrate our approach.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.12702v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12702v1",
      "doi": null
    },
    {
      "id": "2602.12682",
      "title": "A Causal Framework for Quantile Residual Lifetime",
      "authors": [
        "Taekwon Hong",
        "Woojung Bae",
        "Sang Kyu Lee",
        "Dongrak Choi",
        "Jong-Hyeon Jeong"
      ],
      "abstract": "Estimating prognosis conditional on surviving an initial high-risk period is crucial in clinical research. Yet, standard metrics such as hazard ratios are often difficult to interpret, while mean-based summaries are sensitive to outliers and censoring. We propose a formal causal framework for estimating quantiles of residual lifetime among individuals surviving to a landmark time $t_0$. Our primary estimand, the \"Observed Survivor Quantile Contrast\" (OSQC), targets pragmatic prognostic differences within the observed survivor population. To estimate the OSQC, we develop a doubly robust estimator that combines propensity scores, outcome regression, and inverse probability of censoring weights, ensuring consistency under confounding and informative censoring provided that the censoring model is correctly specified and at least one additional nuisance model is correctly specified. Recognizing that the OSQC conflates causal efficacy and compositional selection, we also introduce a reweighting-based supplementary estimator for the \"Principal Survivor Quantile Contrast\" (PSQC) to disentangle these mechanisms under stronger assumptions. Extensive simulations demonstrate the robustness of the proposed estimators and clarify the role of post-treatment selection. We illustrate the framework using data from the SUPPORT study to assess the impact of right heart catheterization on residual lifetime among intensive care unit survivors, and from the NSABP B-14 trial to examine post-surgical prognosis under adjuvant tamoxifen therapy across multiple landmark times.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.12682v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12682v1",
      "doi": null
    },
    {
      "id": "2602.12577",
      "title": "Conjugate Variational Inference for Large Mixed Multinomial Logit Models and Consumer Choice",
      "authors": [
        "Weiben Zhang",
        "Ruben Loaiza-Maya",
        "Michael Stanley Smith",
        "Worapree Maneesoonthorn"
      ],
      "abstract": "Heterogeneity in multinomial choice data is often accounted for using logit models with random coefficients. Such models are called \"mixed\", but they can be difficult to estimate for large datasets. We review current Bayesian variational inference (VI) methods that can do so, and propose a new VI method that scales more effectively. The key innovation is a step that updates efficiently a Gaussian approximation to the conditional posterior of the random coefficients, addressing a bottleneck within the variational optimization. The approach is used to estimate three types of mixed logit models: standard, nested and bundle variants. We first demonstrate the improvement of our new approach over existing VI methods using simulations. Our method is then applied to a large scanner panel dataset of pasta choice. We find consumer response to price and promotion variables exhibits substantial heterogeneity at the grocery store and product levels. Store size, premium and geography are found to be drivers of store level estimates of price elasticities. Extension to bundle choice with pasta sauce improves model accuracy further. Predictions from the mixed models are more accurate than those from fixed coefficients equivalents, and our VI method provides insights in circumstances which other methods find challenging.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "stat.ME",
        "stat.AP"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.12577v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12577v1",
      "doi": null
    },
    {
      "id": "2602.12504",
      "title": "Toggling the Defiers to Relax Monotonicity: The Difference-in-Instrumental-Variables Estimand",
      "authors": [
        "Johann Caro-Burnett"
      ],
      "abstract": "Standard instrumental variables (IV) methods identify a Local Average Treatment Effect under monotonicity, which rules out defiers. In many empirical environments, however, distinct instruments may induce heterogeneous and even opposing behavioral responses. This paper introduces the Difference-in-Instrumental-Variables (DIIV) estimand, which exploits two instruments with opposing compliance patterns to recover a point-identified and behaviorally interpretable causal effect without imposing monotonicity. The estimand yields a convex combination of the marginal treatment effects on compliers and defiers, with weights reflecting differential shifts in treatment take-up across instruments. When monotonicity holds, DIIV coincides with the standard IV estimand. The approach can be implemented using simple linear transformations and standard two-stage least squares procedures. Applications using replication data illustrate its applicability in practice.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "econ.EM"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.12504v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12504v1",
      "doi": null
    },
    {
      "id": "2602.12490",
      "title": "Transformer-based CoVaR: Systemic Risk in Textual Information",
      "authors": [
        "Junyu Chen",
        "Tom Boot",
        "Lingwei Kong",
        "Weining Wang"
      ],
      "abstract": "Conditional Value-at-Risk (CoVaR) quantifies systemic financial risk by measuring the loss quantile of one asset, conditional on another asset experiencing distress. We develop a Transformer-based methodology that integrates financial news articles directly with market data to improve CoVaR estimates. Unlike approaches that use predefined sentiment scores, our method incorporates raw text embeddings generated by a large language model (LLM). We prove explicit error bounds for our Transformer CoVaR estimator, showing that accurate CoVaR learning is possible even with small datasets. Using U.S. market returns and Reuters news items from 2006--2013, our out-of-sample results show that textual information impacts the CoVaR forecasts. With better predictive performance, we identify a pronounced negative dip during market stress periods across several equity assets when comparing the Transformer-based CoVaR to both the CoVaR without text and the CoVaR using traditional sentiment measures. Our results show that textual data can be used to effectively model systemic risk without requiring prohibitively large data sets.",
      "published": "2026-02-13",
      "updated": "2026-02-13",
      "categories": [
        "econ.EM",
        "q-fin.RM",
        "stat.ML"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.12490v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12490v1",
      "doi": null
    },
    {
      "id": "2602.12483",
      "title": "Quantile Randomized Kaczmarz Algorithm with Whitelist Trust Mechanism",
      "authors": [
        "Sofiia Shvaiko",
        "Longxiu Huang",
        "Elizaveta Rebrova"
      ],
      "abstract": "Randomized Kaczmarz (RK) is a simple and fast solver for consistent overdetermined systems, but it is known to be fragile under noise. We study overdetermined $m\\times n$ linear systems with a sparse set of corrupted equations, $ {\\bf A}{\\bf x}^\\star = {\\bf b}, $where only $\\tilde{\\bf b} = {\\bf b} + \\boldsymbol{\\varepsilon}$ is observed with $\\|\\boldsymbol{\\varepsilon}\\|_0 \\le βm$. The recently introduced QuantileRK (QRK) algorithm addresses this issue by testing residuals against a quantile threshold, but computing a per-iteration quantile across many rows is costly. In this work we (i) reanalyze QRK and show that its convergence rate improves monotonically as the corruption fraction $β$ decreases; (ii) propose a simple online detector that flags and removes unreliable rows, which reduces the effective $β$ and speeds up convergence; and (iii) make the method practical by estimating quantiles from a small random subsample of rows, preserving robustness while lowering the per-iteration cost. Simulations on imaging and synthetic data demonstrate the efficiency of the proposed method.",
      "published": "2026-02-12",
      "updated": "2026-02-12",
      "categories": [
        "math.NA",
        "stat.ME"
      ],
      "primaryCategory": "math.NA",
      "pdfUrl": "https://arxiv.org/pdf/2602.12483v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12483v1",
      "doi": null
    },
    {
      "id": "2602.12435",
      "title": "Scalable Changepoint Detection for Large Spatiotemporal Data on the Sphere",
      "authors": [
        "Samantha Shi-Jun",
        "Bo Li"
      ],
      "abstract": "We propose a novel Bayesian framework for changepoint detection in large-scale spherical spatiotemporal data, with broad applicability in environmental and climate sciences. Our approach models changepoints as spatially dependent categorical variables using a multinomial probit model (MPM) with a latent Gaussian process, effectively capturing complex spatial correlation structures on the sphere. To handle the high dimensionality inherent in global datasets, we leverage stochastic partial differential equations (SPDE) and spherical harmonic transformations for efficient representation and scalable inference, drastically reducing computational burden while maintaining high accuracy. Through extensive simulation studies, we demonstrate the efficiency and robustness of the proposed method for changepoint estimation, as well as the significant computational gains achieved through the combined use of the MPM and truncated spectral representations of latent processes. Finally, we apply our method to global aerosol optical depth data, successfully identifying changepoints associated with a major atmospheric event.",
      "published": "2026-02-12",
      "updated": "2026-02-12",
      "categories": [
        "stat.ME",
        "stat.CO"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.12435v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12435v1",
      "doi": null
    },
    {
      "id": "2602.12234",
      "title": "Batch-based Bayesian Optimal Experimental Design in Linear Inverse Problems",
      "authors": [
        "Sofia Mäkinen",
        "Andrew B. Duncan",
        "Tapio Helin"
      ],
      "abstract": "Experimental design is central to science and engineering. A ubiquitous challenge is how to maximize the value of information obtained from expensive or constrained experimental settings. Bayesian optimal experimental design (OED) provides a principled framework for addressing such questions. In this paper, we study experimental design problems such as the optimization of sensor locations over a continuous domain in the context of linear Bayesian inverse problems. We focus in particular on batch design, that is, the simultaneous optimization of multiple design variables, which leads to a notoriously difficult non-convex optimization problem. We tackle this challenge using a promising strategy recently proposed in the frequentist setting, which relaxes A-optimal design to the space of finite positive measures. Our main contribution is the rigorous identification of the Bayesian inference problem corresponding to this relaxed A-optimal OED formulation. Moreover, building on recent work, we develop a Wasserstein gradient-flow -based optimization algorithm for the expected utility and introduce novel regularization schemes that guarantee convergence to an empirical measure. These theoretical results are supported by numerical experiments demonstrating both convergence and the effectiveness of the proposed regularization strategy.",
      "published": "2026-02-12",
      "updated": "2026-02-12",
      "categories": [
        "stat.ME",
        "math.OC"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.12234v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12234v1",
      "doi": null
    },
    {
      "id": "2602.12317",
      "title": "Free Lunch in Medical Image Foundation Model Pre-training via Randomized Synthesis and Disentanglement",
      "authors": [
        "Yuhan Wei",
        "Yuting He",
        "Linshan Wu",
        "Fuxiang Huang",
        "Junlin Hou",
        "Hao Chen"
      ],
      "abstract": "Medical image foundation models (MIFMs) have demonstrated remarkable potential for a wide range of clinical tasks, yet their development is constrained by the scarcity, heterogeneity, and high cost of large-scale annotated datasets. Here, we propose RaSD (Randomized Synthesis and Disentanglement), a scalable framework for pre-training MIFMs entirely on synthetic data. By modeling anatomical structures and appearance variations with randomized Gaussian distributions, RaSD exposes models to sufficient multi-scale structural and appearance perturbations, forcing them to rely on invariant and task-relevant anatomical cues rather than dataset-specific textures, thereby enabling robust and transferable representation learning. We pre-trained RaSD on 1.2 million 3D volumes and 9.6 million 2D images, and extensively evaluated the resulting models across 6 imaging modalities, 48 datasets, and 56 downstream tasks. Across all evaluated downstream tasks, RaSD consistently outperforms training-from-scratch models, achieves the best performance on 17 tasks, and remains comparable to models pre-trained on large real datasets in most others. These results demonstrate that the capacity of synthetic data alone to drive robust representation learning. Our findings establish a paradigm shift in medical AI, demonstrating that synthetic data can serve as a \"free lunch\" for scalable, privacy-preserving, and clinically generalizable foundation models.",
      "published": "2026-02-12",
      "updated": "2026-02-12",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG"
      ],
      "primaryCategory": "q-bio.QM",
      "pdfUrl": "https://arxiv.org/pdf/2602.12317v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12317v1",
      "doi": null
    },
    {
      "id": "2602.12072",
      "title": "Enhanced Forest Inventories for Habitat Mapping: A Case Study in the Sierra Nevada Mountains of California",
      "authors": [
        "Maxime Turgeon",
        "Michael Kieser",
        "Dwight Wolfe",
        "Bruce MacArthur"
      ],
      "abstract": "Traditional forest inventory systems, originally designed to quantify merchantable timber volume, often lack the spatial resolution and structural detail required for modern multi-resource ecosystem management. In this manuscript, we present an Enhanced Forest Inventory (EFI) and demonstrate its utility for high-resolution wildlife habitat mapping. The project area covers 270,000 acres of the Eldorado National Forest in California's Sierra Nevada. By integrating 118 ground-truth Forest Inventory and Analysis (FIA) plots with multi-modal remote sensing data (LiDAR, aerial photography, and Sentinel-2 satellite imagery), we developed predictive models for key forest attributes. Our methodology employed a two-tier segmentation approach, partitioning the landscape into approximately 575,000 reporting units with an average size of 0.5 acre to capture forest heterogeneity. We utilized an Elastic-Net Regression framework and automated feature selection to relate remote sensing metrics to ground-measured variables such as basal area, stems per acre, and canopy cover. These physical metrics were translated into functional habitat attributes to evaluate suitability for two focal species: the California Spotted Owl (Strix occidentalis occidentalis) and the Pacific Fisher (Pekania pennanti). Our analysis identified 25,630 acres of nesting and 26,622 acres of foraging habitat for the owl, and 25,636 acres of likely habitat for the fisher based on structural requirements like large-diameter trees and high canopy closure. The results demonstrate that EFIs provide a critical bridge between forestry and conservation ecology, offering forest managers a spatially explicit tool to monitor ecosystem health and manage vulnerable species in complex environments.",
      "published": "2026-02-12",
      "updated": "2026-02-12",
      "categories": [
        "stat.AP"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.12072v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12072v1",
      "doi": null
    },
    {
      "id": "2602.12043",
      "title": "Improved Inference for CSDID Using the Cluster Jackknife",
      "authors": [
        "Sunny R. Karim",
        "Morten Ørregaard Nielsen",
        "James G. MacKinnon",
        "Matthew D. Webb"
      ],
      "abstract": "Obtaining reliable inferences with traditional difference-in-differences (DiD) methods can be difficult. Problems can arise when both outcomes and errors are serially correlated, when there are few clusters or few treated clusters, when cluster sizes vary greatly, and in various other cases. In recent years, recognition of the ``staggered adoption'' problem has shifted the focus away from inference towards consistent estimation of treatment effects. One of the most popular new estimators is the CSDID procedure of Callaway and Sant'Anna (2021). We find that the issues of over-rejection with few clusters and/or few treated clusters are at least as severe for CSDID as for traditional DiD methods. We also propose using a cluster jackknife for inference with CSDID, which simulations suggest greatly improves inference. We provide software packages in Stata csdidjack and R didjack to calculate cluster-jackknife standard errors easily.",
      "published": "2026-02-12",
      "updated": "2026-02-12",
      "categories": [
        "econ.EM",
        "stat.ME",
        "stat.ML"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.12043v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12043v1",
      "doi": null
    },
    {
      "id": "2602.12310",
      "title": "ChemRecon: a Consolidated Meta-Database Platform for Biochemical Data Integration",
      "authors": [
        "Casper Asbjørn Eriksen",
        "Jakob Lykke Andersen",
        "Rolf Fagerberg",
        "Daniel Merkle"
      ],
      "abstract": "In this paper, we present ChemRecon, a meta-database and Python interface for integrating and exploring biochemical data across multiple heterogeneous resources by consolidating compounds, reactions, enzymes, molecular structures, and atom-to-atom maps from several major databases into a single, consistent ontology. ChemRecon enables unified querying, cross-database analysis, and the construction of graph-based representations of sets of related database entries by the traversal of inter-database connections. This facilitates information extraction which is impossible within any single database, including deriving consensus information from conflicting sources, of which identifying the most probable molecular structure associated with a given compound is just one example. The Python interface is available via pip from the Python Package Index (https://pypi.org/project/chemrecon/). ChemRecon is open-source and the source code is hosted at GitLab (https://gitlab.com/casbjorn/chemrecon). Documentation and additional information is available at https://chemrecon.org.",
      "published": "2026-02-12",
      "updated": "2026-02-12",
      "categories": [
        "q-bio.QM"
      ],
      "primaryCategory": "q-bio.QM",
      "pdfUrl": "https://arxiv.org/pdf/2602.12310v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12310v1",
      "doi": null
    },
    {
      "id": "2602.12026",
      "title": "Protein Circuit Tracing via Cross-layer Transcoders",
      "authors": [
        "Darin Tsui",
        "Kunal Talreja",
        "Daniel Saeedi",
        "Amirali Aghazadeh"
      ],
      "abstract": "Protein language models (pLMs) have emerged as powerful predictors of protein structure and function. However, the computational circuits underlying their predictions remain poorly understood. Recent mechanistic interpretability methods decompose pLM representations into interpretable features, but they treat each layer independently and thus fail to capture cross-layer computation, limiting their ability to approximate the full model. We introduce ProtoMech, a framework for discovering computational circuits in pLMs using cross-layer transcoders that learn sparse latent representations jointly across layers to capture the model's full computational circuitry. Applied to the pLM ESM2, ProtoMech recovers 82-89% of the original performance on protein family classification and function prediction tasks. ProtoMech then identifies compressed circuits that use <1% of the latent space while retaining up to 79% of model accuracy, revealing correspondence with structural and functional motifs, including binding, signaling, and stability. Steering along these circuits enables high-fitness protein design, surpassing baseline methods in more than 70% of cases. These results establish ProtoMech as a principled framework for protein circuit tracing.",
      "published": "2026-02-12",
      "updated": "2026-02-12",
      "categories": [
        "cs.LG",
        "q-bio.QM"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2602.12026v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12026v1",
      "doi": null
    },
    {
      "id": "2602.12023",
      "title": "Decomposition of Spillover Effects Under Misspecification:Pseudo-true Estimands and a Local--Global Extension",
      "authors": [
        "Yechan Park",
        "Xiaodong Yang"
      ],
      "abstract": "Applied work with interference typically models outcomes as functions of own treatment and a low-dimensional exposure mapping of others' treatments, even when that mapping may be misspecified. This raises a basic question: what policy object are exposure-based estimands implicitly targeting, and how should we interpret their direct and spillover components relative to the underlying policy question? We take as primitive the marginal policy effect, defined as the effect of a small change in the treatment probability under the actual experimental design, and show that any researcher-chosen exposure mapping induces a unique pseudo-true outcome model. This model is the best approximation to the underlying potential outcomes that depends only on the user-chosen exposure. Utilizing that representation, the marginal policy effect admits a canonical decomposition into exposure-based direct and spillover effects, and each component provides its optimal approximation to the corresponding oracle objects that would be available if interference were fully known. We then focus on a setting that nests important empirical and theoretical applications in which both local network spillovers and global spillovers, such as market equilibrium, operate. There, the marginal policy effect further decomposes asymptotically into direct, local, and global channels. An important implication is that many existing methods are more robust than previously understood once we reinterpret their targets as channel-specific components of this pseudo-true policy estimand. Simulations and a semi-synthetic experiment calibrated to a large cash-transfer experiment show that these components can be recovered in realistic experimental designs.",
      "published": "2026-02-12",
      "updated": "2026-02-12",
      "categories": [
        "econ.EM",
        "math.ST",
        "stat.ML"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.12023v1",
      "arxivUrl": "http://arxiv.org/abs/2602.12023v1",
      "doi": null
    },
    {
      "id": "2602.11873",
      "title": "Temporally resolved aortic 3D shape reconstruction from a limited number of cine 2D MRI slices",
      "authors": [
        "Gloria Wolkerstorfer",
        "Stefano Buoso",
        "Rabea Schlenker",
        "Jochen von Spiczak",
        "Robert Manka",
        "Sebastian Kozerke"
      ],
      "abstract": "Background and Objective: We propose a shape reconstruction framework to generate time-resolved, patient-specific 3D aortic geometries from a limited number of standard cine 2D magnetic resonance imaging (MRI) acquisitions. A statistical shape model of the aorta is coupled with differentiable volumetric mesh optimization to obtain personalized aortic meshes. Methods: The statistical shape model was constructed from retrospective data and optimized 2D slice placements along the aortic arch were identified. Cine 2D MRI slices were then acquired in 30 subjects (19 volunteers, 11 aortic stenosis patients). After manual segmentation, time-resolved aortic models were generated via differentiable volumetric mesh optimization to derive vessel shape features, centerline parameters, and radial wall strain. In 10 subjects, additional 4D flow MRI was acquired to compare peak-systolic shapes. Results: Anatomically accurate aortic geometries were obtained from as few as six cine 2D MRI slices, achieving a mean +/- standard deviation Dice score of (89.9 +/- 1.6) %, Intersection over Union of (81.7 +/- 2.7) %, Hausdorff distance of (7.3 +/- 3.3) mm, and Chamfer distance of (3.7 +/- 0.6) mm relative to 4D flow MRI. The mean absolute radius error was (0.8 +/- 0.6) mm. Significant age-related differences were observed for all shape features, including radial strain, which decreased progressively ((11.00 +/- 3.11) x 10-2 vs. (3.74 +/- 1.25) x 10-2 vs. (2.89 +/- 0.87) x 10-2 for young, mid-age, and elderly groups). Conclusion: The proposed method enables efficient extraction of time-resolved 3D aortic meshes from limited sets of standard cine 2D MRI acquisitions, suitable for computational shape and strain analysis.",
      "published": "2026-02-12",
      "updated": "2026-02-12",
      "categories": [
        "eess.IV",
        "physics.med-ph",
        "stat.ME"
      ],
      "primaryCategory": "eess.IV",
      "pdfUrl": "https://arxiv.org/pdf/2602.11873v1",
      "arxivUrl": "http://arxiv.org/abs/2602.11873v1",
      "doi": null
    },
    {
      "id": "2602.11712",
      "title": "Potential-energy gating for robust state estimation in bistable stochastic systems",
      "authors": [
        "Luigi Simeone"
      ],
      "abstract": "We introduce potential-energy gating, a method for robust state estimation in systems governed by double-well stochastic dynamics. The observation noise covariance of a Bayesian filter is modulated by the local value of a known or assumed potential energy function: observations are trusted when the state is near a potential minimum and progressively discounted as it approaches the barrier separating metastable wells. This physics-based mechanism differs from purely statistical robust filters, which treat all regions of state space identically, and from constrained filters, which impose hard bounds on states rather than modulating observation trust. We implement the gating within Extended, Unscented, Ensemble, and Adaptive Kalman filters and particle filters, requiring only two additional hyperparameters. Synthetic benchmarks on a Ginzburg-Landau double-well process with 10% outlier contamination and Monte Carlo validation over 100 replications show 57-80% RMSE improvement over the standard Extended Kalman Filter, all statistically significant (p < 10^{-15}, Wilcoxon signed-rank test). A naive topological baseline using only distance to the nearest well achieves 57%, confirming that the continuous energy landscape adds an additional ~21 percentage points. The method is robust to misspecification: even when assumed potential parameters deviate by 50% from their true values, improvement never falls below 47%. Comparing externally forced and spontaneous Kramers-type transitions, gating retains 68% improvement under noise-induced transitions whereas the naive baseline degrades to 30%. As an empirical illustration, we apply the framework to Dansgaard-Oeschger events in the NGRIP delta-18O ice-core record, estimating asymmetry parameter gamma = -0.109 (bootstrap 95% CI: [-0.220, -0.011], excluding zero) and demonstrating that outlier fraction explains 91% of the variance in filter improvement.",
      "published": "2026-02-12",
      "updated": "2026-02-12",
      "categories": [
        "cs.LG",
        "cs.CE",
        "nlin.CD",
        "physics.data-an",
        "stat.ME"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2602.11712v1",
      "arxivUrl": "http://arxiv.org/abs/2602.11712v1",
      "doi": null
    },
    {
      "id": "2602.11711",
      "title": "Estimation of instrument and noise parameters for inverse problem based on prior diffusion model",
      "authors": [
        "Jean-François Giovannelli"
      ],
      "abstract": "This article addresses the issue of estimating observation parameters (response and error parameters) in inverse problems. The focus is on cases where regularization is introduced in a Bayesian framework and the prior is modeled by a diffusion process. In this context, the issue of posterior sampling is well known to be thorny, and a recent paper proposes a notably simple and effective solution. Consequently, it offers an remarkable additional flexibility when it comes to estimating observation parameters. The proposed strategy enables us to define an optimal estimator for both the observation parameters and the image of interest. Furthermore, the strategy provides a means of quantifying uncertainty. In addition, MCMC algorithms allow for the efficient computation of estimates and properties of posteriors, while offering some guarantees. The paper presents several numerical experiments that clearly confirm the computational efficiency and the quality of both estimates and uncertainties quantification.",
      "published": "2026-02-12",
      "updated": "2026-02-12",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.NA",
        "stat.AP"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2602.11711v1",
      "arxivUrl": "http://arxiv.org/abs/2602.11711v1",
      "doi": null
    },
    {
      "id": "2602.11679",
      "title": "Provable Offline Reinforcement Learning for Structured Cyclic MDPs",
      "authors": [
        "Kyungbok Lee",
        "Angelica Cristello Sarteau",
        "Michael R. Kosorok"
      ],
      "abstract": "We introduce a novel cyclic Markov decision process (MDP) framework for multi-step decision problems with heterogeneous stage-specific dynamics, transitions, and discount factors across the cycle. In this setting, offline learning is challenging: optimizing a policy at any stage shifts the state distributions of subsequent stages, propagating mismatch across the cycle. To address this, we propose a modular structural framework that decomposes the cyclic process into stage-wise sub-problems. While generally applicable, we instantiate this principle as CycleFQI, an extension of fitted Q-iteration enabling theoretical analysis and interpretation. It uses a vector of stage-specific Q-functions, tailored to each stage, to capture within-stage sequences and transitions between stages. This modular design enables partial control, allowing some stages to be optimized while others follow predefined policies. We establish finite-sample suboptimality error bounds and derive global convergence rates under Besov regularity, demonstrating that CycleFQI mitigates the curse of dimensionality compared to monolithic baselines. Additionally, we propose a sieve-based method for asymptotic inference of optimal policy values under a margin condition. Experiments on simulated and real-world Type 1 Diabetes data sets demonstrate CycleFQI's effectiveness.",
      "published": "2026-02-12",
      "updated": "2026-02-12",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "math.OC",
        "stat.ME"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2602.11679v1",
      "arxivUrl": "http://arxiv.org/abs/2602.11679v1",
      "doi": null
    },
    {
      "id": "2602.11618",
      "title": "How Well Do Large-Scale Chemical Language Models Transfer to Downstream Tasks?",
      "authors": [
        "Tatsuya Sagawa",
        "Ryosuke Kojima"
      ],
      "abstract": "Chemical Language Models (CLMs) pre-trained on large scale molecular data are widely used for molecular property prediction. However, the common belief that increasing training resources such as model size, dataset size, and training compute improves both pretraining loss and downstream task performance has not been systematically validated in the chemical domain. In this work, we evaluate this assumption by pretraining CLMs while scaling training resources and measuring transfer performance across diverse molecular property prediction (MPP) tasks. We find that while pretraining loss consistently decreases with increased training resources, downstream task performance shows limited improvement. Moreover, alternative metrics based on the Hessian or loss landscape also fail to estimate downstream performance in CLMs. We further identify conditions under which downstream performance saturates or degrades despite continued improvements in pretraining metrics, and analyze the underlying task dependent failure modes through parameter space visualizations. These results expose a gap between pretraining based evaluation and downstream performance, and emphasize the need for model selection and evaluation strategies that explicitly account for downstream task characteristics.",
      "published": "2026-02-12",
      "updated": "2026-02-12",
      "categories": [
        "cs.LG",
        "q-bio.QM"
      ],
      "primaryCategory": "cs.LG",
      "pdfUrl": "https://arxiv.org/pdf/2602.11618v1",
      "arxivUrl": "http://arxiv.org/abs/2602.11618v1",
      "doi": null
    },
    {
      "id": "2602.11610",
      "title": "Improving the adjusted Benjamini--Hochberg method using e-values in knockoff-assisted variable selection",
      "authors": [
        "Aniket Biswas",
        "Aaditya Ramdas"
      ],
      "abstract": "Considering the knockoff-based multiple testing framework of Barber and Candès [2015], we revisit the method of Sarkar and Tang [2022] and identify it as a specific case of an un-normalized e-value weighted Benjamini-Hochberg procedure. Building on this insight, we extend the method to use bounded p-to-e calibrators that enable more refined and flexible weight assignments. Our approach generalizes the method of Sarkar and Tang [2022], which emerges as a special case corresponding to an extreme calibrator. Within this framework, we propose three procedures: an e-value weighted Benjamini-Hochberg method, its adaptive extension using an estimate of the proportion of true null hypotheses, and an adaptive weighted Benjamini-Hochberg method. We establish control of the false discovery rate (FDR) for the proposed methods. While we do not formally prove that the proposed methods outperform those of Barber and Candès [2015] and Sarkar and Tang [2022], simulation studies and real-data analysis demonstrate large and consistent improvement over the latter in all cases, and better performance than the knockoff method in scenarios with low target FDR, a small number of signals, and weak signal strength. Simulation studies and a real-data application in HIV-1 drug resistance analysis demonstrate strong finite sample FDR control and exhibit improved, or at least competitive, power relative to the aforementioned methods.",
      "published": "2026-02-12",
      "updated": "2026-02-12",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.11610v1",
      "arxivUrl": "http://arxiv.org/abs/2602.11610v1",
      "doi": null
    },
    {
      "id": "2602.11520",
      "title": "Locally Interpretable Individualized Treatment Rules for Black-Box Decision Models",
      "authors": [
        "Yasin Khadem Charvadeh",
        "Katherine S. Panageas",
        "Yuan Chen"
      ],
      "abstract": "Individualized treatment rules (ITRs) aim to optimize healthcare by tailoring treatment decisions to patient-specific characteristics. Existing methods typically rely on either interpretable but inflexible models or highly flexible black-box approaches that sacrifice interpretability; moreover, most impose a single global decision rule across patients. We introduce the Locally Interpretable Individualized Treatment Rule (LI-ITR) method, which combines flexible machine learning models to accurately learn complex treatment outcomes with locally interpretable approximations to construct subject-specific treatment rules. LI-ITR employs variational autoencoders to generate realistic local synthetic samples and learns individualized decision rules through a mixture of interpretable experts. Simulation studies show that LI-ITR accurately recovers true subject-specific local coefficients and optimal treatment strategies. An application to precision side-effect management in breast cancer illustrates the necessity of flexible predictive modeling and highlights the practical utility of LI-ITR in estimating optimal treatment rules while providing transparent, clinically interpretable explanations.",
      "published": "2026-02-12",
      "updated": "2026-02-12",
      "categories": [
        "stat.ME",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.11520v1",
      "arxivUrl": "http://arxiv.org/abs/2602.11520v1",
      "doi": null
    },
    {
      "id": "2602.11511",
      "title": "Representation Learning with Blockwise Missingness and Signal Heterogeneity",
      "authors": [
        "Ziqi Liu",
        "Ye Tian",
        "Weijing Tang"
      ],
      "abstract": "Unified representation learning for multi-source data integration faces two important challenges: blockwise missingness and blockwise signal heterogeneity. The former arises from sources observing different, yet potentially overlapping, feature sets, while the latter involves varying signal strengths across subject groups and feature sets. While existing methods perform well with fully observed data or uniform signal strength, their performance degenerates when these two challenges coincide, which is common in practice. To address this, we propose Anchor Projected Principal Component Analysis (APPCA), a general framework for representation learning with structured blockwise missingness that is robust to signal heterogeneity. APPCA first recovers robust group-specific column spaces using all observed feature sets, and then aligns them by projecting shared \"anchor\" features onto these subspaces before performing PCA. This projection step induces a significant denoising effect. We establish estimation error bounds for embedding reconstruction through a fine-grained perturbation analysis. In particular, using a novel spectral slicing technique, our bound eliminates the standard dependency on the signal strength of subject embeddings, relying instead solely on the signal strength of integrated feature sets. We validate the proposed method through extensive simulation studies and an application to multimodal single-cell sequencing data.",
      "published": "2026-02-12",
      "updated": "2026-02-12",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.11511v1",
      "arxivUrl": "http://arxiv.org/abs/2602.11511v1",
      "doi": null
    },
    {
      "id": "2602.11496",
      "title": "High-Dimensional Mediation Analysis for Generalized Linear Models Using Bayesian Variable Selection Guided by Mediator Correlation",
      "authors": [
        "Youngho Bae",
        "Chanmin Kim",
        "Fenglei Wang",
        "Qi Sun",
        "Kyu Ha Lee"
      ],
      "abstract": "High-dimensional mediation analysis aims to identify mediating pathways and to estimate indirect effects linking an exposure to an outcome. In this paper, we propose a Bayesian framework to address key challenges in these analyses, including high dimensionality, complex dependence among omics mediators, and non-continuous outcomes. Furthermore, commonly used approaches assume independent mediators or ignore correlations in the selection stage, which can reduce power when mediators are highly correlated. Addressing these challenges leads to a non-Gaussian likelihood and specialized selection priors, which in turn require efficient and adaptive posterior computation. Our proposed framework selects active pathways under generalized linear models while accounting for mediator dependence. Specifically, the mediators are modeled using a multivariate distribution, exposure-mediator selection is guided by a Markov random field prior on inclusion indicators, and mediator-outcome activation is restricted to mediators supported in the exposure-mediator model through a sequential subsetting Bernoulli prior. Simulation studies show improved operating characteristics in correlated-mediator settings, with appropriate error control under the global null and stable performance under model misspecification. We illustrate the method using real-world metabolomics data to study metabolites that mediate the association between adherence to the Alternate Mediterranean Diet score and two cardiometabolic outcomes.",
      "published": "2026-02-12",
      "updated": "2026-02-12",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.11496v1",
      "arxivUrl": "http://arxiv.org/abs/2602.11496v1",
      "doi": null
    },
    {
      "id": "2602.11403",
      "title": "Who's Winning? Clarifying Estimands Based on Win Statistics in Cluster Randomized Trials",
      "authors": [
        "Kenneth M. Lee",
        "Xi Fang",
        "Fan Li",
        "Michael O. Harhay"
      ],
      "abstract": "Treatment effect estimands based on win statistics, including the win ratio, win odds, and win difference are increasingly popular targets for summarizing endpoints in clinical trials. Such win estimands offer an intuitive approach for prioritizing outcomes by clinical importance. The implementation and interpretation of win estimands is complicated in cluster randomized trials (CRTs), where researchers can target fundamentally different estimands on the individual-level or cluster-level. We numerically demonstrate that individual-pair and cluster-pair win estimands can substantially differ when cluster size is informative: where outcomes and/or treatment effects depend on cluster size. With such informative cluster sizes, individual-pair and cluster-pair win estimands can even yield opposite conclusions regarding treatment benefit. We describe consistent estimators for individual-pair and cluster-pair win estimands and propose a leave-one-cluster-out jackknife variance estimator for inference. Despite being consistent, our simulations highlight that some caution is needed when implementing individual-pair win estimators due to finite-sample bias. In contrast, cluster-pair win estimators are unbiased for their respective targets. Altogether, careful specification of the target estimand is essential when applying win estimators in CRTs. Failure to clearly define whether individual-pair or cluster-pair win estimands are of primary interest may result in answering a dramatically different question than intended.",
      "published": "2026-02-11",
      "updated": "2026-02-11",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.11403v1",
      "arxivUrl": "http://arxiv.org/abs/2602.11403v1",
      "doi": null
    },
    {
      "id": "2602.11379",
      "title": "Regularized Ensemble Forecasting for Learning Weights from Historical and Current Forecasts",
      "authors": [
        "Han Su",
        "Xiaojia Guo",
        "Xiaoke Zhang"
      ],
      "abstract": "Combining forecasts from multiple experts often yields more accurate results than relying on a single expert. In this paper, we introduce a novel regularized ensemble method that extends the traditional linear opinion pool by leveraging both current forecasts and historical performances to set the weights. Unlike existing approaches that rely only on either the current forecasts or past accuracy, our method accounts for both sources simultaneously. It learns weights by minimizing the variance of the combined forecast (or its transformed version) while incorporating a regularization term informed by historical performances. We also show that this approach has a Bayesian interpretation. Different distributional assumptions within this Bayesian framework yield different functional forms for the variance component and the regularization term, adapting the method to various scenarios. In empirical studies on Walmart sales and macroeconomic forecasting, our ensemble outperforms leading benchmark models both when experts' full forecasting histories are available and when experts enter and exit over time, resulting in incomplete historical records. Throughout, we provide illustrative examples that show how the optimal weights are determined and, based on the empirical results, we discuss where the framework's strengths lie and when experts' past versus current forecasts are more informative.",
      "published": "2026-02-11",
      "updated": "2026-02-11",
      "categories": [
        "stat.AP",
        "econ.GN",
        "stat.ME"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.11379v1",
      "arxivUrl": "http://arxiv.org/abs/2602.11379v1",
      "doi": null
    },
    {
      "id": "2602.11333",
      "title": "Cross-Fitting-Free Debiased Machine Learning with Multiway Dependence",
      "authors": [
        "Kaicheng Chen",
        "Harold D. Chiang"
      ],
      "abstract": "This paper develops an asymptotic theory for two-step debiased machine learning (DML) estimators in generalised method of moments (GMM) models with general multiway clustered dependence, without relying on cross-fitting. While cross-fitting is commonly employed, it can be statistically inefficient and computationally burdensome when first-stage learners are complex and the effective sample size is governed by the number of independent clusters. We show that valid inference can be achieved without sample splitting by combining Neyman-orthogonal moment conditions with a localisation-based empirical process approach, allowing for an arbitrary number of clustering dimensions. The resulting DML-GMM estimators are shown to be asymptotically linear and asymptotically normal under multiway clustered dependence. A central technical contribution of the paper is the derivation of novel global and local maximal inequalities for general classes of functions of sums of separately exchangeable arrays, which underpin our theoretical arguments and are of independent interest.",
      "published": "2026-02-11",
      "updated": "2026-02-11",
      "categories": [
        "econ.EM",
        "stat.ML"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.11333v1",
      "arxivUrl": "http://arxiv.org/abs/2602.11333v1",
      "doi": null
    },
    {
      "id": "2602.11325",
      "title": "Amortised and provably-robust simulation-based inference",
      "authors": [
        "Ayush Bharti",
        "Charita Dellaporta",
        "Yuga Hikida",
        "François-Xavier Briol"
      ],
      "abstract": "Complex simulator-based models are now routinely used to perform inference across the sciences and engineering, but existing inference methods are often unable to account for outliers and other extreme values in data which occur due to faulty measurement instruments or human error. In this paper, we introduce a novel approach to simulation-based inference grounded in generalised Bayesian inference and a neural approximation of a weighted score-matching loss. This leads to a method that is both amortised and provably robust to outliers, a combination not achieved by existing approaches. Furthermore, through a carefully chosen conditional density model, we demonstrate that inference can be further simplified and performed without the need for Markov chain Monte Carlo sampling, thereby offering significant computational advantages, with complexity that is only a small fraction of that of current state-of-the-art approaches.",
      "published": "2026-02-11",
      "updated": "2026-02-11",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.CO",
        "stat.ME"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2602.11325v1",
      "arxivUrl": "http://arxiv.org/abs/2602.11325v1",
      "doi": null
    },
    {
      "id": "2602.11132",
      "title": "A New Look at Bayesian Testing",
      "authors": [
        "Jyotishka Datta",
        "Nicholas G. Polson",
        "Vadim Sokolov",
        "Daniel Zantedeschi"
      ],
      "abstract": "We develop a unified framework for Bayesian hypothesis testing through the theory of moderate deviations, providing explicit asymptotic expansions for Bayes risk and optimal test statistics. Our analysis reveals that Bayesian test cutoffs operate on the moderate deviation scale $\\sqrt{\\log n/n}$, in sharp contrast to the sample-size-invariant calibrations of classical testing. This fundamental difference explains the Lindley paradox and establishes the risk-theoretic superiority of Bayesian procedures over fixed-$α$ Neyman-Pearson tests. We extend the seminal Rubin (1965) program to contemporary settings including high-dimensional sparse inference, goodness-of-fit testing, and model selection. The framework unifies several classical results: Jeffreys' $\\sqrt{\\log n}$ threshold, the BIC penalty $(d/2)\\log n$, and the Chernoff-Stein error exponents all emerge naturally from moderate deviation analysis of Bayes risk. Our results provide theoretical foundations for adaptive significance levels and connect Bayesian testing to information theory through gambling-based interpretations.",
      "published": "2026-02-11",
      "updated": "2026-02-11",
      "categories": [
        "math.ST",
        "stat.ME"
      ],
      "primaryCategory": "math.ST",
      "pdfUrl": "https://arxiv.org/pdf/2602.11132v1",
      "arxivUrl": "http://arxiv.org/abs/2602.11132v1",
      "doi": null
    },
    {
      "id": "2602.11118",
      "title": "A Doubly Robust Machine Learning Approach for Disentangling Treatment Effect Heterogeneity with Functional Outcomes",
      "authors": [
        "Filippo Salmaso",
        "Lorenzo Testa",
        "Francesca Chiaromonte"
      ],
      "abstract": "Causal inference is paramount for understanding the effects of interventions, yet extracting personalized insights from increasingly complex data remains a significant challenge for modern machine learning. This is the case, in particular, when considering functional outcomes observed over a continuous domain (e.g., time, or space). Estimation of heterogeneous treatment effects, known as CATE, has emerged as a crucial tool for personalized decision-making, but existing meta-learning frameworks are largely limited to scalar outcomes, failing to provide satisfying results in scientific applications that leverage the rich, continuous information encoded in functional data. Here, we introduce FOCaL (Functional Outcome Causal Learning), a novel, doubly robust meta-learner specifically engineered to estimate a functional heterogeneous treatment effect (F-CATE). FOCaL integrates advanced functional regression techniques for both outcome modeling and functional pseudo-outcome reconstruction, thereby enabling the direct and robust estimation of F-CATE. We provide a rigorous theoretical derivation of FOCaL, demonstrate its performance and robustness compared to existing non-robust functional methods through comprehensive simulation studies, and illustrate its practical utility on diverse real-world functional datasets. FOCaL advances the capabilities of machine intelligence to infer nuanced, individualized causal effects from complex data, paving the way for more precise and trustworthy AI systems in personalized medicine, adaptive policy design, and fundamental scientific discovery.",
      "published": "2026-02-11",
      "updated": "2026-02-11",
      "categories": [
        "stat.ME",
        "stat.ML"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.11118v1",
      "arxivUrl": "http://arxiv.org/abs/2602.11118v1",
      "doi": null
    },
    {
      "id": "2602.11107",
      "title": "Renet: Principled and Efficient Relaxation for the Elastic Net via Dynamic Objective Selection",
      "authors": [
        "Albert Dorador"
      ],
      "abstract": "We introduce Renet, a principled generalization of the Relaxed Lasso to the Elastic Net family of estimators. While, on the one hand, $\\ell_1$-regularization is a standard tool for variable selection in high-dimensional regimes and, on the other hand, the $\\ell_2$ penalty provides stability and solution uniqueness through strict convexity, the standard Elastic Net nevertheless suffers from shrinkage bias that frequently yields suboptimal prediction accuracy. We propose to address this limitation through a framework called \\textit{relaxation}. Existing relaxation implementations rely on naive linear interpolations of penalized and unpenalized solutions, which ignore the non-linear geometry that characterizes the entire regularization path and risk violating the Karush-Kuhn-Tucker conditions. Renet addresses these limitations by enforcing sign consistency through an adaptive relaxation procedure that dynamically dispatches between convex blending and efficient sub-path refitting. Furthermore, we identify and formalize a unique synergy between relaxation and the ``One-Standard-Error'' rule: relaxation serves as a robust debiasing mechanism, allowing practitioners to leverage the parsimony of the 1-SE rule without the traditional loss in predictive fidelity. Our theoretical framework incorporates automated stability safeguards for ultra-high dimensional regimes and is supported by a comprehensive benchmarking suite across 20 synthetic and real-world datasets, demonstrating that Renet consistently outperforms the standard Elastic Net and provides a more robust alternative to the Adaptive Elastic Net in high-dimensional, low signal-to-noise ratio and high-multicollinearity regimes. By leveraging an adaptive solver backend, Renet delivers these statistical gains while offering a computational profile that remains competitive with state-of-the-art coordinate descent implementations.",
      "published": "2026-02-11",
      "updated": "2026-02-11",
      "categories": [
        "stat.ME",
        "cs.LG",
        "stat.ML"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.11107v1",
      "arxivUrl": "http://arxiv.org/abs/2602.11107v1",
      "doi": null
    },
    {
      "id": "2602.11080",
      "title": "Constrained Fiducial Inference for Gaussian Models",
      "authors": [
        "Hank Flury",
        "Jan Hannig",
        "Richard Smith"
      ],
      "abstract": "We propose a new fiducial Markov Chain Monte Carlo (MCMC) method for fitting parametric Gaussian models. We utilize the Cayley transform to decompose the parametric covariance matrix, which in turn allows us to formulate a general data generating algorithm for Gaussian data. Leveraging constrained generalized fiducial inference, we are able to create the basis of an MCMC algorithm, which can be specified to parametric models with minimal effort. The appeal of this novel approach is the wide class of models which it permits, ease of implementation and the posterior-like fiducial distribution without the need for a prior. We provide background information for the derivation of the relevant fiducial quantities, and a proof that the proposed MCMC algorithm targets the correct fiducial distribution. We need not assume independence nor identical distribution of the data, which makes the method attractive for application to time series and spatial data. Well-performing simulation results of the MA(1) and Matérn models are presented.",
      "published": "2026-02-11",
      "updated": "2026-02-11",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.11080v1",
      "arxivUrl": "http://arxiv.org/abs/2602.11080v1",
      "doi": null
    },
    {
      "id": "2602.11059",
      "title": "A Gibbs posterior sampler for inverse problem based on prior diffusion model",
      "authors": [
        "Jean-François Giovannelli"
      ],
      "abstract": "This paper addresses the issue of inversion in cases where (1) the observation system is modeled by a linear transformation and additive noise, (2) the problem is ill-posed and regularization is introduced in a Bayesian framework by an a prior density, and (3) the latter is modeled by a diffusion process adjusted on an available large set of examples. In this context, it is known that the issue of posterior sampling is a thorny one. This paper introduces a Gibbs algorithm. It appears that this avenue has not been explored, and we show that this approach is particularly effective and remarkably simple. In addition, it offers a guarantee of convergence in a clearly identified situation. The results are clearly confirmed by numerical simulations.",
      "published": "2026-02-11",
      "updated": "2026-02-11",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.AP"
      ],
      "primaryCategory": "stat.ML",
      "pdfUrl": "https://arxiv.org/pdf/2602.11059v1",
      "arxivUrl": "http://arxiv.org/abs/2602.11059v1",
      "doi": null
    },
    {
      "id": "2602.10969",
      "title": "Weighting-Based Identification and Estimation in Graphical Models of Missing Data",
      "authors": [
        "Anna Guo",
        "Razieh Nabi"
      ],
      "abstract": "We propose a constructive algorithm for identifying complete data distributions in graphical models of missing data. The complete data distribution is unrestricted, while the missingness mechanism is assumed to factorize according to a conditional directed acyclic graph. Our approach follows an interventionist perspective in which missingness indicators are treated as variables that can be intervened on. A central challenge in this setting is that sequences of interventions on missingness indicators may induce and propagate selection bias, so that identification can fail even when a propensity score is invariant to available interventions. To address this challenge, we introduce a tree-based identification algorithm that explicitly tracks the creation and propagation of selection bias and determines whether it can be avoided through admissible intervention strategies. The resulting tree provides both a diagnostic and a constructive characterization of identifiability under a given missingness mechanism. Building on these results, we develop recursive inverse probability weighting procedures that mirror the intervention logic of the identification algorithm, yielding valid estimating equations for both the missingness mechanism and functionals of the complete data distribution. Simulation studies and a real-data application illustrate the practical performance of the proposed methods. An accompanying R package, flexMissing, implements all proposed procedures.",
      "published": "2026-02-11",
      "updated": "2026-02-11",
      "categories": [
        "stat.ME",
        "stat.ML"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.10969v1",
      "arxivUrl": "http://arxiv.org/abs/2602.10969v1",
      "doi": null
    },
    {
      "id": "2602.10960",
      "title": "Integrating granular data into a multilayer network: an interbank model of the euro area for systemic risk assessment",
      "authors": [
        "Ilias Aarab",
        "Thomas Gottron",
        "Andrea Colombo",
        "Jörg Reddig",
        "Annalauro Ianiro"
      ],
      "abstract": "Micro-structural models of contagion and systemic risk emphasize that shock propagation is inherently multi-channel, spanning counterparty exposures, short-term funding and roll-over risk, securities cross-holdings, and common-asset (fire-sale) spillovers. Empirical implementations, however, often rely on stylized or simulated networks, or focus on a single exposure dimension, reflecting the practical difficulty of reconciling heterogeneous granular collections into a coherent representation with consistent identifiers and consolidation rules. We close part of this gap by constructing an empirically grounded multilayer network for euro area significant banking groups that integrates several supervisory and statistical datasets into layer-consistent exposure matrices defined on a common node set. Each layer corresponds to a distinct transmission channel, long- and short-term credit, securities cross-holdings, short-term secured funding, and overlapping external portfolios, and nodes are enriched with balance-sheet information to support model calibration. We document pronounced cross-layer heterogeneity in connectivity and centrality, and show that an aggregated (flattened) representation can mask economically relevant structure and misidentify the institutions that are systemically important in specific markets. We then illustrate how the resulting network disciplines standard systemic-risk analytics by implementing a centrality-based propagation measure and a micro-structural agent-based framework on real exposures. The approach provides a data-grounded basis for layer-aware systemic-risk assessment and stress testing across multiple dimensions of the banking network.",
      "published": "2026-02-11",
      "updated": "2026-02-11",
      "categories": [
        "q-fin.ST",
        "cs.CE",
        "econ.EM",
        "q-fin.RM",
        "stat.CO"
      ],
      "primaryCategory": "q-fin.ST",
      "pdfUrl": "https://arxiv.org/pdf/2602.10960v1",
      "arxivUrl": "http://arxiv.org/abs/2602.10960v1",
      "doi": "10.1007/s11634-026-00668-7"
    },
    {
      "id": "2602.10955",
      "title": "Prior Smoothing for Multivariate Disease Mapping Models",
      "authors": [
        "Garazi Retegui",
        "María Dolores Ugarte",
        "Jaione Etxeberria",
        "Alan E. Gelfand"
      ],
      "abstract": "To date, we have seen the emergence of a large literature on multivariate disease mapping. That is, incidence of (or mortality from) multiple diseases is recorded at the scale of areal units where incidence (mortality) across the diseases is expected to manifest dependence. The modeling involves a hierarchical structure: a Poisson model for disease counts (conditioning on the rates) at the first stage, and a specification of a function of the rates using spatial random effects at the second stage. These random effects are specified as a prior and introduce spatial smoothing to the rate (or risk) estimates. What we see in the literature is the amount of smoothing induced under a given prior across areal units compared with the observed/empirical risks. Our contribution here extends previous research on smoothing in univariate areal data models. Specifically, for three different choices of multivariate prior, we investigate both within prior smoothing according to hyperparameters and across prior smoothing. Its benefit to the user is to illuminate the expected nature of departure from perfect fit associated with these priors since model performance is not a question of goodness of fit. We propose both theoretical and empirical metrics for our investigation and illustrate with both simulated and real data.",
      "published": "2026-02-11",
      "updated": "2026-02-11",
      "categories": [
        "stat.ME",
        "stat.AP"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.10955v1",
      "arxivUrl": "http://arxiv.org/abs/2602.10955v1",
      "doi": null
    },
    {
      "id": "2602.10925",
      "title": "Fact or friction: Jumps at ultra high frequency",
      "authors": [
        "Kim Christensen",
        "Roel C. A. Oomen",
        "Mark Podolskij"
      ],
      "abstract": "This paper shows that jumps in financial asset prices are often erroneously identified and are, in fact, rare events accounting for a very small proportion of the total price variation. We apply new econometric techniques to a comprehensive set of ultra high-frequency equity and foreign exchange tick data recorded at millisecond precision, allowing us to examine the price evolution at the individual order level. We show that in both theory and practice, traditional measures of jump variation based on lower-frequency data tend to spuriously assign a burst of volatility to the jump component. As a result, the true price variation coming from jumps is overstated. Our estimates based on tick data suggest that the jump variation is an order of magnitude smaller than typical estimates found in the existing literature.",
      "published": "2026-02-11",
      "updated": "2026-02-11",
      "categories": [
        "econ.EM"
      ],
      "primaryCategory": "econ.EM",
      "pdfUrl": "https://arxiv.org/pdf/2602.10925v1",
      "arxivUrl": "http://arxiv.org/abs/2602.10925v1",
      "doi": "10.1016/j.jfineco.2014.07.007"
    },
    {
      "id": "2602.10924",
      "title": "Non-centred Bayesian inference for discrete-valued state-transition models: the Rippler algorithm",
      "authors": [
        "James Neill",
        "Lloyd A. C. Chapman",
        "Chris Jewell"
      ],
      "abstract": "Stochastic state-transition models of infectious disease transmission can be used to deduce relevant drivers of transmission when fitted to data using statistically principled methods. Fitting this individual-level data requires inference on individuals' unobserved disease statuses over time, which form a high-dimensional and highly correlated state space. We introduce a novel Bayesian (data-augmentation Markov chain Monte Carlo) algorithm for jointly estimating the model parameters and unobserved disease statuses, which we call the Rippler algorithm. This is a non-centred method that can be applied to any individual-based state-transition model. We compare the Rippler algorithm to the state-of-the-art inference methods for individual-based stochastic epidemic models and find that it performs better than these methods as the number of disease states in the model increases.",
      "published": "2026-02-11",
      "updated": "2026-02-11",
      "categories": [
        "stat.ME"
      ],
      "primaryCategory": "stat.ME",
      "pdfUrl": "https://arxiv.org/pdf/2602.10924v1",
      "arxivUrl": "http://arxiv.org/abs/2602.10924v1",
      "doi": null
    },
    {
      "id": "2602.10784",
      "title": "Integrating Unsupervised and Supervised Learning for the Prediction of Defensive Schemes in American football",
      "authors": [
        "Rouven Michels",
        "Robert Bajons",
        "Jan-Ole Fischer"
      ],
      "abstract": "Anticipating defensive coverage schemes is a crucial yet challenging task for offenses in American football. Because defenders' assignments are intentionally disguised before the snap, they remain difficult to recognize in real time. To address this challenge, we develop a statistical framework that integrates supervised and unsupervised learning using player tracking data. Our goal is to forecast the defensive coverage scheme -- man or zone -- through elastic net logistic regression and gradient-boosted decision trees with incrementally derived features. We first use features from the pre-motion situation, then incorporate players' trajectories during motion in a naive way, and finally include features derived from a hidden Markov model (HMM). Based on player movements, the non-homogeneous HMM infers latent defensive assignments between offensive and defensive players during motion and transforms decoded state sequences into informative features for the supervised models. These HMM-based features enhance predictive performance and are significantly associated with coverage outcomes. Moreover, estimated random effects offer interpretable insights into how different defenses and positions adjust their coverage responsibilities.",
      "published": "2026-02-11",
      "updated": "2026-02-11",
      "categories": [
        "stat.AP"
      ],
      "primaryCategory": "stat.AP",
      "pdfUrl": "https://arxiv.org/pdf/2602.10784v1",
      "arxivUrl": "http://arxiv.org/abs/2602.10784v1",
      "doi": null
    }
  ],
  "metadata": {
    "lastUpdated": "2026-02-16T16:07:58.427Z",
    "totalPapers": 50,
    "categories": [
      "stat.ME",
      "stat.AP",
      "econ.EM",
      "q-bio.QM"
    ],
    "queryDate": "2026-02-16"
  }
}